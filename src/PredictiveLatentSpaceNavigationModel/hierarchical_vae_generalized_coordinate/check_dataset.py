import os
import sys
from typing import Dict, Tuple, Optional
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
import sqlite3
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr

# ===== numpyå‹ã®SQLiteè‡ªå‹•å¤‰æ›ã‚’ç™»éŒ² =====
sqlite3.register_adapter(np.float32, float)
sqlite3.register_adapter(np.float64, float)
sqlite3.register_adapter(np.int32, int)
sqlite3.register_adapter(np.int64, int)
sqlite3.register_adapter(np.bool_, bool)


class GeneralizedCoordinateDataset(Dataset):
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ100Hzï¼‰ã‚’æ´»ç”¨
    """

    def __init__(self,
                 df: pd.DataFrame,
                 seq_len: int = 100,
                 dt: float = 0.01,  # 100Hz sampling = 0.01s interval
                 use_precomputed: bool = True):
        """
        Args:
            df: è»Œé“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆæ—¢ã«é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦è¨ˆç®—æ¸ˆã¿ï¼‰
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            dt: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆ100Hz = 0.01sï¼‰
            use_precomputed: äº‹å‰è¨ˆç®—æ¸ˆã¿ã®é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        self.seq_len = seq_len
        self.dt = dt
        self.use_precomputed = use_precomputed

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å®šç¾©
        self.position_cols = ['HandlePosX', 'HandlePosY']
        self.velocity_cols = ['HandleVelX', 'HandleVelY'] if use_precomputed else None
        self.acceleration_cols = ['HandleAccX', 'HandleAccY'] if use_precomputed else None

        # è©¦è¡Œã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        self.trials = list(df.groupby(['subject_id', 'trial_num']))

        print(f"âœ… ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   - ç·è©¦è¡Œæ•°: {len(self.trials)}")
        print(f"   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}")
        print(f"   - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°: {1 / dt:.0f}Hz")
        print(f"   - äº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {use_precomputed}")

    def __len__(self) -> int:
        return len(self.trials)

    def extract_generalized_coordinates(self, trial_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        è©¦è¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€èˆ¬åŒ–åº§æ¨™ã‚’æŠ½å‡º

        Returns:
            basic_coords: [seq_len, 8] åŸºæœ¬åº§æ¨™ï¼ˆVAEå­¦ç¿’ç”¨ï¼‰
            full_coords: [seq_len, 12] å®Œå…¨åº§æ¨™ï¼ˆè©•ä¾¡ç”¨ï¼‰
        """
        # === åŸºæœ¬åº§æ¨™ã®å–å¾— ===

        # 1. ä½ç½®ï¼ˆ0æ¬¡ï¼‰
        position = trial_df[self.position_cols].values  # [seq_len, 2]

        # 2. é€Ÿåº¦ï¼ˆ1æ¬¡ï¼‰
        if self.use_precomputed and self.velocity_cols:
            velocity = trial_df[self.velocity_cols].values
        else:
            # æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
            velocity = np.gradient(position, axis=0) / self.dt

        # 3. åŠ é€Ÿåº¦ï¼ˆ2æ¬¡ï¼‰
        if self.use_precomputed and self.acceleration_cols:
            acceleration = trial_df[self.acceleration_cols].values
        else:
            # æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
            acceleration = np.gradient(velocity, axis=0) / self.dt

        # 4. ã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆ3æ¬¡ï¼‰- å¸¸ã«æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
        jerk = np.gradient(acceleration, axis=0) / self.dt

        # === åˆæˆç‰¹å¾´ã®è¨ˆç®— ===

        # 5. é€Ÿåº¦ã®å¤§ãã•
        speed = np.linalg.norm(velocity, axis=1, keepdims=True)

        # 6. åŠ é€Ÿåº¦ã®å¤§ãã•
        acceleration_magnitude = np.linalg.norm(acceleration, axis=1, keepdims=True)

        # 7. ã‚¸ãƒ£ãƒ¼ã‚¯ã®å¤§ãã•
        jerk_magnitude = np.linalg.norm(jerk, axis=1, keepdims=True)

        # 8. æ›²ç‡
        curvature = self._compute_curvature(position)

        # === åº§æ¨™ã®çµåˆ ===

        # åŸºæœ¬åº§æ¨™ï¼ˆVAEå­¦ç¿’ç”¨ï¼‰: 8æ¬¡å…ƒ
        basic_coords = np.concatenate([
            position,  # 2æ¬¡å…ƒ
            velocity,  # 2æ¬¡å…ƒ
            acceleration,  # 2æ¬¡å…ƒ
            jerk  # 2æ¬¡å…ƒ
        ], axis=1)

        # å®Œå…¨åº§æ¨™ï¼ˆè©•ä¾¡ç”¨ï¼‰: 12æ¬¡å…ƒ
        full_coords = np.concatenate([
            basic_coords,  # 8æ¬¡å…ƒ
            speed,  # 1æ¬¡å…ƒ
            acceleration_magnitude,  # 1æ¬¡å…ƒ
            jerk_magnitude,  # 1æ¬¡å…ƒ
            curvature  # 1æ¬¡å…ƒ
        ], axis=1)

        return basic_coords, full_coords

    def _compute_curvature(self, position: np.ndarray) -> np.ndarray:
        """
        è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—

        Args:
            position: [seq_len, 2] ä½ç½®ãƒ‡ãƒ¼ã‚¿

        Returns:
            curvature: [seq_len, 1] æ›²ç‡
        """
        if len(position) < 3:
            return np.zeros((len(position), 1))

        # 1æ¬¡ãƒ»2æ¬¡å¾®åˆ†
        dx = np.gradient(position[:, 0])
        dy = np.gradient(position[:, 1])
        ddx = np.gradient(dx)
        ddy = np.gradient(dy)

        # æ›²ç‡å…¬å¼: Îº = |x'y'' - y'x''| / (x'Â² + y'Â²)^(3/2)
        numerator = np.abs(dx * ddy - dy * ddx)
        denominator = (dx ** 2 + dy ** 2) ** (3 / 2)

        # ã‚¼ãƒ­é™¤ç®—å›é¿
        denominator = np.where(denominator < 1e-8, 1e-8, denominator)
        curvature = numerator / denominator

        # ç•°å¸¸å€¤ã‚’åˆ¶é™
        curvature = np.clip(curvature, 0, 100)

        return curvature.reshape(-1, 1)

    def _pad_or_truncate(self, coords: np.ndarray) -> np.ndarray:
        """
        å›ºå®šé•·åŒ–å‡¦ç†ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šæ¨ã¦ï¼‰

        Args:
            coords: [actual_len, coord_dim] åº§æ¨™ãƒ‡ãƒ¼ã‚¿

        Returns:
            processed_coords: [seq_len, coord_dim] å›ºå®šé•·åº§æ¨™
        """
        actual_len, coord_dim = coords.shape

        if actual_len > self.seq_len:
            # åˆ‡ã‚Šæ¨ã¦
            return coords[:self.seq_len]
        elif actual_len < self.seq_len:
            # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding = np.zeros((self.seq_len - actual_len, coord_dim))
            return np.vstack([coords, padding])
        else:
            return coords

    def __getitem__(self, idx) -> Tuple[torch.Tensor, int, torch.Tensor]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Returns:
            full_coords: [seq_len, 12] å®Œå…¨ãªä¸€èˆ¬åŒ–åº§æ¨™
            subject_id: è¢«é¨“è€…ID
            is_expert: ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«
        """
        (subject_id, trial_num), trial_df = self.trials[idx]

        # ä¸€èˆ¬åŒ–åº§æ¨™ã‚’æŠ½å‡º
        basic_coords, full_coords = self.extract_generalized_coordinates(trial_df)

        # å›ºå®šé•·åŒ–
        full_coords = self._pad_or_truncate(full_coords)

        # ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        is_expert = trial_df['is_expert'].iloc[0] if 'is_expert' in trial_df.columns else 0

        return (
            torch.tensor(full_coords, dtype=torch.float32),
            subject_id,
            torch.tensor(is_expert, dtype=torch.long)
        )

class TrajectoryDataset(Dataset):
    def __init__(self, df: pd.DataFrame, seq_len: int = 100, feature_cols=['HandlePosX', 'HandlePosY','HandleVelX','HandleVelY','HandleAccX','HandleAccY']):
        self.seq_len = seq_len
        self.feature_cols = feature_cols
        self.trials = list(df.groupby(['subject_id', 'trial_num']))

    def __len__(self) -> int:
        return len(self.trials)

    def __getitem__(self, idx) -> tuple:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

        :return: tuple(è»Œé“ãƒ†ãƒ³ã‚½ãƒ«, è¢«é¨“è€…ID, ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«)
        """
        (subject_id, _), trial_df = self.trials[idx]

        # --- å„è©¦è¡Œãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å‰å‡¦ç† ---
        # 1. è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’Numpyé…åˆ—ã«å¤‰æ›
        trajectory_abs = trial_df[self.feature_cols].values

        # 2. å·®åˆ†è¨ˆç®—
        trajectory_diff = np.diff(trajectory_abs, axis=0)
        trajectory_diff = np.insert(trajectory_diff, 0, [0, 0], axis=0)

        # 3. å›ºå®šé•·åŒ– (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/åˆ‡ã‚Šæ¨ã¦)
        if len(trajectory_diff) > self.seq_len:
            processed_trajectory = trajectory_diff[:self.seq_len]
        else:
            padding = np.zeros((self.seq_len - len(trajectory_diff), len(self.feature_cols)))
            processed_trajectory = np.vstack([trajectory_diff, padding])

        # 4. ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        is_expert = trial_df['is_expert'].iloc[0]

        # 5. ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦è¿”ã™
        return (
            torch.tensor(processed_trajectory, dtype=torch.float32),
            subject_id,
            torch.tensor(is_expert, dtype=torch.long)
        )


def create_generalized_dataloaders(master_data_path: str,
                                   seq_len: int,
                                   batch_size: int,
                                   use_precomputed: bool = True,
                                   random_seed: int = 42) -> Tuple[DataLoader, DataLoader, DataLoader, pd.DataFrame]:
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ

    Args:
        master_data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        use_precomputed: äº‹å‰è¨ˆç®—æ¸ˆã¿é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ã‚’ä½¿ç”¨
        random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
        train_loader, val_loader, test_loader, test_df
    """
    try:
        master_df = pd.read_parquet(master_data_path)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {master_data_path}")
        print(f"   - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(master_df):,}")
        print(f"   - è¢«é¨“è€…æ•°: {master_df['subject_id'].nunique()}")
        print(f"   - è©¦è¡Œæ•°: {master_df.groupby(['subject_id', 'trial_num']).ngroups}")
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{master_data_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None, None, None
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None, None, None, None

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['subject_id', 'trial_num', 'HandlePosX', 'HandlePosY']
    if use_precomputed:
        required_cols.extend(['HandleVelX', 'HandleVelY', 'HandleAccX', 'HandleAccY'])

    missing_cols = [col for col in required_cols if col not in master_df.columns]
    if missing_cols:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_cols}")
        return None, None, None, None

    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:")
    for col in ['HandlePosX', 'HandlePosY']:
        var_val = master_df[col].var()
        print(f"   - {col} åˆ†æ•£: {var_val:.6f}")

    if use_precomputed:
        for col in ['HandleVelX', 'HandleVelY', 'HandleAccX', 'HandleAccY']:
            var_val = master_df[col].var()
            print(f"   - {col} åˆ†æ•£: {var_val:.6f}")

    # è¢«é¨“è€…ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    np.random.seed(random_seed)
    subject_ids = master_df['subject_id'].unique()
    np.random.shuffle(subject_ids)

    if len(subject_ids) < 3:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã«ã¯æœ€ä½3äººã®è¢«é¨“è€…ãŒå¿…è¦ã§ã™ã€‚")

    # åˆ†å‰²æ¯”ç‡: train 60%, val 20%, test 20%
    n_subjects = len(subject_ids)
    n_train = max(1, int(n_subjects * 0.6))
    n_val = max(1, int(n_subjects * 0.2))
    n_test = n_subjects - n_train - n_val

    if n_test < 1:
        n_test = 1
        n_val = n_subjects - n_train - n_test

    train_ids = subject_ids[:n_train]
    val_ids = subject_ids[n_train:n_train + n_val]
    test_ids = subject_ids[n_train + n_val:]

    train_df = master_df[master_df['subject_id'].isin(train_ids)]
    val_df = master_df[master_df['subject_id'].isin(val_ids)]
    test_df = master_df[master_df['subject_id'].isin(test_ids)]

    print(f"\nğŸ”„ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"   - å­¦ç¿’ç”¨: {len(train_ids)}äºº ({len(train_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    print(f"   - æ¤œè¨¼ç”¨: {len(val_ids)}äºº ({len(val_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    print(f"   - ãƒ†ã‚¹ãƒˆç”¨: {len(test_ids)}äºº ({len(test_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = GeneralizedCoordinateDataset(train_df, seq_len=seq_len, use_precomputed=use_precomputed)
    val_dataset = GeneralizedCoordinateDataset(val_df, seq_len=seq_len, use_precomputed=use_precomputed)
    test_dataset = GeneralizedCoordinateDataset(test_df, seq_len=seq_len, use_precomputed=use_precomputed)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†:")
    print(f"   - å­¦ç¿’ãƒãƒƒãƒæ•°: {len(train_loader)}")
    print(f"   - æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_loader)}")
    print(f"   - ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: {len(test_loader)}")
    print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")

    return train_loader, val_loader, test_loader, test_df

def analyze_generalized_coordinates(dataset: GeneralizedCoordinateDataset,
                                    num_samples: int = 5) -> None:
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ

    Args:
        dataset: ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        num_samples: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    print(f"\nğŸ”¬ ä¸€èˆ¬åŒ–åº§æ¨™åˆ†æï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°: {num_samples}ï¼‰")
    print("=" * 60)

    coord_names = [
        'pos_x', 'pos_y', 'vel_x', 'vel_y', 'acc_x', 'acc_y', 'jerk_x', 'jerk_y',
        'speed', 'acc_magnitude', 'jerk_magnitude', 'curvature'
    ]

    all_coords = []
    subject_info = []

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for i in range(min(num_samples, len(dataset))):
        coords, subject_id, is_expert = dataset[i]
        coords_np = coords.numpy()
        all_coords.append(coords_np)
        subject_info.append((subject_id, is_expert.item()))

    if not all_coords:
        print("âŒ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    all_coords = np.stack(all_coords)  # [num_samples, seq_len, 12]

    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
    print("ğŸ“Š å„åº§æ¨™ã®çµ±è¨ˆæƒ…å ±:")
    print(f"{'åº§æ¨™å':<15} {'å¹³å‡':<12} {'æ¨™æº–åå·®':<12} {'æœ€å°å€¤':<12} {'æœ€å¤§å€¤':<12}")
    print("-" * 60)

    for i, name in enumerate(coord_names):
        coord_data = all_coords[:, :, i].flatten()
        coord_data = coord_data[coord_data != 0]  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å¤–

        if len(coord_data) > 0:
            mean_val = np.mean(coord_data)
            std_val = np.std(coord_data)
            min_val = np.min(coord_data)
            max_val = np.max(coord_data)

            print(f"{name:<15} {mean_val:<12.6f} {std_val:<12.6f} {min_val:<12.6f} {max_val:<12.6f}")

    # è¢«é¨“è€…æƒ…å ±
    print(f"\nğŸ‘¥ è¢«é¨“è€…æƒ…å ±:")
    for i, (subject_id, is_expert) in enumerate(subject_info):
        expert_status = "ç†Ÿé”è€…" if is_expert else "åˆå¿ƒè€…"
        print(f"   ã‚µãƒ³ãƒ—ãƒ« {i + 1}: è¢«é¨“è€…{subject_id} ({expert_status})")

    # åº§æ¨™é–“ã®ç›¸é–¢ï¼ˆåŸºæœ¬åº§æ¨™vsåˆæˆç‰¹å¾´ï¼‰
    print(f"\nğŸ”— åŸºæœ¬åº§æ¨™ã¨åˆæˆç‰¹å¾´ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")

    # é€Ÿåº¦ã®å¤§ãã•ã®æ•´åˆæ€§
    vel_x = all_coords[:, :, 2].flatten()
    vel_y = all_coords[:, :, 3].flatten()
    speed_computed = np.sqrt(vel_x ** 2 + vel_y ** 2)
    speed_stored = all_coords[:, :, 8].flatten()

    # éã‚¼ãƒ­è¦ç´ ã®ã¿ã§æ¯”è¼ƒ
    mask = (vel_x != 0) | (vel_y != 0)
    if mask.sum() > 0:
        speed_error = np.mean(np.abs(speed_computed[mask] - speed_stored[mask]))
        print(f"   é€Ÿåº¦å¤§ãã•èª¤å·®: {speed_error:.8f}")

    # åŠ é€Ÿåº¦ã®å¤§ãã•ã®æ•´åˆæ€§
    acc_x = all_coords[:, :, 4].flatten()
    acc_y = all_coords[:, :, 5].flatten()
    acc_mag_computed = np.sqrt(acc_x ** 2 + acc_y ** 2)
    acc_mag_stored = all_coords[:, :, 9].flatten()

    mask = (acc_x != 0) | (acc_y != 0)
    if mask.sum() > 0:
        acc_error = np.mean(np.abs(acc_mag_computed[mask] - acc_mag_stored[mask]))
        print(f"   åŠ é€Ÿåº¦å¤§ãã•èª¤å·®: {acc_error:.8f}")


if __name__ == "__main__":
    data_path = "PredictiveLatentSpaceNavigationModel/DataPreprocess/my_data.parquet"
    seq_len = 100
    batch_size = 16

    print("ğŸš€ ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_loader, val_loader, test_loader, test_df = create_generalized_dataloaders(
        master_data_path=data_path,
        seq_len=seq_len,
        batch_size=batch_size,
        use_precomputed=True,  # äº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        random_seed=42
    )

    if train_loader is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        exit(1)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒã®ç¢ºèª
    print("\nğŸ§ª ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ:")
    for i, (coords, subject_ids, is_expert) in enumerate(train_loader):
        print(f"   ãƒãƒƒãƒ {i + 1}:")
        print(f"     - åº§æ¨™shape: {coords.shape}")
        print(f"     - è¢«é¨“è€…ID: {subject_ids}")
        print(f"     - ç†Ÿç·´åº¦: {is_expert}")

        if i >= 2:  # æœ€åˆã®3ãƒãƒƒãƒã®ã¿ãƒ†ã‚¹ãƒˆ
            break

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ
    analyze_generalized_coordinates(train_loader.dataset, num_samples=10)

    print("\nâœ… ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")