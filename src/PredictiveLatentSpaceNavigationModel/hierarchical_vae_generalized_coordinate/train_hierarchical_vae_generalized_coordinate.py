import os
import sys
import argparse
import warnings
from typing import Dict, Tuple, Optional
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import ChainedScheduler
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime
import yaml
import sqlite3
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.linear_model import LinearRegression
from sklearn.metrics import adjusted_rand_score
from sklearn.cluster import KMeans
from scipy.stats import pearsonr
import seaborn as sns
import json

# ===== numpyå‹ã®SQLiteè‡ªå‹•å¤‰æ›ã‚’ç™»éŒ² =====
sqlite3.register_adapter(np.float32, float)
sqlite3.register_adapter(np.float64, float)
sqlite3.register_adapter(np.int32, int)
sqlite3.register_adapter(np.int64, int)
sqlite3.register_adapter(np.bool_, bool)

# --- ãƒ‘ã‚¹è¨­å®šã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# éšå±¤å‹VAEã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from models.hierarchical_vae_generalized_coordinate import GeneralizedCoordinateHierarchicalVAE
except ImportError:
    print("è­¦å‘Š: models.hierarchical_vae ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    sys.exit(1)

try:
    from DataPreprocess.data_preprocess import calculate_jerk, calculate_path_efficiency, calculate_approach_angle, \
        calculate_sparc
except ImportError:
    print("è­¦å‘Š: data_preprocess ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")


class SkillAxisAnalyzer:
    """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã§ã®ä¸Šæ‰‹ã•ã®è»¸ã‚’åˆ†æãƒ»æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆä¸€èˆ¬åŒ–åº§æ¨™å¯¾å¿œç‰ˆï¼‰"""

    def __init__(self):
        self.skill_improvement_directions = {}
        self.performance_correlations = {}

    def analyze_skill_axes(self, z_skill_data, performance_data):
        """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®ç›¸é–¢åˆ†æ"""
        print("=== ä¸€èˆ¬åŒ–åº§æ¨™VAE ã‚¹ã‚­ãƒ«è»¸åˆ†æé–‹å§‹ ===")

        skill_dim = z_skill_data.shape[1]

        # å„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã¨ã®ç›¸é–¢åˆ†æ
        for metric_name, metric_values in performance_data.items():
            if len(metric_values) == 0:
                continue

            correlations = []

            for dim in range(skill_dim):
                if len(set(metric_values)) > 1 and np.std(metric_values) > 1e-6:
                    try:
                        corr, p_value = pearsonr(z_skill_data[:, dim], metric_values)
                        correlations.append((corr, p_value, dim))
                    except:
                        correlations.append((0.0, 1.0, dim))
                else:
                    correlations.append((0.0, 1.0, dim))

            correlations.sort(key=lambda x: abs(x[0]), reverse=True)
            best_corr, best_p, best_dim = correlations[0]

            self.performance_correlations[metric_name] = {
                'best_dimension': best_dim,
                'correlation': best_corr,
                'p_value': best_p,
                'all_correlations': correlations
            }

            print(f"{metric_name}: æœ€å¼·ç›¸é–¢æ¬¡å…ƒ={best_dim}, r={best_corr:.4f}, p={best_p:.4f}")

        # ç·åˆçš„ãªä¸Šæ‰‹ã•è»¸ã®æŠ½å‡º
        self._extract_overall_skill_axis(z_skill_data, performance_data)

        # å€‹åˆ¥æŒ‡æ¨™ã®æ”¹å–„æ–¹å‘
        self._extract_specific_improvement_directions(z_skill_data, performance_data)

    def _extract_overall_skill_axis(self, z_skill_data, performance_data):
        """ç·åˆçš„ãªã‚¹ã‚­ãƒ«å‘ä¸Šè»¸ã‚’æŠ½å‡º"""
        print("\n--- ç·åˆã‚¹ã‚­ãƒ«è»¸ã®æŠ½å‡º ---")

        normalized_metrics = {}

        for metric_name, values in performance_data.items():
            values = np.array(values)

            if len(values) == 0 or np.std(values) < 1e-6:
                continue

            # æŒ‡æ¨™ã®æ–¹å‘æ€§ã‚’çµ±ä¸€ï¼ˆä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã¯ç¬¦å·åè»¢ï¼‰
            if metric_name in ['trial_time', 'trial_error', 'jerk', 'trial_variability']:
                normalized_values = -values
            else:
                normalized_values = values

            # æ¨™æº–åŒ–
            if np.std(normalized_values) > 1e-6:
                normalized_values = (normalized_values - normalized_values.mean()) / normalized_values.std()
                normalized_metrics[metric_name] = normalized_values

        if len(normalized_metrics) == 0:
            print("è­¦å‘Š: æœ‰åŠ¹ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢
        overall_skill_score = np.zeros(len(z_skill_data))
        weight_sum = 0

        for metric_name, normalized_values in normalized_metrics.items():
            overall_skill_score += normalized_values
            weight_sum += 1

        if weight_sum > 0:
            overall_skill_score /= weight_sum

        # ç·šå½¢å›å¸°
        try:
            reg = LinearRegression()
            reg.fit(z_skill_data, overall_skill_score)

            overall_improvement_direction = reg.coef_
            improvement_magnitude = np.linalg.norm(overall_improvement_direction)

            if improvement_magnitude > 1e-6:
                overall_improvement_direction = overall_improvement_direction / improvement_magnitude

                self.skill_improvement_directions['overall'] = {
                    'direction': overall_improvement_direction,
                    'r_squared': reg.score(z_skill_data, overall_skill_score),
                    'coefficients': reg.coef_
                }

                print(f"ç·åˆã‚¹ã‚­ãƒ«è»¸: RÂ²={reg.score(z_skill_data, overall_skill_score):.4f}")
            else:
                print("è­¦å‘Š: ç·åˆæ”¹å–„æ–¹å‘ã®è¨ˆç®—ã«å¤±æ•—")
        except Exception as e:
            print(f"ç·åˆã‚¹ã‚­ãƒ«è»¸æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    def _extract_specific_improvement_directions(self, z_skill_data, performance_data):
        """å€‹åˆ¥æŒ‡æ¨™ã®æ”¹å–„æ–¹å‘ã‚’æŠ½å‡º"""
        print("\n--- å€‹åˆ¥æŒ‡æ¨™æ”¹å–„æ–¹å‘ã®æŠ½å‡º ---")

        for metric_name, values in performance_data.items():
            values = np.array(values)

            if len(values) == 0 or np.std(values) < 1e-6:
                continue

            # æ–¹å‘æ€§ã‚’çµ±ä¸€
            if metric_name in ['trial_time', 'trial_error', 'jerk', 'trial_variability']:
                target_values = -values
            else:
                target_values = values

            try:
                reg = LinearRegression()
                reg.fit(z_skill_data, target_values)

                improvement_direction = reg.coef_
                improvement_magnitude = np.linalg.norm(improvement_direction)

                if improvement_magnitude > 1e-6:
                    improvement_direction = improvement_direction / improvement_magnitude

                    self.skill_improvement_directions[metric_name] = {
                        'direction': improvement_direction,
                        'r_squared': reg.score(z_skill_data, target_values),
                        'coefficients': reg.coef_
                    }

                    print(f"{metric_name}: RÂ²={reg.score(z_skill_data, target_values):.4f}")
            except Exception as e:
                print(f"{metric_name}ã®æ”¹å–„æ–¹å‘æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    def get_improvement_direction(self, metric='overall', confidence_threshold=0.1):
        """æŒ‡å®šã•ã‚ŒãŸæŒ‡æ¨™ã®æ”¹å–„æ–¹å‘ã‚’å–å¾—"""
        if metric not in self.skill_improvement_directions:
            print(f"è­¦å‘Š: {metric}ã®æ”¹å–„æ–¹å‘ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            if 'overall' in self.skill_improvement_directions:
                print("ç·åˆæ–¹å‘ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                metric = 'overall'
            else:
                raise ValueError("åˆ©ç”¨å¯èƒ½ãªæ”¹å–„æ–¹å‘ãŒã‚ã‚Šã¾ã›ã‚“")

        direction_info = self.skill_improvement_directions[metric]

        if direction_info['r_squared'] < confidence_threshold:
            print(f"è­¦å‘Š: {metric}ã®RÂ²={direction_info['r_squared']:.4f}ãŒé–¾å€¤{confidence_threshold}ã‚’ä¸‹å›ã‚Šã¾ã™")

        return direction_info['direction']


class GeneralizedCoordinateDataset(Dataset):
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ100Hzï¼‰ã‚’æ´»ç”¨
    """

    def __init__(self,
                 df: pd.DataFrame,
                 seq_len: int = 100,
                 dt: float = 0.01,  # 100Hz sampling = 0.01s interval
                 use_precomputed: bool = True):
        """
        Args:
            df: è»Œé“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆæ—¢ã«é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦è¨ˆç®—æ¸ˆã¿ï¼‰
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            dt: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆ100Hz = 0.01sï¼‰
            use_precomputed: äº‹å‰è¨ˆç®—æ¸ˆã¿ã®é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        self.seq_len = seq_len
        self.dt = dt
        self.use_precomputed = use_precomputed

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å®šç¾©
        self.position_cols = ['HandlePosX', 'HandlePosY']
        self.velocity_cols = ['HandleVelX', 'HandleVelY'] if use_precomputed else None
        self.acceleration_cols = ['HandleAccX', 'HandleAccY'] if use_precomputed else None

        # è©¦è¡Œã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        self.trials = list(df.groupby(['subject_id', 'trial_num']))

        print(f"âœ… ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   - ç·è©¦è¡Œæ•°: {len(self.trials)}")
        print(f"   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}")
        print(f"   - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°: {1 / dt:.0f}Hz")
        print(f"   - äº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {use_precomputed}")

    def __len__(self) -> int:
        return len(self.trials)

    def extract_generalized_coordinates(self, trial_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        è©¦è¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€èˆ¬åŒ–åº§æ¨™ã‚’æŠ½å‡º

        Returns:
            basic_coords: [seq_len, 8] åŸºæœ¬åº§æ¨™ï¼ˆVAEå­¦ç¿’ç”¨ï¼‰
            full_coords: [seq_len, 12] å®Œå…¨åº§æ¨™ï¼ˆè©•ä¾¡ç”¨ï¼‰
        """
        # === åŸºæœ¬åº§æ¨™ã®å–å¾— ===

        # 1. ä½ç½®ï¼ˆ0æ¬¡ï¼‰
        position = trial_df[self.position_cols].values  # [seq_len, 2]

        # 2. é€Ÿåº¦ï¼ˆ1æ¬¡ï¼‰
        if self.use_precomputed and self.velocity_cols:
            velocity = trial_df[self.velocity_cols].values
        else:
            # æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
            velocity = np.gradient(position, axis=0) / self.dt

        # 3. åŠ é€Ÿåº¦ï¼ˆ2æ¬¡ï¼‰
        if self.use_precomputed and self.acceleration_cols:
            acceleration = trial_df[self.acceleration_cols].values
        else:
            # æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
            acceleration = np.gradient(velocity, axis=0) / self.dt

        # 4. ã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆ3æ¬¡ï¼‰- å¸¸ã«æ•°å€¤å¾®åˆ†ã§è¨ˆç®—
        jerk = np.gradient(acceleration, axis=0) / self.dt

        # === åˆæˆç‰¹å¾´ã®è¨ˆç®— ===

        # 5. é€Ÿåº¦ã®å¤§ãã•
        speed = np.linalg.norm(velocity, axis=1, keepdims=True)

        # 6. åŠ é€Ÿåº¦ã®å¤§ãã•
        acceleration_magnitude = np.linalg.norm(acceleration, axis=1, keepdims=True)

        # 7. ã‚¸ãƒ£ãƒ¼ã‚¯ã®å¤§ãã•
        jerk_magnitude = np.linalg.norm(jerk, axis=1, keepdims=True)

        # 8. æ›²ç‡
        curvature = self._compute_curvature(position)

        # === åº§æ¨™ã®çµåˆ ===

        # åŸºæœ¬åº§æ¨™ï¼ˆVAEå­¦ç¿’ç”¨ï¼‰: 8æ¬¡å…ƒ
        basic_coords = np.concatenate([
            position,  # 2æ¬¡å…ƒ
            velocity,  # 2æ¬¡å…ƒ
            acceleration,  # 2æ¬¡å…ƒ
            jerk  # 2æ¬¡å…ƒ
        ], axis=1)

        # å®Œå…¨åº§æ¨™ï¼ˆè©•ä¾¡ç”¨ï¼‰: 12æ¬¡å…ƒ
        full_coords = np.concatenate([
            basic_coords,  # 8æ¬¡å…ƒ
            speed,  # 1æ¬¡å…ƒ
            acceleration_magnitude,  # 1æ¬¡å…ƒ
            jerk_magnitude,  # 1æ¬¡å…ƒ
            curvature  # 1æ¬¡å…ƒ
        ], axis=1)

        return basic_coords, full_coords

    def _compute_curvature(self, position: np.ndarray) -> np.ndarray:
        """
        è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—

        Args:
            position: [seq_len, 2] ä½ç½®ãƒ‡ãƒ¼ã‚¿

        Returns:
            curvature: [seq_len, 1] æ›²ç‡
        """
        if len(position) < 3:
            return np.zeros((len(position), 1))

        # 1æ¬¡ãƒ»2æ¬¡å¾®åˆ†
        dx = np.gradient(position[:, 0])
        dy = np.gradient(position[:, 1])
        ddx = np.gradient(dx)
        ddy = np.gradient(dy)

        # æ›²ç‡å…¬å¼: Îº = |x'y'' - y'x''| / (x'Â² + y'Â²)^(3/2)
        numerator = np.abs(dx * ddy - dy * ddx)
        denominator = (dx ** 2 + dy ** 2) ** (3 / 2)

        # ã‚¼ãƒ­é™¤ç®—å›é¿
        denominator = np.where(denominator < 1e-8, 1e-8, denominator)
        curvature = numerator / denominator

        # ç•°å¸¸å€¤ã‚’åˆ¶é™
        curvature = np.clip(curvature, 0, 100)

        return curvature.reshape(-1, 1)

    def _pad_or_truncate(self, coords: np.ndarray) -> np.ndarray:
        """
        å›ºå®šé•·åŒ–å‡¦ç†ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šæ¨ã¦ï¼‰

        Args:
            coords: [actual_len, coord_dim] åº§æ¨™ãƒ‡ãƒ¼ã‚¿

        Returns:
            processed_coords: [seq_len, coord_dim] å›ºå®šé•·åº§æ¨™
        """
        actual_len, coord_dim = coords.shape

        if actual_len > self.seq_len:
            # åˆ‡ã‚Šæ¨ã¦
            return coords[:self.seq_len]
        elif actual_len < self.seq_len:
            # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding = np.zeros((self.seq_len - actual_len, coord_dim))
            return np.vstack([coords, padding])
        else:
            return coords

    def __getitem__(self, idx) -> Tuple[torch.Tensor, int, torch.Tensor]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Returns:
            full_coords: [seq_len, 12] å®Œå…¨ãªä¸€èˆ¬åŒ–åº§æ¨™
            subject_id: è¢«é¨“è€…ID
            is_expert: ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«
        """
        (subject_id, trial_num), trial_df = self.trials[idx]

        # ä¸€èˆ¬åŒ–åº§æ¨™ã‚’æŠ½å‡º
        basic_coords, full_coords = self.extract_generalized_coordinates(trial_df)

        # å›ºå®šé•·åŒ–
        full_coords = self._pad_or_truncate(full_coords)

        # ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        is_expert = trial_df['is_expert'].iloc[0] if 'is_expert' in trial_df.columns else 0

        return (
            torch.tensor(full_coords, dtype=torch.float32),
            subject_id,
            torch.tensor(is_expert, dtype=torch.long)
        )

class TrajectoryDataset(Dataset):
    def __init__(self, df: pd.DataFrame, seq_len: int = 100, feature_cols=['HandlePosX', 'HandlePosY','HandleVelX','HandleVelY','HandleAccX','HandleAccY']):
        self.seq_len = seq_len
        self.feature_cols = feature_cols
        self.trials = list(df.groupby(['subject_id', 'trial_num']))

    def __len__(self) -> int:
        return len(self.trials)

    def __getitem__(self, idx) -> tuple:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

        :return: tuple(è»Œé“ãƒ†ãƒ³ã‚½ãƒ«, è¢«é¨“è€…ID, ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«)
        """
        (subject_id, _), trial_df = self.trials[idx]

        # --- å„è©¦è¡Œãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å‰å‡¦ç† ---
        # 1. è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’Numpyé…åˆ—ã«å¤‰æ›
        trajectory_abs = trial_df[self.feature_cols].values

        # 2. å·®åˆ†è¨ˆç®—
        trajectory_diff = np.diff(trajectory_abs, axis=0)
        trajectory_diff = np.insert(trajectory_diff, 0, [0, 0], axis=0)

        # 3. å›ºå®šé•·åŒ– (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/åˆ‡ã‚Šæ¨ã¦)
        if len(trajectory_diff) > self.seq_len:
            processed_trajectory = trajectory_diff[:self.seq_len]
        else:
            padding = np.zeros((self.seq_len - len(trajectory_diff), len(self.feature_cols)))
            processed_trajectory = np.vstack([trajectory_diff, padding])

        # 4. ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        is_expert = trial_df['is_expert'].iloc[0]

        # 5. ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦è¿”ã™
        return (
            torch.tensor(processed_trajectory, dtype=torch.float32),
            subject_id,
            torch.tensor(is_expert, dtype=torch.long)
        )


def create_generalized_dataloaders(master_data_path: str,
                                   seq_len: int,
                                   batch_size: int,
                                   use_precomputed: bool = True,
                                   random_seed: int = 42) -> Tuple[DataLoader, DataLoader, DataLoader, pd.DataFrame]:
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ

    Args:
        master_data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        use_precomputed: äº‹å‰è¨ˆç®—æ¸ˆã¿é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ã‚’ä½¿ç”¨
        random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
        train_loader, val_loader, test_loader, test_df
    """
    try:
        master_df = pd.read_parquet(master_data_path)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {master_data_path}")
        print(f"   - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(master_df):,}")
        print(f"   - è¢«é¨“è€…æ•°: {master_df['subject_id'].nunique()}")
        print(f"   - è©¦è¡Œæ•°: {master_df.groupby(['subject_id', 'trial_num']).ngroups}")
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{master_data_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None, None, None
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None, None, None, None

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['subject_id', 'trial_num', 'HandlePosX', 'HandlePosY']
    if use_precomputed:
        required_cols.extend(['HandleVelX', 'HandleVelY', 'HandleAccX', 'HandleAccY'])

    missing_cols = [col for col in required_cols if col not in master_df.columns]
    if missing_cols:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_cols}")
        return None, None, None, None

    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:")
    for col in ['HandlePosX', 'HandlePosY']:
        var_val = master_df[col].var()
        print(f"   - {col} åˆ†æ•£: {var_val:.6f}")

    if use_precomputed:
        for col in ['HandleVelX', 'HandleVelY', 'HandleAccX', 'HandleAccY']:
            var_val = master_df[col].var()
            print(f"   - {col} åˆ†æ•£: {var_val:.6f}")

    # è¢«é¨“è€…ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    np.random.seed(random_seed)
    subject_ids = master_df['subject_id'].unique()
    np.random.shuffle(subject_ids)

    if len(subject_ids) < 3:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã«ã¯æœ€ä½3äººã®è¢«é¨“è€…ãŒå¿…è¦ã§ã™ã€‚")

    # åˆ†å‰²æ¯”ç‡: train 60%, val 20%, test 20%
    n_subjects = len(subject_ids)
    n_train = max(1, int(n_subjects * 0.6))
    n_val = max(1, int(n_subjects * 0.2))
    n_test = n_subjects - n_train - n_val

    if n_test < 1:
        n_test = 1
        n_val = n_subjects - n_train - n_test

    train_ids = subject_ids[:n_train]
    val_ids = subject_ids[n_train:n_train + n_val]
    test_ids = subject_ids[n_train + n_val:]

    train_df = master_df[master_df['subject_id'].isin(train_ids)]
    val_df = master_df[master_df['subject_id'].isin(val_ids)]
    test_df = master_df[master_df['subject_id'].isin(test_ids)]

    print(f"\nğŸ”„ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"   - å­¦ç¿’ç”¨: {len(train_ids)}äºº ({len(train_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    print(f"   - æ¤œè¨¼ç”¨: {len(val_ids)}äºº ({len(val_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    print(f"   - ãƒ†ã‚¹ãƒˆç”¨: {len(test_ids)}äºº ({len(test_df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = GeneralizedCoordinateDataset(train_df, seq_len=seq_len, use_precomputed=use_precomputed)
    val_dataset = GeneralizedCoordinateDataset(val_df, seq_len=seq_len, use_precomputed=use_precomputed)
    test_dataset = GeneralizedCoordinateDataset(test_df, seq_len=seq_len, use_precomputed=use_precomputed)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†:")
    print(f"   - å­¦ç¿’ãƒãƒƒãƒæ•°: {len(train_loader)}")
    print(f"   - æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_loader)}")
    print(f"   - ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: {len(test_loader)}")
    print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")

    return train_loader, val_loader, test_loader, test_df


def analyze_generalized_coordinates(dataset: GeneralizedCoordinateDataset,
                                    num_samples: int = 5) -> None:
    """
    ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ

    Args:
        dataset: ä¸€èˆ¬åŒ–åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        num_samples: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    print(f"\nğŸ”¬ ä¸€èˆ¬åŒ–åº§æ¨™åˆ†æï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°: {num_samples}ï¼‰")
    print("=" * 60)

    coord_names = [
        'pos_x', 'pos_y', 'vel_x', 'vel_y', 'acc_x', 'acc_y', 'jerk_x', 'jerk_y',
        'speed', 'acc_magnitude', 'jerk_magnitude', 'curvature'
    ]

    all_coords = []
    subject_info = []

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for i in range(min(num_samples, len(dataset))):
        coords, subject_id, is_expert = dataset[i]
        coords_np = coords.numpy()
        all_coords.append(coords_np)
        subject_info.append((subject_id, is_expert.item()))

    if not all_coords:
        print("âŒ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    all_coords = np.stack(all_coords)  # [num_samples, seq_len, 12]

    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
    print("ğŸ“Š å„åº§æ¨™ã®çµ±è¨ˆæƒ…å ±:")
    print(f"{'åº§æ¨™å':<15} {'å¹³å‡':<12} {'æ¨™æº–åå·®':<12} {'æœ€å°å€¤':<12} {'æœ€å¤§å€¤':<12}")
    print("-" * 60)

    for i, name in enumerate(coord_names):
        coord_data = all_coords[:, :, i].flatten()
        coord_data = coord_data[coord_data != 0]  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å¤–

        if len(coord_data) > 0:
            mean_val = np.mean(coord_data)
            std_val = np.std(coord_data)
            min_val = np.min(coord_data)
            max_val = np.max(coord_data)

            print(f"{name:<15} {mean_val:<12.6f} {std_val:<12.6f} {min_val:<12.6f} {max_val:<12.6f}")

    # è¢«é¨“è€…æƒ…å ±
    print(f"\nğŸ‘¥ è¢«é¨“è€…æƒ…å ±:")
    for i, (subject_id, is_expert) in enumerate(subject_info):
        expert_status = "ç†Ÿé”è€…" if is_expert else "åˆå¿ƒè€…"
        print(f"   ã‚µãƒ³ãƒ—ãƒ« {i + 1}: è¢«é¨“è€…{subject_id} ({expert_status})")

    # åº§æ¨™é–“ã®ç›¸é–¢ï¼ˆåŸºæœ¬åº§æ¨™vsåˆæˆç‰¹å¾´ï¼‰
    print(f"\nğŸ”— åŸºæœ¬åº§æ¨™ã¨åˆæˆç‰¹å¾´ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")

    # é€Ÿåº¦ã®å¤§ãã•ã®æ•´åˆæ€§
    vel_x = all_coords[:, :, 2].flatten()
    vel_y = all_coords[:, :, 3].flatten()
    speed_computed = np.sqrt(vel_x ** 2 + vel_y ** 2)
    speed_stored = all_coords[:, :, 8].flatten()

    # éã‚¼ãƒ­è¦ç´ ã®ã¿ã§æ¯”è¼ƒ
    mask = (vel_x != 0) | (vel_y != 0)
    if mask.sum() > 0:
        speed_error = np.mean(np.abs(speed_computed[mask] - speed_stored[mask]))
        print(f"   é€Ÿåº¦å¤§ãã•èª¤å·®: {speed_error:.8f}")

    # åŠ é€Ÿåº¦ã®å¤§ãã•ã®æ•´åˆæ€§
    acc_x = all_coords[:, :, 4].flatten()
    acc_y = all_coords[:, :, 5].flatten()
    acc_mag_computed = np.sqrt(acc_x ** 2 + acc_y ** 2)
    acc_mag_stored = all_coords[:, :, 9].flatten()

    mask = (acc_x != 0) | (acc_y != 0)
    if mask.sum() > 0:
        acc_error = np.mean(np.abs(acc_mag_computed[mask] - acc_mag_stored[mask]))
        print(f"   åŠ é€Ÿåº¦å¤§ãã•èª¤å·®: {acc_error:.8f}")


def update_db(db_path: str, experiment_id: int, data: dict):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°"""
    try:
        with sqlite3.connect(db_path) as conn:
            set_clause = ", ".join([f"{key} = ?" for key in data.keys()])
            query = f"UPDATE hierarchical_experiments SET {set_clause} WHERE id = ?"
            values = tuple(data.values()) + (experiment_id,)
            conn.execute(query, values)
            conn.commit()
    except Exception as e:
        print(f"!!! DBæ›´æ–°ã‚¨ãƒ©ãƒ¼ (ID: {experiment_id}): {e} !!!")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ†ãƒ¼ãƒ–ãƒ«åã‚‚è©¦ã™
        try:
            with sqlite3.connect(db_path) as conn:
                set_clause = ", ".join([f"{key} = ?" for key in data.keys()])
                query = f"UPDATE experiments SET {set_clause} WHERE id = ?"
                values = tuple(data.values()) + (experiment_id,)
                conn.execute(query, values)
                conn.commit()
                print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: å¤ã„ãƒ†ãƒ¼ãƒ–ãƒ«åã§æ›´æ–°")
        except Exception as e2:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")


def validate_and_convert_config(config: dict) -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å¦¥å½“æ€§æ¤œè¨¼ã¨å‹å¤‰æ›ï¼ˆä¸€èˆ¬åŒ–åº§æ¨™VAEç”¨ï¼‰"""

    # å‹å¤‰æ›ãŒå¿…è¦ãªæ•°å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    numeric_conversions = {
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        'model.basic_coord_dim': int,
        'model.derived_coord_dim': int,
        'model.seq_len': int,
        'model.hidden_dim': int,
        'model.primitive_latent_dim': int,
        'model.skill_latent_dim': int,
        'model.style_latent_dim': int,
        'model.beta_primitive': float,
        'model.beta_skill': float,
        'model.beta_style': float,
        'model.physics_weight': float,
        'model.separation_weight': float,

        # å­¦ç¿’è¨­å®š
        'training.batch_size': int,
        'training.num_epochs': int,
        'training.lr': float,
        'training.weight_decay': float,
        'training.clip_grad_norm': float,
        'training.warmup_epochs': int,
        'training.scheduler_T_0': int,
        'training.scheduler_T_mult': int,
        'training.scheduler_eta_min': float,
        'training.patience': int,

        # ãƒ‡ãƒ¼ã‚¿è¨­å®š
        'data.use_precomputed': bool,
        'data.random_seed': int,

        # å€‹äººæœ€é©åŒ–è¨­å®š
        'exemplar_generation.skill_enhancement_factor': float,
        'exemplar_generation.confidence_threshold': float,
    }

    def get_nested_value(data, key_path):
        """ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã‹ã‚‰å€¤ã‚’å–å¾—"""
        keys = key_path.split('.')
        value = data
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        return value

    def set_nested_value(data, key_path, value):
        """ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã«å€¤ã‚’è¨­å®š"""
        keys = key_path.split('.')
        current = data
        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]
        current[keys[-1]] = value

    # æ•°å€¤å¤‰æ›ã®å®Ÿè¡Œ
    for key_path, target_type in numeric_conversions.items():
        current_value = get_nested_value(config, key_path)

        if current_value is not None:
            try:
                # æ–‡å­—åˆ—ã®å ´åˆã¯æ•°å€¤ã«å¤‰æ›
                if isinstance(current_value, str):
                    if target_type == float:
                        converted_value = float(current_value)
                    elif target_type == int:
                        converted_value = int(float(current_value))  # 1e3 â†’ 1000.0 â†’ 1000
                    elif target_type == bool:
                        converted_value = str(current_value).lower() in ['true', '1', 'yes']
                    else:
                        converted_value = target_type(current_value)
                elif isinstance(current_value, (int, float, bool)):
                    converted_value = target_type(current_value)
                else:
                    converted_value = current_value

                set_nested_value(config, key_path, converted_value)

                # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                if str(current_value) != str(converted_value):
                    print(
                        f"å‹å¤‰æ›: {key_path} = {current_value} ({type(current_value)}) â†’ {converted_value} ({type(converted_value)})")

            except (ValueError, TypeError) as e:
                print(f"è­¦å‘Š: {key_path}ã®å‹å¤‰æ›ã«å¤±æ•—: {current_value} â†’ {target_type.__name__} (ã‚¨ãƒ©ãƒ¼: {e})")

    # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œè¨¼
    required_sections = {
        'data': ['data_path'],
        'model': ['basic_coord_dim', 'derived_coord_dim', 'seq_len', 'hidden_dim', 'primitive_latent_dim', 'skill_latent_dim', 'style_latent_dim'],
        'training': ['batch_size', 'num_epochs', 'lr'],
        'logging': ['output_dir']
    }

    for section, keys in required_sections.items():
        if section not in config:
            raise ValueError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«'{section}'ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
        for key in keys:
            if key not in config[section]:
                raise ValueError(f"'{section}.{key}'ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
    data_path = config['data'].get('data_path', '')
    if data_path and not os.path.exists(data_path):
        print(f"è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« {data_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return config


def extract_latent_variables_generalized(model, test_loader, device):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º"""
    model.eval()
    all_z_style = []
    all_z_skill = []
    all_z_primitive = []
    all_subject_ids = []
    all_is_expert = []
    all_reconstructions = []
    all_originals = []

    with torch.no_grad():
        for full_coords, subject_ids, is_expert in tqdm(test_loader, desc="ä¸€èˆ¬åŒ–åº§æ¨™æ½œåœ¨å¤‰æ•°æŠ½å‡ºä¸­"):
            full_coords = full_coords.to(device)

            # åŸºæœ¬åº§æ¨™ã‚’æŠ½å‡ºã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            basic_coords, _ = model.split_coordinates(full_coords)
            encoded = model.encode_hierarchically(basic_coords)

            z_style = encoded['z_style']
            z_skill = encoded['z_skill']
            z_primitive = encoded['z_primitive']

            # å®Œå…¨åº§æ¨™ã§å†æ§‹æˆ
            outputs = model(full_coords, subject_ids, is_expert)
            reconstructed = outputs['reconstructed']

            all_z_style.append(z_style.cpu().numpy())
            all_z_skill.append(z_skill.cpu().numpy())
            all_z_primitive.append(z_primitive.cpu().numpy())
            all_subject_ids.extend(subject_ids)
            all_is_expert.append(is_expert.cpu().numpy())
            all_reconstructions.append(reconstructed.cpu().numpy())
            all_originals.append(full_coords.cpu().numpy())

    return {
        'z_style': np.vstack(all_z_style),
        'z_skill': np.vstack(all_z_skill),
        'z_primitive': np.vstack(all_z_primitive),
        'subject_ids': all_subject_ids,
        'is_expert': np.concatenate(all_is_expert),
        'reconstructions': np.vstack(all_reconstructions),
        'originals': np.vstack(all_originals)
    }


def run_skill_axis_analysis_generalized(model, test_loader, test_df, device):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEç”¨ã‚¹ã‚­ãƒ«è»¸åˆ†æã‚’å®Ÿè¡Œ"""
    print("=== ä¸€èˆ¬åŒ–åº§æ¨™VAE ã‚¹ã‚­ãƒ«è»¸åˆ†æé–‹å§‹ ===")

    model.eval()
    all_z_skill = []
    all_subject_ids = []

    # ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º
    with torch.no_grad():
        for full_coords, subject_ids, is_expert in test_loader:
            full_coords = full_coords.to(device)
            basic_coords, _ = model.split_coordinates(full_coords)
            encoded = model.encode_hierarchically(basic_coords)
            all_z_skill.append(encoded['z_skill'].cpu().numpy())
            all_subject_ids.extend(subject_ids)

    z_skill_data = np.vstack(all_z_skill)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’å–å¾—
    perf_cols = [col for col in test_df.columns if col.startswith('perf_')]
    if perf_cols:
        performance_df = test_df.groupby(['subject_id', 'trial_num']).first()[perf_cols].reset_index()

        # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’åˆã‚ã›ã‚‹
        min_length = min(len(z_skill_data), len(performance_df))
        z_skill_data = z_skill_data[:min_length]
        performance_df = performance_df.iloc[:min_length]

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¾æ›¸ã‚’ä½œæˆ
        performance_data = {}
        for col in perf_cols:
            metric_name = col.replace('perf_', '')
            performance_data[metric_name] = performance_df[col].values
    else:
        print("è­¦å‘Š: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§åˆ†æã—ã¾ã™ã€‚")
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
        performance_data = {
            'smoothness': np.random.randn(len(z_skill_data)),
            'efficiency': np.random.randn(len(z_skill_data)),
            'accuracy': np.random.randn(len(z_skill_data))
        }

    # ã‚¹ã‚­ãƒ«è»¸åˆ†æã‚’å®Ÿè¡Œ
    analyzer = SkillAxisAnalyzer()
    analyzer.analyze_skill_axes(z_skill_data, performance_data)

    print("ä¸€èˆ¬åŒ–åº§æ¨™VAE ã‚¹ã‚­ãƒ«è»¸åˆ†æå®Œäº†ï¼")
    return analyzer


def generate_axis_based_exemplars_generalized(model, analyzer, test_loader, device, save_path):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEç”¨è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ"""
    print("=== ä¸€èˆ¬åŒ–åº§æ¨™VAE è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ ===")

    model.eval()

    # ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    with torch.no_grad():
        for full_coords, subject_ids, is_expert in test_loader:
            full_coords = full_coords.to(device)

            # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨
            learner_coords = full_coords[[0]]
            break

    # ç•°ãªã‚‹æ”¹å–„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ãŠæ‰‹æœ¬ã‚’ç”Ÿæˆ
    target_metrics = ['overall']
    if len(analyzer.skill_improvement_directions) > 1:
        available_metrics = list(analyzer.skill_improvement_directions.keys())
        target_metrics.extend([m for m in available_metrics[:2] if m != 'overall'])

    enhancement_factor = 0.15
    generated_exemplars = {}

    with torch.no_grad():
        # ç¾åœ¨ãƒ¬ãƒ™ãƒ«
        current_exemplar = model.generate_personalized_exemplar(learner_coords, skill_enhancement_factor=0.0)
        generated_exemplars['current'] = current_exemplar.cpu().numpy().squeeze()

        # å„æŒ‡æ¨™ã§ã®æ”¹å–„
        for target in target_metrics:
            if target == 'overall':
                continue

            try:
                # ã‚«ã‚¹ã‚¿ãƒ ãŠæ‰‹æœ¬ç”Ÿæˆï¼ˆæ”¹å–„æ–¹å‘æŒ‡å®šï¼‰
                basic_coords, _ = model.split_coordinates(learner_coords)
                encoded = model.encode_hierarchically(basic_coords)
                learner_style = encoded['z_style']
                learner_skill = encoded['z_skill']

                improvement_direction = analyzer.get_improvement_direction(target)
                improvement_direction = torch.tensor(
                    improvement_direction,
                    dtype=torch.float32,
                    device=device
                ).unsqueeze(0)

                enhanced_skill = learner_skill + enhancement_factor * improvement_direction
                enhanced_basic = model.decode_hierarchically(learner_style, enhanced_skill)
                enhanced_derived = model.compute_derived_features(enhanced_basic)
                enhanced_exemplar = model.combine_coordinates(enhanced_basic, enhanced_derived)

                generated_exemplars[target] = enhanced_exemplar.cpu().numpy().squeeze()

            except Exception as e:
                print(f"{target}ã§ã®æ”¹å–„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ”¹å–„
                enhanced_exemplar = model.generate_personalized_exemplar(learner_coords,
                                                                         skill_enhancement_factor=enhancement_factor)
                generated_exemplars[target] = enhanced_exemplar.cpu().numpy().squeeze()

    # å¯è¦–åŒ–
    n_exemplars = len(generated_exemplars)
    fig, axes = plt.subplots(1, n_exemplars, figsize=(5 * n_exemplars, 5))
    if n_exemplars == 1:
        axes = [axes]

    for i, (target, coords) in enumerate(generated_exemplars.items()):
        # åŸºæœ¬åº§æ¨™ã‹ã‚‰ä½ç½®è»Œé“ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®2æ¬¡å…ƒãŒä½ç½®ï¼‰
        position_coords = coords[:, :2]

        axes[i].plot(position_coords[:, 0], position_coords[:, 1], 'b-', linewidth=2, alpha=0.8)
        axes[i].scatter(position_coords[0, 0], position_coords[0, 1], c='green', s=100, label='Start', zorder=5)
        axes[i].scatter(position_coords[-1, 0], position_coords[-1, 1], c='red', s=100, label='End', zorder=5)

        if target == 'current':
            axes[i].set_title(f'Current Level')
            axes[i].plot(position_coords[:, 0], position_coords[:, 1], 'g-', linewidth=3, alpha=0.7)
        else:
            axes[i].set_title(f'Enhanced for\n{target.replace("_", " ").title()}')

        axes[i].set_xlabel('X Position (m)')
        axes[i].set_ylabel('Y Position (m)')
        axes[i].grid(True, alpha=0.3)
        axes[i].legend()
        axes[i].set_aspect('equal')

    plt.suptitle(
        'Generalized Coordinate VAE: Axis-Based Personalized Exemplars\n(Physics-Consistent Skill Enhancement)',
        fontsize=14)
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ä¸€èˆ¬åŒ–åº§æ¨™VAE è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆå®Œäº†: {save_path}")


def quantitative_evaluation_generalized(latent_data: dict, test_df: pd.DataFrame, output_dir: str):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEç”¨ã®å®šé‡çš„è©•ä¾¡"""
    print("=== ä¸€èˆ¬åŒ–åº§æ¨™VAEå®šé‡çš„è©•ä¾¡é–‹å§‹ ===")

    # å†æ§‹æˆæ€§èƒ½
    mse = np.mean((latent_data['originals'] - latent_data['reconstructions']) ** 2)
    print(f"å†æ§‹æˆMSE: {mse:.6f}")

    # ç‰©ç†çš„æ•´åˆæ€§è©•ä¾¡
    originals = latent_data['originals']
    reconstructions = latent_data['reconstructions']

    # åŸºæœ¬åº§æ¨™ vs åˆæˆç‰¹å¾´ã®æ•´åˆæ€§
    basic_orig = originals[:, :, :8]  # åŸºæœ¬åº§æ¨™
    derived_orig = originals[:, :, 8:]  # åˆæˆç‰¹å¾´
    basic_recon = reconstructions[:, :, :8]
    derived_recon = reconstructions[:, :, 8:]

    basic_mse = np.mean((basic_orig - basic_recon) ** 2)
    derived_mse = np.mean((derived_orig - derived_recon) ** 2)

    print(f"åŸºæœ¬åº§æ¨™MSE: {basic_mse:.6f}")
    print(f"åˆæˆç‰¹å¾´MSE: {derived_mse:.6f}")
    print(f"ç‰©ç†çš„æ•´åˆæ€§æ¯”: {derived_mse / (basic_mse + 1e-8):.6f}")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡º
    perf_cols = [col for col in test_df.columns if col.startswith('perf_')]

    if perf_cols:
        performance_df = test_df.groupby(['subject_id', 'trial_num']).first()[['is_expert'] + perf_cols].reset_index()
    else:
        print("è­¦å‘Š: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        performance_df = pd.DataFrame({'is_expert': [0, 1] * (len(latent_data['z_style']) // 2)})

    # æ½œåœ¨å¤‰æ•°ã‚’DataFrameã«å¤‰æ›
    z_style_df = pd.DataFrame(
        latent_data['z_style'],
        columns=[f'z_style_{i}' for i in range(latent_data['z_style'].shape[1])]
    )
    z_skill_df = pd.DataFrame(
        latent_data['z_skill'],
        columns=[f'z_skill_{i}' for i in range(latent_data['z_skill'].shape[1])]
    )
    z_primitive_df = pd.DataFrame(
        latent_data['z_primitive'],
        columns=[f'z_primitive_{i}' for i in range(latent_data['z_primitive'].shape[1])]
    )

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    min_len = min(len(performance_df), len(z_style_df))
    analysis_df = pd.concat([
        performance_df.reset_index(drop=True).iloc[:min_len],
        z_style_df.iloc[:min_len],
        z_skill_df.iloc[:min_len],
        z_primitive_df.iloc[:min_len]
    ], axis=1)

    # éšå±¤åˆ¥ç›¸é–¢åˆ†æ
    correlations = {'style': {}, 'skill': {}, 'primitive': {}}

    if perf_cols:
        metric_names = [col.replace('perf_', '') for col in perf_cols]
        available_metrics = [metric for metric in metric_names if f'perf_{metric}' in analysis_df.columns]
    else:
        available_metrics = []

    for hierarchy in ['style', 'skill', 'primitive']:
        z_cols = [col for col in analysis_df.columns if f'z_{hierarchy}' in col]

        for metric_name in available_metrics:
            metric_col = f'perf_{metric_name}'
            correlations[hierarchy][metric_name] = []

            for z_col in z_cols:
                subset = analysis_df[[metric_col, z_col]].dropna()

                if len(subset) > 1 and subset[metric_col].std() > 1e-6 and subset[z_col].std() > 1e-6:
                    corr, p_value = pearsonr(subset[z_col], subset[metric_col])
                else:
                    corr, p_value = 0.0, 1.0

                correlations[hierarchy][metric_name].append((corr, p_value))

    # ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æ€§è©•ä¾¡
    style_ari = 0.0
    if 'subject_id' in analysis_df.columns:
        try:
            z_style_data = analysis_df[[col for col in analysis_df.columns if 'z_style' in col]].values
            subject_ids = analysis_df['subject_id'].values
            n_subjects = len(np.unique(subject_ids))

            if len(z_style_data) > n_subjects and n_subjects > 1:
                kmeans = KMeans(n_clusters=n_subjects, random_state=42)
                style_clusters = kmeans.fit_predict(z_style_data)
                subject_labels = pd.Categorical(subject_ids).codes
                style_ari = adjusted_rand_score(subject_labels, style_clusters)
        except Exception as e:
            print(f"ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")

    eval_results = {
        'reconstruction_mse': mse,
        'basic_coordinate_mse': basic_mse,
        'derived_feature_mse': derived_mse,
        'physics_consistency_ratio': derived_mse / (basic_mse + 1e-8),
        'hierarchical_correlations': correlations,
        'style_separation_score': style_ari,
        'performance_data': performance_df.to_dict('records') if perf_cols else []
    }

    print("=" * 20 + " ä¸€èˆ¬åŒ–åº§æ¨™VAEå®šé‡çš„è©•ä¾¡å®Œäº† " + "=" * 20)
    return eval_results, analysis_df

def run_generalized_evaluation(model, test_loader, test_df, output_dir, device, experiment_id):
    """æ”¹è‰¯ç‰ˆä¸€èˆ¬åŒ–åº§æ¨™VAEè©•ä¾¡ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æçµ±åˆï¼‰"""
    print("\n" + "=" * 50)
    print("æ”¹è‰¯ç‰ˆä¸€èˆ¬åŒ–åº§æ¨™VAEè©•ä¾¡é–‹å§‹ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æä»˜ãï¼‰")
    print("=" * 50)

    # 1. ã‚¹ã‚­ãƒ«è»¸åˆ†æ
    analyzer = run_skill_axis_analysis_generalized(model, test_loader, test_df, device)

    # 2. éšå±¤æ½œåœ¨å¤‰æ•°æŠ½å‡º
    latent_data = extract_latent_variables_generalized(model, test_loader, device)

    # 3. å®šé‡çš„è©•ä¾¡
    eval_results, analysis_df = quantitative_evaluation_generalized(latent_data, test_df, output_dir)

    # 4. è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ
    exemplar_v2_path = os.path.join(output_dir, 'plots', f'generalized_axis_based_exemplars_exp{experiment_id}.png')
    generate_axis_based_exemplars_generalized(model, analyzer, test_loader, device, exemplar_v2_path)

    # 5. çµæœçµ±åˆ
    eval_results.update({
        'skill_axis_analysis_completed': True,
        'axis_based_exemplars_path': exemplar_v2_path,
        'skill_improvement_directions_available': list(analyzer.skill_improvement_directions.keys()),
        'best_skill_correlations': {k: v['correlation'] for k, v in analyzer.performance_correlations.items()}
    })

    # 6. çµæœä¿å­˜
    eval_results_path = os.path.join(output_dir, 'results', f'generalized_evaluation_v2_exp{experiment_id}.json')
    os.makedirs(os.path.dirname(eval_results_path), exist_ok=True)

    def convert_to_serializable(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        else:
            return obj

    serializable_results = convert_to_serializable(eval_results)

    with open(eval_results_path, 'w') as f:
        json.dump(serializable_results, f, indent=2)

    eval_results['evaluation_results_path'] = eval_results_path

    print("=" * 50)
    print("æ”¹è‰¯ç‰ˆä¸€èˆ¬åŒ–åº§æ¨™VAEè©•ä¾¡å®Œäº†")
    print("=" * 50)

    return eval_results


def setup_directories(output_dir):
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    dirs = [
        output_dir,
        os.path.join(output_dir, 'checkpoints'),
        os.path.join(output_dir, 'plots'),
        os.path.join(output_dir, 'logs'),
        os.path.join(output_dir, 'results')
    ]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)


def plot_generalized_training_curves(history, save_path):
    """ä¸€èˆ¬åŒ–åº§æ¨™VAEã®å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))

    # å…¨ä½“çš„ãªæå¤±
    axes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0, 0].plot(history['val_loss'], label='Validation Loss', color='red')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Total Loss')
    axes[0, 0].set_title('Total Training & Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    axes[0, 0].set_yscale('log')

    # ç‰©ç†åˆ¶ç´„æå¤±
    axes[0, 1].plot(history['physics_loss'], label='Physics Loss', color='green')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Physics Loss')
    axes[0, 1].set_title('Physics Constraint Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    axes[0, 1].set_yscale('log')

    # åŸºæœ¬åº§æ¨™å†æ§‹æˆæå¤±
    axes[0, 2].plot(history['basic_reconstruction_loss'], label='Basic Recon Loss', color='orange')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Basic Reconstruction Loss')
    axes[0, 2].set_title('Basic Coordinate Reconstruction')
    axes[0, 2].legend()
    axes[0, 2].grid(True)
    axes[0, 2].set_yscale('log')

    # åˆæˆç‰¹å¾´æ•´åˆæ€§æå¤±
    axes[1, 0].plot(history['derived_consistency_loss'], label='Derived Consistency', color='purple')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Derived Feature Consistency')
    axes[1, 0].set_title('Derived Feature Consistency')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    axes[1, 0].set_yscale('log')

    # KLæå¤±
    axes[1, 1].plot(history['kl_loss'], label='Total KL Loss', color='brown')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('KL Divergence')
    axes[1, 1].set_title('KL Divergence Loss')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    axes[1, 1].set_yscale('log')

    # åˆ†é›¢æå¤±
    axes[1, 2].plot(history['separation_loss'], label='Separation Loss', color='pink')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Style-Skill Separation')
    axes[1, 2].set_title('Style-Skill Separation Loss')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    axes[1, 2].set_yscale('log')

    # å­¦ç¿’ç‡
    axes[2, 0].plot(history.get('learning_rates', []), color='red')
    axes[2, 0].set_xlabel('Epoch')
    axes[2, 0].set_ylabel('Learning Rate')
    axes[2, 0].set_title('Learning Rate Schedule')
    axes[2, 0].grid(True)
    axes[2, 0].set_yscale('log')

    # æå¤±æ¯”è¼ƒ
    if len(history['train_loss']) > 1:
        axes[2, 1].plot(history['basic_reconstruction_loss'], label='Basic Recon', alpha=0.7)
        axes[2, 1].plot(history['derived_consistency_loss'], label='Derived Consistency', alpha=0.7)
        axes[2, 1].plot(history['physics_loss'], label='Physics', alpha=0.7)
        axes[2, 1].set_xlabel('Epoch')
        axes[2, 1].set_ylabel('Loss Components')
        axes[2, 1].set_title('Loss Component Comparison')
        axes[2, 1].legend()
        axes[2, 1].grid(True)
        axes[2, 1].set_yscale('log')

    # ç‰©ç†çš„æ•´åˆæ€§æ¯”
    if 'physics_consistency_ratio' in history:
        axes[2, 2].plot(history['physics_consistency_ratio'], label='Consistency Ratio', color='navy')
        axes[2, 2].set_xlabel('Epoch')
        axes[2, 2].set_ylabel('Derived/Basic Loss Ratio')
        axes[2, 2].set_title('Physics Consistency Ratio')
        axes[2, 2].legend()
        axes[2, 2].grid(True)

    plt.suptitle('Generalized Coordinate Hierarchical VAE Training Curves', fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ä¸€èˆ¬åŒ–åº§æ¨™VAEå­¦ç¿’æ›²ç·šã‚’ä¿å­˜: {save_path}")


def train_generalized_model(config_path: str, experiment_id: int, db_path: str):
    """å®Œå…¨ç‰ˆä¸€èˆ¬åŒ–åº§æ¨™VAEå­¦ç¿’ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æçµ±åˆï¼‰"""
    print("=== ä¸€èˆ¬åŒ–åº§æ¨™VAEå­¦ç¿’é–‹å§‹ ===")

    # 1. è¨­å®šèª­ã¿è¾¼ã¿
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    try:
        config = validate_and_convert_config(config)
        print("ä¸€èˆ¬åŒ–åº§æ¨™VAEè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼: âœ… æ­£å¸¸")
    except ValueError as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        update_db(db_path, experiment_id, {
            'status': 'failed',
            'end_time': datetime.now().isoformat(),
            'notes': f"Config validation error: {str(e)}"
        })
        raise e

    # DBã«å­¦ç¿’é–‹å§‹ã‚’é€šçŸ¥
    update_db(db_path, experiment_id, {
        'status': 'running',
        'start_time': datetime.now().isoformat()
    })

    try:
        # 2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        output_dir = config['logging']['output_dir']
        setup_directories(output_dir)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"-- ä¸€èˆ¬åŒ–åº§æ¨™VAEå®Ÿé¨“ID: {experiment_id} | ãƒ‡ãƒã‚¤ã‚¹: {device} ---")

        # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        try:
            train_loader, val_loader, test_loader, test_df = create_generalized_dataloaders(
                master_data_path=config['data']['data_path'],
                seq_len=config['model']['seq_len'],
                batch_size=config['training']['batch_size'],
                use_precomputed=config['data'].get('use_precomputed', True),
                random_seed=config['data'].get('random_seed', 42)
            )
            if train_loader is None:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            update_db(db_path, experiment_id, {
                'status': 'failed',
                'end_time': datetime.now().isoformat()
            })
            raise e

        # 4. ä¸€èˆ¬åŒ–åº§æ¨™VAEãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        model_config = {
            'basic_coord_dim': config['model']['basic_coord_dim'],
            'derived_coord_dim': config['model']['derived_coord_dim'],
            'seq_len': config['model']['seq_len'],
            'hidden_dim': config['model']['hidden_dim'],
            'latent_dims': {
                'primitive': config['model']['primitive_latent_dim'],
                'skill': config['model']['skill_latent_dim'],
                'style': config['model']['style_latent_dim']
            },
            'beta_weights': {
                'primitive': config['model'].get('beta_primitive', 1.0),
                'skill': config['model'].get('beta_skill', 2.0),
                'style': config['model'].get('beta_style', 4.0)
            },
            'physics_weight': config['model'].get('physics_weight', 0.1),
            'separation_weight': config['model'].get('separation_weight', 0.5)
        }

        model = GeneralizedCoordinateHierarchicalVAE(**model_config).to(device)

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        optimizer = optim.AdamW(
            model.parameters(),
            config['training']['lr'],
            weight_decay=config['training'].get('weight_decay', 1e-5)
        )

        main_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=config['training'].get('scheduler_T_0', 15),
            T_mult=config['training'].get('scheduler_T_mult', 2),
            eta_min=config['training'].get('scheduler_eta_min', 1e-6)
        )
        warmup_scheduler = optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.1,
            total_iters=config['training'].get('warmup_epochs', 10)
        )
        scheduler = ChainedScheduler([warmup_scheduler, main_scheduler])

        # 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        best_val_loss = float('inf')
        epochs_no_improve = 0
        patience = config['training'].get('patience', 20)
        history = {
            'train_loss': [], 'val_loss': [], 'learning_rates': [],
            'physics_loss': [], 'basic_reconstruction_loss': [], 'derived_consistency_loss': [],
            'kl_loss': [], 'separation_loss': [], 'physics_consistency_ratio': []
        }

        print(f"å­¦ç¿’é–‹å§‹: {config['training']['num_epochs']}ã‚¨ãƒãƒƒã‚¯, patience={patience}")

        for epoch in range(config['training']['num_epochs']):
            model.train()

            epoch_losses = {
                'total': [], 'physics': [], 'basic_recon': [], 'derived_consistency': [],
                'kl': [], 'separation': []
            }

            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{config["training"]["num_epochs"]} [Train]')
            for full_coords, subject_ids, skill_levels in progress_bar:
                full_coords = full_coords.to(device)
                skill_levels = skill_levels.to(device)

                optimizer.zero_grad()
                outputs = model(full_coords, subject_ids, skill_levels)

                loss = outputs['total_loss']
                loss.backward()

                torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    config['training'].get('clip_grad_norm', 1.0)
                )
                optimizer.step()

                # æå¤±è¨˜éŒ²
                individual_losses = outputs['individual_losses']
                epoch_losses['total'].append(outputs['total_loss'].item())

                # ç‰©ç†åˆ¶ç´„æå¤±
                physics_loss = (
                        individual_losses['physics_physics_consistency'] +
                        individual_losses['physics_smoothness'] +
                        individual_losses['physics_energy_conservation']
                ).item()
                epoch_losses['physics'].append(physics_loss)

                # åŸºæœ¬åº§æ¨™å†æ§‹æˆæå¤±
                basic_coords, _ = model.split_coordinates(full_coords)
                basic_recon_loss = torch.nn.functional.mse_loss(
                    outputs['reconstructed_basic'], basic_coords
                ).item()
                epoch_losses['basic_recon'].append(basic_recon_loss)

                # åˆæˆç‰¹å¾´æ•´åˆæ€§æå¤±
                _, target_derived = model.split_coordinates(full_coords)
                derived_consistency_loss = torch.nn.functional.mse_loss(
                    outputs['reconstructed_derived'], target_derived
                ).item()
                epoch_losses['derived_consistency'].append(derived_consistency_loss)

                # KLæå¤±
                kl_loss = (
                        individual_losses['kl_primitive'] +
                        individual_losses['kl_skill'] +
                        individual_losses['kl_style']
                ).item()
                epoch_losses['kl'].append(kl_loss)

                # åˆ†é›¢æå¤±
                sep_loss = 0.0
                for key in ['separation_style_clustering', 'separation_skill_structure', 'separation_orthogonality']:
                    if key in individual_losses:
                        sep_loss += individual_losses[key].item()
                epoch_losses['separation'].append(sep_loss)

                progress_bar.set_postfix({'Loss': np.mean(epoch_losses['total'])})

            # ã‚¨ãƒãƒƒã‚¯æå¤±è¨˜éŒ²
            history['train_loss'].append(np.mean(epoch_losses['total']))
            history['physics_loss'].append(np.mean(epoch_losses['physics']))
            history['basic_reconstruction_loss'].append(np.mean(epoch_losses['basic_recon']))
            history['derived_consistency_loss'].append(np.mean(epoch_losses['derived_consistency']))
            history['kl_loss'].append(np.mean(epoch_losses['kl']))
            history['separation_loss'].append(np.mean(epoch_losses['separation']))
            history['learning_rates'].append(optimizer.param_groups[0]['lr'])

            # ç‰©ç†çš„æ•´åˆæ€§æ¯”
            basic_loss_avg = np.mean(epoch_losses['basic_recon'])
            derived_loss_avg = np.mean(epoch_losses['derived_consistency'])
            consistency_ratio = derived_loss_avg / (basic_loss_avg + 1e-8)
            history['physics_consistency_ratio'].append(consistency_ratio)

            # æ¤œè¨¼ãƒ«ãƒ¼ãƒ—
            model.eval()
            epoch_val_losses = []
            with torch.no_grad():
                for full_coords, subject_ids, skill_levels in val_loader:
                    full_coords = full_coords.to(device)
                    skill_levels = skill_levels.to(device)
                    outputs = model(full_coords, subject_ids, skill_levels)
                    epoch_val_losses.append(outputs['total_loss'].item())

            current_val_loss = np.mean(epoch_val_losses)
            history['val_loss'].append(current_val_loss)

            print(f"Epoch {epoch + 1}: Train Loss: {history['train_loss'][-1]:.4f}, "
                  f"Val Loss: {current_val_loss:.4f}")
            print(f"  Physics: {history['physics_loss'][-1]:.4f}, "
                  f"Basic Recon: {history['basic_reconstruction_loss'][-1]:.4f}, "
                  f"Derived Consistency: {history['derived_consistency_loss'][-1]:.6f}")
            print(f"  KL: {history['kl_loss'][-1]:.4f}, "
                  f"Separation: {history['separation_loss'][-1]:.4f}, "
                  f"Consistency Ratio: {consistency_ratio:.6f}")

            # å­¦ç¿’ç‡æ›´æ–°
            scheduler.step()

            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ & ã‚¢ãƒ¼ãƒªãƒ¼ã‚¹ãƒˆãƒƒãƒ—
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                epochs_no_improve = 0
                best_model_path = os.path.join(output_dir, 'checkpoints',
                                               f'best_generalized_model_exp{experiment_id}.pth')
                model.save_model(best_model_path)
                print(f" -> New best model saved! (Loss: {best_val_loss:.4f})")
            else:
                epochs_no_improve += 1

            if epochs_no_improve > patience:
                print(f"\nEarly stopping triggered after {patience} epochs with no improvement.")
                break

        # 6. å­¦ç¿’çµ‚äº†å¾Œã®å‡¦ç†
        final_model_path = os.path.join(output_dir, 'checkpoints', f'final_generalized_model_exp{experiment_id}.pth')
        model.save_model(final_model_path)

        plot_path = os.path.join(output_dir, 'plots', f'generalized_training_curves_exp{experiment_id}.png')
        plot_generalized_training_curves(history, plot_path)

        print("=== å­¦ç¿’å®Œäº†ã€è©•ä¾¡é–‹å§‹ ===")

        # 7. æ”¹è‰¯ç‰ˆè©•ä¾¡å®Ÿè¡Œ
        try:
            eval_results = run_generalized_evaluation(model, test_loader, test_df, output_dir, device, experiment_id)

            # æœ€é«˜ç›¸é–¢å€¤ã‚’è¨ˆç®—
            best_correlation = 0.0
            best_correlation_metric = 'none'
            if 'best_skill_correlations' in eval_results and eval_results['best_skill_correlations']:
                for metric, corr in eval_results['best_skill_correlations'].items():
                    if abs(corr) > abs(best_correlation):
                        best_correlation = corr
                        best_correlation_metric = metric

            # è©•ä¾¡çµæœã‚’DBã«è¨˜éŒ²
            update_db(db_path, experiment_id, {
                'status': 'completed',
                'end_time': datetime.now().isoformat(),
                'final_total_loss': history['train_loss'][-1],
                'best_val_loss': best_val_loss,
                'final_epoch': len(history['train_loss']),
                'early_stopped': epochs_no_improve > patience,

                # ä¸€èˆ¬åŒ–åº§æ¨™VAEç‰¹æœ‰ã®æŒ‡æ¨™
                'final_physics_loss': history['physics_loss'][-1],
                'final_basic_reconstruction_loss': history['basic_reconstruction_loss'][-1],
                'final_derived_consistency_loss': history['derived_consistency_loss'][-1],
                'final_physics_consistency_ratio': history['physics_consistency_ratio'][-1],

                # è©•ä¾¡æŒ‡æ¨™
                'reconstruction_mse': eval_results['reconstruction_mse'],
                'basic_coordinate_mse': eval_results['basic_coordinate_mse'],
                'derived_feature_mse': eval_results['derived_feature_mse'],
                'physics_consistency_ratio': eval_results['physics_consistency_ratio'],
                'style_separation_score': eval_results.get('style_separation_score', 0.0),
                'skill_performance_correlation': best_correlation,
                'best_skill_correlation_metric': best_correlation_metric,

                # ã‚¹ã‚­ãƒ«è»¸åˆ†æçµæœ
                'skill_axis_analysis_completed': eval_results.get('skill_axis_analysis_completed', False),
                'skill_improvement_directions_count': len(
                    eval_results.get('skill_improvement_directions_available', [])),
                'axis_based_improvement_enabled': True,

                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                'model_path': final_model_path,
                'best_model_path': best_model_path if 'best_model_path' in locals() else final_model_path,
                'training_curves_path': plot_path,
                'axis_based_exemplars_path': eval_results.get('axis_based_exemplars_path'),
                'evaluation_results_path': eval_results.get('evaluation_results_path'),

                'notes': f"ä¸€èˆ¬åŒ–åº§æ¨™VAE - MSE: {eval_results['reconstruction_mse']:.6f}, "
                         f"Physics Ratio: {eval_results['physics_consistency_ratio']:.6f}, "
                         f"Style ARI: {eval_results.get('style_separation_score', 0.0):.4f}, "
                         f"Best Corr: {best_correlation:.4f} ({best_correlation_metric})"
            })

            print(f"=== å®Ÿé¨“ID: {experiment_id} æ­£å¸¸å®Œäº† ===")
            print(f"æœ€çµ‚çµæœ:")
            print(f"  å†æ§‹æˆMSE: {eval_results['reconstruction_mse']:.6f}")
            print(f"  ç‰©ç†çš„æ•´åˆæ€§æ¯”: {eval_results['physics_consistency_ratio']:.6f}")
            print(f"  ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢ARI: {eval_results.get('style_separation_score', 0.0):.4f}")
            print(f"  æœ€é«˜ã‚¹ã‚­ãƒ«ç›¸é–¢: {best_correlation:.4f} ({best_correlation_metric})")

        except Exception as eval_error:
            print(f"è©•ä¾¡æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {eval_error}")
            import traceback
            traceback.print_exc()

            update_db(db_path, experiment_id, {
                'status': 'completed',
                'end_time': datetime.now().isoformat(),
                'final_total_loss': history['train_loss'][-1],
                'best_val_loss': best_val_loss,
                'model_path': final_model_path,
                'training_curves_path': plot_path,
                'notes': f"å­¦ç¿’å®Œäº†ã€ä½†ã—è©•ä¾¡å¤±æ•—: {str(eval_error)}"
            })

    except Exception as e:
        print(f"!!! ä¸€èˆ¬åŒ–åº§æ¨™VAEå®Ÿé¨“ID: {experiment_id} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e} !!!")
        import traceback
        traceback.print_exc()

        update_db(db_path, experiment_id, {
            'status': 'failed',
            'end_time': datetime.now().isoformat(),
            'notes': f"å­¦ç¿’ä¸­ã‚¨ãƒ©ãƒ¼: {str(e)}"
        })
        raise e

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å®Œå…¨ç‰ˆä¸€èˆ¬åŒ–åº§æ¨™VAEå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument('--config', type=str, required=True, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--experiment_id', type=int, required=True, help='å®Ÿé¨“ç®¡ç†DBã®ID')
    parser.add_argument('--db_path', type=str, required=True, help='å®Ÿé¨“ç®¡ç†DBã®ãƒ‘ã‚¹')

    args = parser.parse_args()

    print(f"éšå±¤å‹VAEå­¦ç¿’é–‹å§‹:")
    print(f"  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {args.config}")
    print(f"  å®Ÿé¨“ID: {args.experiment_id}")
    print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {args.db_path}")

    train_generalized_model(config_path=args.config, experiment_id=args.experiment_id, db_path=args.db_path)