import os
import sys
import argparse
import warnings
import numpy as np
import pandas as pd
import torch
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import ChainedScheduler
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime
import yaml
import sqlite3
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import adjusted_rand_score
from sklearn.cluster import KMeans
from scipy.stats import pearsonr
import seaborn as sns
import json
import joblib

# scikit-learnã®ç‰¹å®šã®UserWarningã‚’ç„¡è¦–ã™ã‚‹
warnings.filterwarnings("ignore", category=UserWarning, module='sklearn')

# ===== numpyå‹ã®SQLiteè‡ªå‹•å¤‰æ›ã‚’ç™»éŒ² =====
sqlite3.register_adapter(np.float32, float)
sqlite3.register_adapter(np.float64, float)
sqlite3.register_adapter(np.int32, int)
sqlite3.register_adapter(np.int64, int)
sqlite3.register_adapter(np.bool_, bool)

# --- ãƒ‘ã‚¹è¨­å®šã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)
from models.beta_vae_generalized_coordinate import BetaVAEGeneralizedCoordinate

class SkillAxisAnalyzer:
    """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã§ã®ä¸Šæ‰‹ã•ã®è»¸ã‚’åˆ†æãƒ»æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.skill_improvement_directions = {}
        self.performance_correlations = {}

    def analyze_skill_axes(self, z_skill_data, performance_data):
        """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®ç›¸é–¢åˆ†æ"""
        print("=== ã‚¹ã‚­ãƒ«è»¸åˆ†æé–‹å§‹ ===")

        skill_dim = z_skill_data.shape[1]

        # å„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã¨ã®ç›¸é–¢åˆ†æ
        for metric_name, metric_values in performance_data.items():
            if len(metric_values) == 0:
                continue

            correlations = []

            for dim in range(skill_dim):
                if len(set(metric_values)) > 1 and np.std(metric_values) > 1e-6:
                    try:
                        corr, p_value = pearsonr(z_skill_data[:, dim], metric_values)
                        correlations.append((corr, p_value, dim))
                    except:
                        correlations.append((0.0, 1.0, dim))
                else:
                    correlations.append((0.0, 1.0, dim))

            correlations.sort(key=lambda x: abs(x[0]), reverse=True)
            best_corr, best_p, best_dim = correlations[0]

            self.performance_correlations[metric_name] = {
                'best_dimension': best_dim,
                'correlation': best_corr,
                'p_value': best_p,
                'all_correlations': correlations
            }

            print(f"{metric_name}: æœ€å¼·ç›¸é–¢æ¬¡å…ƒ={best_dim}, r={best_corr:.4f}, p={best_p:.4f}")

        # ç·åˆçš„ãªä¸Šæ‰‹ã•è»¸ã®æŠ½å‡º
        self._extract_overall_skill_axis(z_skill_data, performance_data)

        # å€‹åˆ¥æŒ‡æ¨™ã®æ”¹å–„æ–¹å‘
        self._extract_specific_improvement_directions(z_skill_data, performance_data)

    def _extract_overall_skill_axis(self, z_skill_data, performance_data):
        """ç·åˆçš„ãªã‚¹ã‚­ãƒ«å‘ä¸Šè»¸ã‚’æŠ½å‡º"""
        print("\n--- ç·åˆã‚¹ã‚­ãƒ«è»¸ã®æŠ½å‡º ---")

        normalized_metrics = {}

        for metric_name, values in performance_data.items():
            values = np.array(values)

            if len(values) == 0 or np.std(values) < 1e-6:
                continue

            # æŒ‡æ¨™ã®æ–¹å‘æ€§ã‚’çµ±ä¸€
            if metric_name in ['trial_time', 'trial_error', 'jerk', 'trial_variability']:
                normalized_values = -values
            else:
                normalized_values = values

            # æ¨™æº–åŒ–
            if np.std(normalized_values) > 1e-6:
                normalized_values = (normalized_values - normalized_values.mean()) / normalized_values.std()
                normalized_metrics[metric_name] = normalized_values

        if len(normalized_metrics) == 0:
            print("è­¦å‘Š: æœ‰åŠ¹ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢
        overall_skill_score = np.zeros(len(z_skill_data))
        weight_sum = 0

        for metric_name, normalized_values in normalized_metrics.items():
            overall_skill_score += normalized_values
            weight_sum += 1

        if weight_sum > 0:
            overall_skill_score /= weight_sum

        # ç·šå½¢å›å¸°
        try:
            reg = LinearRegression()
            reg.fit(z_skill_data, overall_skill_score)

            overall_improvement_direction = reg.coef_
            improvement_magnitude = np.linalg.norm(overall_improvement_direction)

            if improvement_magnitude > 1e-6:
                overall_improvement_direction = overall_improvement_direction / improvement_magnitude

                self.skill_improvement_directions['overall'] = {
                    'direction': overall_improvement_direction,
                    'r_squared': reg.score(z_skill_data, overall_skill_score),
                    'coefficients': reg.coef_
                }

                print(f"ç·åˆã‚¹ã‚­ãƒ«è»¸: RÂ²={reg.score(z_skill_data, overall_skill_score):.4f}")
            else:
                print("è­¦å‘Š: ç·åˆæ”¹å–„æ–¹å‘ã®è¨ˆç®—ã«å¤±æ•—")
        except Exception as e:
            print(f"ç·åˆã‚¹ã‚­ãƒ«è»¸æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    def _extract_specific_improvement_directions(self, z_skill_data, performance_data):
        """å€‹åˆ¥æŒ‡æ¨™ã®æ”¹å–„æ–¹å‘ã‚’æŠ½å‡º"""
        print("\n--- å€‹åˆ¥æŒ‡æ¨™æ”¹å–„æ–¹å‘ã®æŠ½å‡º ---")

        for metric_name, values in performance_data.items():
            values = np.array(values)

            if len(values) == 0 or np.std(values) < 1e-6:
                continue

            # æ–¹å‘æ€§ã‚’çµ±ä¸€
            if metric_name in ['trial_time', 'trial_error', 'jerk', 'trial_variability']:
                target_values = -values
            else:
                target_values = values

            try:
                reg = LinearRegression()
                reg.fit(z_skill_data, target_values)

                improvement_direction = reg.coef_
                improvement_magnitude = np.linalg.norm(improvement_direction)

                if improvement_magnitude > 1e-6:
                    improvement_direction = improvement_direction / improvement_magnitude

                    self.skill_improvement_directions[metric_name] = {
                        'direction': improvement_direction,
                        'r_squared': reg.score(z_skill_data, target_values),
                        'coefficients': reg.coef_
                    }

                    print(f"{metric_name}: RÂ²={reg.score(z_skill_data, target_values):.4f}")
            except Exception as e:
                print(f"{metric_name}ã®æ”¹å–„æ–¹å‘æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    def get_improvement_direction(self, metric='overall', confidence_threshold=0.1):
        """æŒ‡å®šã•ã‚ŒãŸæŒ‡æ¨™ã®æ”¹å–„æ–¹å‘ã‚’å–å¾—"""
        if metric not in self.skill_improvement_directions:
            print(f"è­¦å‘Š: {metric}ã®æ”¹å–„æ–¹å‘ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            if 'overall' in self.skill_improvement_directions:
                print("ç·åˆæ–¹å‘ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                metric = 'overall'
            else:
                raise ValueError("åˆ©ç”¨å¯èƒ½ãªæ”¹å–„æ–¹å‘ãŒã‚ã‚Šã¾ã›ã‚“")

        direction_info = self.skill_improvement_directions[metric]

        if direction_info['r_squared'] < confidence_threshold:
            print(f"è­¦å‘Š: {metric}ã®RÂ²={direction_info['r_squared']:.4f}ãŒé–¾å€¤{confidence_threshold}ã‚’ä¸‹å›ã‚Šã¾ã™")

        return direction_info['direction']

class GeneralizedCoordinateDataset(Dataset):
    def __init__(self, df: pd.DataFrame, scalers: dict,feature_cols: list, seq_len: int = 100):
        self.seq_len = seq_len
        self.scalers = scalers
        self.feature_cols = feature_cols

        # ãƒ‡ãƒ¼ã‚¿ã‚’è©¦è¡Œï¼ˆtrialï¼‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ãƒªã‚¹ãƒˆã«å¤‰æ›
        self.trials = list(df.groupby(['subject_id', 'trial_num']))
        print(f"Dataset initialized with {len(self.trials)} trials")
        print(f"Feature columns: {self.feature_cols}")
        print(f"Available scalers: {list(self.scalers.keys())}")

    def __len__(self) -> int:
        return len(self.trials)

    def __getitem__(self, idx) -> tuple:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

        :return: tuple(è»Œé“ãƒ†ãƒ³ã‚½ãƒ«, è¢«é¨“è€…ID, ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«)
        """
        # 1. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹è©¦è¡Œãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        (subject_id, _), trial_df = self.trials[idx]

        # 2. ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’Numpyé…åˆ—ã«å¤‰æ›
        features = trial_df[self.feature_cols].values

        # 3. ç‰©ç†é‡åˆ¥ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨
        scaled_features = apply_physics_based_scaling(features, self.scalers, self.feature_cols)

        # 4. å›ºå®šé•·åŒ–
        if len(scaled_features) > self.seq_len:
            processed_trajectory = scaled_features[:self.seq_len]
        else:
            # æœ€å¾Œã®å€¤ã‚’ç¹°ã‚Šè¿”ã—
            padding_length = self.seq_len - len(scaled_features)
            last_value = scaled_features[-1:].repeat(padding_length, axis=0)
            processed_trajectory = np.vstack([scaled_features, last_value])

        # 5. ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        is_expert = trial_df['is_expert'].iloc[0]

        # 5. ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦è¿”ã™
        return (
            torch.tensor(processed_trajectory, dtype=torch.float32),
            subject_id,
            torch.tensor(is_expert, dtype=torch.long)
        )


def create_dataloaders(processed_data_dir: str, seq_len: int, batch_size: int, random_seed: int = 42) -> tuple:
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    train_data_path = os.path.join(processed_data_dir, 'train_data.parquet')
    test_data_path = os.path.join(processed_data_dir, 'test_data.parquet')
    scalers_path = os.path.join(processed_data_dir, 'scalers.joblib')
    feature_config_path = os.path.join(processed_data_dir, 'feature_config.joblib')

    try:
        # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’èª­ã¿è¾¼ã¿
        train_val_df = pd.read_parquet(train_data_path)
        test_df = pd.read_parquet(test_data_path)
        scalers = joblib.load(scalers_path)
        feature_config = joblib.load(feature_config_path)
        feature_cols = feature_config['feature_cols']

        print(f"Loaded scalers: {list(scalers.keys())}")
        print(f"Feature columns: {feature_cols}")

    except FileNotFoundError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚-> {e}")
        return None, None, None, None

    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰² ---
    train_val_subject_ids = train_val_df['subject_id'].unique()

    if len(train_val_subject_ids) < 2:
        # æœ€ä½ã§ã‚‚1äººãŒå­¦ç¿’ã€1äººãŒæ¤œè¨¼ã«å¿…è¦
        print("è­¦å‘Š: æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¢«é¨“è€…ãŒ2äººä»¥ä¸Šå¿…è¦ã§ã™ã€‚æ¤œè¨¼ã‚»ãƒƒãƒˆãªã—ã§é€²ã‚ã¾ã™ã€‚")
        train_ids = train_val_subject_ids
        val_ids = []
    else:
        train_ids, val_ids = train_test_split(
            train_val_subject_ids,
            test_size=0.25,  # ä¾‹ãˆã°å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å†…ã®25%ã‚’æ¤œè¨¼ç”¨ã«ã™ã‚‹ (4äººã„ãŸã‚‰3äººå­¦ç¿’, 1äººæ¤œè¨¼)
            random_state=random_seed
        )

    train_df = train_val_df[train_val_df['subject_id'].isin(train_ids)]
    val_df = train_val_df[train_val_df['subject_id'].isin(val_ids)]

    print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨={len(train_ids)}äºº, æ¤œè¨¼ç”¨={len(val_ids)}äºº, ãƒ†ã‚¹ãƒˆç”¨={len(test_df['subject_id'].unique())}äºº")

    # --- 4. Datasetã®ä½œæˆ (scalerã‚’æ¸¡ã™) ---
    train_dataset = GeneralizedCoordinateDataset(train_df, scalers, feature_cols ,seq_len)
    val_dataset = GeneralizedCoordinateDataset(val_df, scalers, feature_cols, seq_len) if not val_df.empty else None
    test_dataset = GeneralizedCoordinateDataset(test_df, scalers,feature_cols, seq_len)

    # --- 5. DataLoaderã®ä½œæˆ ---
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) if val_dataset else None
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader, test_df

def apply_physics_based_scaling(data: np.ndarray, scalers: dict, feature_cols: list = None) -> np.ndarray:
    """ç‰©ç†é‡åˆ¥ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨"""

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç‰¹å¾´é‡é †åº
    if feature_cols is None:
        feature_cols = ['HandlePosDiffX', 'HandlePosDiffY', 'HandleVelX', 'HandleVelY',
                        'HandleAccX', 'HandleAccY']

    if data.shape[1] != len(feature_cols):
        raise ValueError(f"Expected {len(feature_cols)} features, got {data.shape[1]}")

    scaled_data = np.zeros_like(data)

    # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    feature_type_map = {
        'HandlePosDiffX': 'position_diff',
        'HandlePosDiffY': 'position_diff',
        'HandleVelDiffX': 'velocity_diff',
        'HandleVelDiffY': 'velocity_diff',
        'HandleAccDiffX': 'acceleration_diff',
        'HandleAccDiffY': 'acceleration_diff',
        'HandlePosX': 'position',
        'HandlePosY': 'position',
        'HandleVelX': 'velocity',
        'HandleVelY': 'velocity',
        'HandleAccX': 'acceleration',
        'HandleAccY': 'acceleration',
        'JerkX': 'jerk',
        'JerkY': 'jerk'
    }

    # å„ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    type_indices = {}
    for i, col in enumerate(feature_cols):
        feature_type = feature_type_map.get(col, 'unknown')
        if feature_type not in type_indices:
            type_indices[feature_type] = []
        type_indices[feature_type].append(i)

    # ã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨
    for feature_type, indices in type_indices.items():
        if feature_type in scalers:
            scaled_data[:, indices] = scalers[feature_type].transform(data[:, indices])
            # print(f"Applied {feature_type} scaling to indices {indices}")
        else:
            print(f"Warning: No scaler found for {feature_type}, using original data")
            scaled_data[:, indices] = data[:, indices]

    return scaled_data

def update_db(db_path: str, experiment_id: int, data: dict):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æŒ‡å®šã•ã‚ŒãŸå®Ÿé¨“IDã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã™ã‚‹"""
    try:
        with sqlite3.connect(db_path) as conn:
            set_clause = ", ".join([f"{key} = ?" for key in data.keys()])
            query = f"UPDATE beta_vae_generalized_coordinate_experiments SET {set_clause} WHERE id = ?"
            values = tuple(data.values()) + (experiment_id, )
            conn.execute(query, values)
            conn.commit()
    except Exception as e:
        print(f"!!! DBæ›´æ–°ã‚¨ãƒ©ãƒ¼ (ID: {experiment_id}): {e} !!!")

def validate_and_convert_config(config: dict) -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å¦¥å½“æ€§æ¤œè¨¼ã¨å‹å¤‰æ›"""

    # å‹å¤‰æ›ãŒå¿…è¦ãªæ•°å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    numeric_conversions = {
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        'model.input_dim': int,
        'model.seq_len': int,
        'model.hidden_dim': int,
        'model.style_latent_dim': int,
        'model.skill_latent_dim': int,
        'model.beta': float,
        'model.n_layers': int,
        'model.contrastive_weight': float,
        'model.use_triplet': bool,

        # å­¦ç¿’è¨­å®š
        'training.batch_size': int,
        'training.num_epochs': int,
        'training.lr': float,
        'training.weight_decay': float,
        'training.clip_grad_norm': float,
        'training.warmup_epochs': int,
        'training.scheduler_T_0': int,
        'training.scheduler_T_mult': int,
        'training.scheduler_eta_min': float,
        'training.patience': int,
    }

    def get_nested_value(data, key_path):
        """ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã‹ã‚‰å€¤ã‚’å–å¾—"""
        keys = key_path.split('.')
        value = data
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        return value

    def set_nested_value(data, key_path, value):
        """ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã«å€¤ã‚’è¨­å®š"""
        keys = key_path.split('.')
        current = data
        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]
        current[keys[-1]] = value

    # æ•°å€¤å¤‰æ›ã®å®Ÿè¡Œ
    for key_path, target_type in numeric_conversions.items():
        current_value = get_nested_value(config, key_path)

        if current_value is not None:
            try:
                # æ–‡å­—åˆ—ã®å ´åˆã¯æ•°å€¤ã«å¤‰æ›
                if isinstance(current_value, str):
                    if target_type == float:
                        converted_value = float(current_value)
                    elif target_type == int:
                        converted_value = int(float(current_value))  # 1e3 â†’ 1000.0 â†’ 1000
                    else:
                        converted_value = target_type(current_value)
                elif isinstance(current_value, (int, float)):
                    converted_value = target_type(current_value)
                else:
                    converted_value = current_value

                set_nested_value(config, key_path, converted_value)

                # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                if str(current_value) != str(converted_value):
                    print(
                        f"å‹å¤‰æ›: {key_path} = {current_value} ({type(current_value)}) â†’ {converted_value} ({type(converted_value)})")

            except (ValueError, TypeError) as e:
                print(f"è­¦å‘Š: {key_path}ã®å‹å¤‰æ›ã«å¤±æ•—: {current_value} â†’ {target_type.__name__} (ã‚¨ãƒ©ãƒ¼: {e})")

    # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œè¨¼
    required_sections = {
        'data': ['data_path'],
        'model': ['input_dim', 'seq_len', 'hidden_dim', 'style_latent_dim', 'skill_latent_dim', 'beta', 'n_layers', 'contrastive_weight', 'use_triplet'],
        'training': ['batch_size', 'num_epochs', 'lr'],
        'logging': ['output_dir']
    }

    for section, keys in required_sections.items():
        if section not in config:
            raise ValueError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«'{section}'ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
        for key in keys:
            if key not in config[section]:
                raise ValueError(f"'{section}.{key}'ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
    data_path = config['data'].get('data_path', '')
    if data_path and not os.path.exists(data_path):
        print(f"è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« {data_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return config

def extract_latent_variables_hierarchical(model, test_loader, device):
    """éšå±¤å‹VAEã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º"""
    model.eval()
    all_z_style = []
    all_z_skill = []
    all_subject_ids = []
    all_is_expert = []
    all_reconstructions = []
    all_originals = []

    with torch.no_grad():
        for trajectories, subject_ids, is_expert in tqdm(test_loader, desc="éšå±¤æ½œåœ¨å¤‰æ•°æŠ½å‡ºä¸­"):
            trajectories = trajectories.to(device)

            encoded = model.encode(trajectories)
            z_style = encoded['z_style']
            z_skill = encoded['z_skill']

            reconstructed = model.decode(z_style, z_skill)

            all_z_style.append(z_style.cpu().numpy())
            all_z_skill.append(z_skill.cpu().numpy())
            all_subject_ids.append(subject_ids)
            all_is_expert.append(is_expert.cpu().numpy())
            all_reconstructions.append(reconstructed.cpu().numpy())
            all_originals.append(trajectories.cpu().numpy())

    return {
        'z_style': np.vstack(all_z_style),
        'z_skill': np.vstack(all_z_skill),
        'subject_ids': all_subject_ids,
        'is_expert': np.concatenate(all_is_expert),
        'reconstructions': np.vstack(all_reconstructions),
        'originals': np.vstack(all_originals)
    }

def run_skill_axis_analysis(model, test_loader, test_df, device):
    """ã‚¹ã‚­ãƒ«è»¸åˆ†æã‚’å®Ÿè¡Œ"""
    print("=== ã‚¹ã‚­ãƒ«è»¸åˆ†æé–‹å§‹ ===")

    model.eval()
    all_z_skill = []
    all_subject_ids = []

    # ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º
    with torch.no_grad():
        for trajectories, subject_ids, is_expert in test_loader:
            trajectories = trajectories.to(device)
            encoded = model.encode_hierarchically(trajectories)
            all_z_skill.append(encoded['z_skill'].cpu().numpy())
            all_subject_ids.extend(subject_ids)

    z_skill_data = np.vstack(all_z_skill)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’å–å¾—
    perf_cols = [col for col in test_df.columns if col.startswith('perf_')]
    performance_df = test_df.groupby(['subject_id', 'trial_num']).first()[perf_cols].reset_index()

    # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’åˆã‚ã›ã‚‹
    min_length = min(len(z_skill_data), len(performance_df))
    z_skill_data = z_skill_data[:min_length]
    performance_df = performance_df.iloc[:min_length]

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¾æ›¸ã‚’ä½œæˆ
    performance_data = {}
    for col in perf_cols:
        metric_name = col.replace('perf_', '')
        performance_data[metric_name] = performance_df[col].values

    # ã‚¹ã‚­ãƒ«è»¸åˆ†æã‚’å®Ÿè¡Œ
    analyzer = SkillAxisAnalyzer()
    analyzer.analyze_skill_axes(z_skill_data, performance_data)

    print("ã‚¹ã‚­ãƒ«è»¸åˆ†æå®Œäº†ï¼")
    return analyzer


def simple_vae_diagnosis(model, test_loader, device, output_dir, experiment_id):
    """ã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹VAEè¨ºæ–­"""
    print("=== ã‚·ãƒ³ãƒ—ãƒ«VAEè¨ºæ–­é–‹å§‹ ===")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)

    model.eval()
    all_z_style = []
    all_z_skill = []
    all_subject_ids = []

    print("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
    with torch.no_grad():
        for batch_idx, (trajectories, subject_ids, is_expert) in enumerate(test_loader):
            trajectories = trajectories.to(device)

            try:
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                encoded = model.encode_hierarchically(trajectories)
                all_z_style.append(encoded['z_style'].cpu().numpy())
                all_z_skill.append(encoded['z_skill'].cpu().numpy())
                all_subject_ids.extend(subject_ids)

                if batch_idx == 0:
                    print(f"ãƒãƒƒãƒå½¢çŠ¶ç¢ºèª: z_style={encoded['z_style'].shape}, z_skill={encoded['z_skill'].shape}")

            except Exception as e:
                print(f"ãƒãƒƒãƒ {batch_idx} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

    if not all_z_style:
        print("âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None

    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    z_style = np.vstack(all_z_style)
    z_skill = np.vstack(all_z_skill)

    print(f"åé›†å®Œäº†:")
    print(f"  z_style: {z_style.shape}")
    print(f"  z_skill: {z_skill.shape}")
    print(f"  è¢«é¨“è€…IDæ•°: {len(set(all_subject_ids))}")
    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_subject_ids)}")

    # è¢«é¨“è€…IDã‚’æ•°å€¤ãƒ©ãƒ™ãƒ«ã«å¤‰æ›
    unique_subjects = sorted(list(set(all_subject_ids)))
    subject_to_label = {subj: i for i, subj in enumerate(unique_subjects)}
    subject_labels = [subject_to_label[subj] for subj in all_subject_ids]

    print(f"è¢«é¨“è€…: {unique_subjects}")

    # === åŸºæœ¬çµ±è¨ˆ ===
    print("\n=== åŸºæœ¬çµ±è¨ˆ ===")
    style_mean = np.mean(z_style, axis=0)
    style_std = np.std(z_style, axis=0)
    style_var_total = np.var(z_style.flatten())

    print(f"ã‚¹ã‚¿ã‚¤ãƒ«æ½œåœ¨å¤‰æ•°:")
    print(f"  å¹³å‡ã®çµ¶å¯¾å€¤: {np.mean(np.abs(style_mean)):.4f}")
    print(f"  æ¨™æº–åå·®ã®å¹³å‡: {np.mean(style_std):.4f}")
    print(f"  å…¨ä½“åˆ†æ•£: {style_var_total:.4f}")
    print(f"  æœ€å¤§å€¤: {np.max(z_style):.4f}")
    print(f"  æœ€å°å€¤: {np.min(z_style):.4f}")

    # === è¢«é¨“è€…é–“ãƒ»è¢«é¨“è€…å†…åˆ†æ•£åˆ†æ ===
    print("\n=== åˆ†æ•£åˆ†æ ===")

    # è¢«é¨“è€…ã”ã¨ã®å¹³å‡è¨ˆç®—
    subject_means = []
    within_vars = []

    for subj in unique_subjects:
        mask = np.array(all_subject_ids) == subj
        subject_data = z_style[mask]

        subject_mean = np.mean(subject_data, axis=0)
        subject_means.append(subject_mean)

        if len(subject_data) > 1:
            within_var = np.var(subject_data, axis=0).mean()
            within_vars.append(within_var)
            print(f"  {subj}: è©¦è¡Œæ•°={len(subject_data)}, å†…åˆ†æ•£={within_var:.4f}")

    subject_means = np.array(subject_means)
    between_var = np.var(subject_means, axis=0).mean()
    avg_within_var = np.mean(within_vars) if within_vars else 0.01

    separation_ratio = between_var / avg_within_var

    print(f"\nåˆ†æ•£åˆ†è§£:")
    print(f"  è¢«é¨“è€…é–“åˆ†æ•£: {between_var:.4f}")
    print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.4f}")
    print(f"  åˆ†é›¢æ¯” (between/within): {separation_ratio:.4f}")

    # åˆ¤å®š
    if separation_ratio < 1.0:
        print("  âŒ è¢«é¨“è€…é–“åˆ†é›¢ãŒä¸ååˆ†")
    elif separation_ratio < 1.5:
        print("  âš ï¸  è¢«é¨“è€…é–“åˆ†é›¢ãŒå¼±ã„")
    else:
        print("  âœ… è¢«é¨“è€…é–“åˆ†é›¢ãŒè‰¯å¥½")

    # === å¯è¦–åŒ– ===
    print("\n=== å¯è¦–åŒ–ä½œæˆä¸­ ===")

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # 1. ã‚¹ã‚¿ã‚¤ãƒ«ç©ºé–“ã®PCA
    try:
        pca = PCA(n_components=min(2, z_style.shape[1]))
        z_style_pca = pca.fit_transform(z_style)

        scatter = axes[0, 0].scatter(z_style_pca[:, 0], z_style_pca[:, 1],
                                     c=subject_labels, cmap='tab10', alpha=0.7, s=20)
        axes[0, 0].set_title(f'Style PCA\n(Contribution rate: {pca.explained_variance_ratio_.sum():.3f})')
        axes[0, 0].set_xlabel('PC1')
        axes[0, 0].set_ylabel('PC2')

        # è¢«é¨“è€…ã”ã¨ã®é‡å¿ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        for i, subj in enumerate(unique_subjects):
            mask = np.array(subject_labels) == i
            if np.any(mask):
                center = np.mean(z_style_pca[mask], axis=0)
                axes[0, 0].scatter(center[0], center[1], c='red', s=100, marker='x')
                axes[0, 0].annotate(subj, center, xytext=(5, 5), textcoords='offset points')

        plt.colorbar(scatter, ax=axes[0, 0])

    except Exception as e:
        axes[0, 0].text(0.5, 0.5, f'PCA Error: {str(e)}',
                        transform=axes[0, 0].transAxes, ha='center')
        print(f"PCA ã‚¨ãƒ©ãƒ¼: {e}")

    # 2. ã‚¹ã‚¿ã‚¤ãƒ«ç©ºé–“ã®t-SNEï¼ˆãƒ‡ãƒ¼ã‚¿ãŒååˆ†ãªå ´åˆã®ã¿ï¼‰
    if len(z_style) >= 30:
        try:
            perplexity = min(30, len(z_style) // 4)
            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=300)
            z_style_tsne = tsne.fit_transform(z_style)

            scatter = axes[0, 1].scatter(z_style_tsne[:, 0], z_style_tsne[:, 1],
                                         c=subject_labels, cmap='tab10', alpha=0.7, s=20)
            axes[0, 1].set_title('Style t-SNE')
            plt.colorbar(scatter, ax=axes[0, 1])

        except Exception as e:
            axes[0, 1].text(0.5, 0.5, f't-SNE Error: {str(e)}',
                            transform=axes[0, 1].transAxes, ha='center')
            print(f"t-SNE Error: {e}")
    else:
        axes[0, 1].text(0.5, 0.5, 'Insufficient number of samples\n(t-SNE)',
                        transform=axes[0, 1].transAxes, ha='center')

    # 3. æ¬¡å…ƒåˆ¥åˆ†æ•£
    dim_vars = np.var(z_style, axis=0)
    axes[0, 2].bar(range(len(dim_vars)), dim_vars)
    axes[0, 2].set_title('Style dim variance')
    axes[0, 2].set_xlabel('dim')
    axes[0, 2].set_ylabel('variance')

    # 4. è¢«é¨“è€…åˆ¥åˆ†å¸ƒï¼ˆæœ€åˆã®2æ¬¡å…ƒï¼‰
    for i, subj in enumerate(unique_subjects):
        mask = np.array(subject_labels) == i
        if np.any(mask):
            subject_data = z_style[mask]
            axes[1, 0].scatter(subject_data[:, 0], subject_data[:, 1],
                               label=f'{subj}', alpha=0.6, s=20)

    axes[1, 0].set_title('Style Space (dim0 vs 1)')
    axes[1, 0].set_xlabel('Style dim 0')
    axes[1, 0].set_ylabel('Style dim 1')
    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    # 5. è¢«é¨“è€…é–“è·é›¢è¡Œåˆ—
    distance_matrix = np.zeros((len(unique_subjects), len(unique_subjects)))
    for i, subj_i in enumerate(unique_subjects):
        for j, subj_j in enumerate(unique_subjects):
            mean_i = subject_means[i]
            mean_j = subject_means[j]
            distance_matrix[i, j] = np.linalg.norm(mean_i - mean_j)

    im = axes[1, 1].imshow(distance_matrix, cmap='viridis')
    axes[1, 1].set_title('Inter-subject distance matrix')
    axes[1, 1].set_xticks(range(len(unique_subjects)))
    axes[1, 1].set_yticks(range(len(unique_subjects)))
    axes[1, 1].set_xticklabels(unique_subjects, rotation=45)
    axes[1, 1].set_yticklabels(unique_subjects)
    plt.colorbar(im, ax=axes[1, 1])

    # 6. æ´»æ€§åŒ–åº¦åˆ†å¸ƒ
    activation_magnitude = np.linalg.norm(z_style, axis=1)
    subject_activations = [activation_magnitude[np.array(subject_labels) == i]
                           for i in range(len(unique_subjects))]

    bp = axes[1, 2].boxplot(subject_activations, labels=unique_subjects)
    axes[1, 2].set_title('Activation level by subject')
    axes[1, 2].set_xlabel('subjects')
    axes[1, 2].set_ylabel('||z_style||')
    axes[1, 2].tick_params(axis='x', rotation=45)

    plt.tight_layout()

    # ä¿å­˜
    save_path = os.path.join(output_dir, f'vae_simple_diagnosis_exp{experiment_id}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"å¯è¦–åŒ–ä¿å­˜: {save_path}")

    # === çµæœã‚µãƒãƒªãƒ¼ ===
    print("\n" + "=" * 50)
    print("è¨ºæ–­çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 50)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åé›†: æˆåŠŸ")
    print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(z_style)}")
    print(f"ğŸ‘¥ è¢«é¨“è€…æ•°: {len(unique_subjects)}")
    print(f"ğŸ“ˆ åˆ†é›¢æ¯”: {separation_ratio:.3f}")
    print(f"ğŸ¯ ã‚¹ã‚¿ã‚¤ãƒ«æ´»æ€§åŒ–: {np.mean(style_std):.3f}")
    print(f"ğŸ“‰ å…¨ä½“åˆ†æ•£: {style_var_total:.3f}")

    if separation_ratio < 1.0:
        print("\nğŸš¨ å•é¡Œ: è¢«é¨“è€…é–“åˆ†é›¢ãŒä¸ååˆ†")
        print("   ğŸ’¡ æ¨å¥¨: beta_styleã‚’0.05ä»¥ä¸‹ã«ä¸‹ã’ã‚‹")
        print("   ğŸ’¡ æ¨å¥¨: style_learning_startã‚’0.15ã«æ—©ã‚ã‚‹")

    if np.mean(style_std) < 0.2:
        print("\nâš ï¸  å•é¡Œ: ã‚¹ã‚¿ã‚¤ãƒ«æ½œåœ¨å¤‰æ•°ã®æ´»æ€§åŒ–ä¸è¶³")
        print("   ğŸ’¡ æ¨å¥¨: beta_styleã‚’ã•ã‚‰ã«ä¸‹ã’ã‚‹")
        print("   ğŸ’¡ æ¨å¥¨: å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹")

    return {
        'separation_ratio': separation_ratio,
        'style_activation': np.mean(style_std),
        'style_variance': style_var_total,
        'subject_means': subject_means,
        'distance_matrix': distance_matrix,
        'z_style': z_style,
        'z_skill': z_skill,
        'subject_labels': subject_labels,
        'unique_subjects': unique_subjects
    }

def generate_axis_based_exemplars(model, analyzer, test_loader, device, save_path):
    """è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ"""
    print("=== è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ ===")

    model.eval()

    # ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    with torch.no_grad():
        for trajectories, subject_ids, is_expert in test_loader:
            trajectories = trajectories.to(device)
            encoded = model.encode_hierarchically(trajectories)

            # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨
            z_style = encoded['z_style'][[0]]
            current_skill = encoded['z_skill'][[0]]
            break

    # ç•°ãªã‚‹æ”¹å–„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ãŠæ‰‹æœ¬ã‚’ç”Ÿæˆ
    target_metrics = ['overall', 'trial_time', 'trial_error']
    enhancement_factor = 0.15

    generated_exemplars = {}

    with torch.no_grad():
        # ç¾åœ¨ãƒ¬ãƒ™ãƒ«
        current_exemplar = model.decode_hierarchically(z_style, current_skill)
        generated_exemplars['current'] = current_exemplar.cpu().numpy().squeeze()

        # å„æŒ‡æ¨™ã§ã®æ”¹å–„
        for target in target_metrics:
            try:
                improvement_direction = analyzer.get_improvement_direction(target)
                improvement_direction = torch.tensor(
                    improvement_direction,
                    dtype=torch.float32,
                    device=device
                ).unsqueeze(0)

                enhanced_skill = current_skill + enhancement_factor * improvement_direction
                enhanced_exemplar = model.decode_hierarchically(z_style, enhanced_skill)
                generated_exemplars[target] = enhanced_exemplar.cpu().numpy().squeeze()

            except Exception as e:
                print(f"{target}ã§ã®æ”¹å–„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ æ”¹å–„
                skill_noise = torch.randn_like(current_skill) * 0.1
                enhanced_skill = current_skill + enhancement_factor * skill_noise
                enhanced_exemplar = model.decode_hierarchically(z_style, enhanced_skill)
                generated_exemplars[target] = enhanced_exemplar.cpu().numpy().squeeze()

    # å¯è¦–åŒ–
    n_exemplars = len(generated_exemplars)
    fig, axes = plt.subplots(1, n_exemplars, figsize=(5 * n_exemplars, 5))
    if n_exemplars == 1:
        axes = [axes]

    for i, (target, traj) in enumerate(generated_exemplars.items()):
        cumsum_traj = np.cumsum(traj, axis=0)

        axes[i].plot(cumsum_traj[:, 0], cumsum_traj[:, 1], 'b-', linewidth=2, alpha=0.8)
        axes[i].scatter(cumsum_traj[0, 0], cumsum_traj[0, 1], c='green', s=100, label='Start', zorder=5)
        axes[i].scatter(cumsum_traj[-1, 0], cumsum_traj[-1, 1], c='red', s=100, label='End', zorder=5)

        if target == 'current':
            axes[i].set_title(f'Current Level')
            axes[i].plot(cumsum_traj[:, 0], cumsum_traj[:, 1], 'g-', linewidth=3, alpha=0.7)
        else:
            axes[i].set_title(f'Enhanced for\n{target.replace("_", " ").title()}')

        axes[i].set_xlabel('X Position')
        axes[i].set_ylabel('Y Position')
        axes[i].grid(True, alpha=0.3)
        axes[i].legend()
        axes[i].set_aspect('equal')

    plt.suptitle('Axis-Based Personalized Exemplars\n(Performance-Guided Skill Enhancement)', fontsize=14)
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆå®Œäº†: {save_path}")

def quantitative_evaluation_hierarchical(latent_data: dict, test_df: pd.DataFrame, output_dir: str):
    """BetaVAEç”¨ã®å®šé‡çš„è©•ä¾¡"""
    print("=== BetaVAEå®šé‡çš„è©•ä¾¡é–‹å§‹ ===")

    # å†æ§‹æˆæ€§èƒ½
    mse = np.mean((latent_data['originals'] - latent_data['reconstructions']) ** 2)
    print(f"å†æ§‹æˆMSE: {mse:.6f}")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’æŠ½å‡º
    perf_cols = [col for col in test_df.columns if col.startswith('perf_')]
    if not perf_cols:
        raise ValueError("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    performance_df = test_df.groupby(['subject_id', 'trial_num']).first()[['is_expert'] + perf_cols].reset_index()

    # æ½œåœ¨å¤‰æ•°ã‚’DataFrameã«å¤‰æ›
    z_style_df = pd.DataFrame(
        latent_data['z_style'],
        columns=[f'z_style_{i}' for i in range(latent_data['z_style'].shape[1])]
    )
    z_skill_df = pd.DataFrame(
        latent_data['z_skill'],
        columns=[f'z_skill_{i}' for i in range(latent_data['z_skill'].shape[1])]
    )

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    analysis_df = pd.concat([
        performance_df.reset_index(drop=True),
        z_style_df,
        z_skill_df,
    ], axis=1)

    # éšå±¤åˆ¥ç›¸é–¢åˆ†æ
    correlations = {'style': {}, 'skill': {}}
    metric_names = ['trial_time', 'trial_error', 'jerk', 'path_efficiency', 'approach_angle', 'sparc',
                    'trial_variability']
    available_metrics = [metric for metric in metric_names if f'perf_{metric}' in analysis_df.columns]

    for hierarchy in ['style', 'skill']:
        z_cols = [col for col in analysis_df.columns if f'z_{hierarchy}' in col]

        for metric_name in available_metrics:
            metric_col = f'perf_{metric_name}'
            correlations[hierarchy][metric_name] = []

            for z_col in z_cols:
                subset = analysis_df[[metric_col, z_col]].dropna()

                if len(subset) > 1 and subset[metric_col].std() > 1e-6 and subset[z_col].std() > 1e-6:
                    corr, p_value = pearsonr(subset[z_col], subset[metric_col])
                else:
                    corr, p_value = 0.0, 1.0

                correlations[hierarchy][metric_name].append((corr, p_value))

    # ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æ€§è©•ä¾¡
    style_ari = 0.0
    if 'subject_id' in analysis_df.columns:
        try:
            z_style_data = analysis_df[[col for col in analysis_df.columns if 'z_style' in col]].values
            n_subjects = len(analysis_df['subject_id'].unique())

            if len(z_style_data) > n_subjects:
                kmeans = KMeans(n_clusters=n_subjects, random_state=42)
                style_clusters = kmeans.fit_predict(z_style_data)
                subject_labels = pd.Categorical(analysis_df['subject_id']).codes
                style_ari = adjusted_rand_score(subject_labels, style_clusters)
        except Exception as e:
            print(f"ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")

    eval_results = {
        'reconstruction_mse': mse,
        'hierarchical_correlations': correlations,
        'style_separation_score': style_ari,
        'performance_data': performance_df.to_dict('records')
    }

    print("=" * 20 + " BetaVAEå®šé‡çš„è©•ä¾¡å®Œäº† " + "=" * 20)
    return eval_results, analysis_df


def run_beta_vae_evaluation(model, test_loader, test_df, output_dir, device, experiment_id):
    """æ”¹è‰¯ç‰ˆBetaVAEè©•ä¾¡ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æçµ±åˆï¼‰"""
    print("\n" + "=" * 50)
    print("æ”¹è‰¯ç‰ˆBetaVAEè©•ä¾¡é–‹å§‹ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æä»˜ãï¼‰")
    print("=" * 50)

    # 1. ã‚¹ã‚­ãƒ«è»¸åˆ†æ
    analyzer = run_skill_axis_analysis(model, test_loader, test_df, device)

    # 2. éšå±¤æ½œåœ¨å¤‰æ•°æŠ½å‡º
    latent_data = extract_latent_variables_hierarchical(model, test_loader, device)

    # 3. å®šé‡çš„è©•ä¾¡
    eval_results, analysis_df = quantitative_evaluation_hierarchical(latent_data, test_df, output_dir)

    # 4. è»¸ãƒ™ãƒ¼ã‚¹å€‹äººæœ€é©åŒ–ãŠæ‰‹æœ¬ç”Ÿæˆ
    exemplar_v2_path = os.path.join(output_dir, 'plots', f'axis_based_exemplars_exp{experiment_id}.png')
    generate_axis_based_exemplars(model, analyzer, test_loader, device, exemplar_v2_path)

    # 5. æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–
    latent_space_result_path = os.path.join(output_dir, 'latent_space')
    latent_space_result = simple_vae_diagnosis(model, test_loader, device, latent_space_result_path, experiment_id)

    # 6. çµæœçµ±åˆ
    eval_results.update({
        'skill_axis_analysis_completed': True,
        'axis_based_exemplars_path': exemplar_v2_path,
        'skill_improvement_directions_available': list(analyzer.skill_improvement_directions.keys()),
        'best_skill_correlations': {k: v['correlation'] for k, v in analyzer.performance_correlations.items()},
        'latent_space_analysis': latent_space_result
    })

    # 7. çµæœä¿å­˜
    eval_results_path = os.path.join(output_dir, 'results', f'beta_vae_evaluation_exp{experiment_id}.json')
    os.makedirs(os.path.dirname(eval_results_path), exist_ok=True)

    def convert_to_serializable(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        else:
            return obj

    serializable_results = convert_to_serializable(eval_results)

    with open(eval_results_path, 'w') as f:
        json.dump(serializable_results, f, indent=2)

    eval_results['evaluation_results_path'] = eval_results_path

    print("=" * 50)
    print("æ”¹è‰¯ç‰ˆBetaVAEè©•ä¾¡å®Œäº†")
    print("=" * 50)

    return eval_results


def setup_directories(output_dir):
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    dirs = [
        output_dir,
        os.path.join(output_dir, 'checkpoints'),
        os.path.join(output_dir, 'plots'),
        os.path.join(output_dir, 'logs'),
        os.path.join(output_dir, 'results')
    ]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)

def plot_hierarchical_training_curves(history, save_path):
    """éšå±¤å‹VAEã®å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # å…¨ä½“çš„ãªæå¤±
    axes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0, 0].plot(history['val_loss'], label='Validation Loss', color='red')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Total Loss')
    axes[0, 0].set_title('Total Training & Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    axes[0, 0].set_yscale('log')

    # å†æ§‹æˆæå¤±
    axes[0, 1].plot(history['recon_loss'], label='Reconstruction Loss', color='green')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Reconstruction Loss')
    axes[0, 1].set_title('Reconstruction Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    axes[0, 1].set_yscale('log')

    # å­¦ç¿’ç‡
    axes[0, 2].plot(history['learning_rates'], color='purple')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Learning Rate')
    axes[0, 2].set_title('Learning Rate Schedule')
    axes[0, 2].grid(True)
    axes[0, 2].set_yscale('log')

    # KLæå¤±
    axes[1, 0].plot(history['contrastive_loss'], label='Contrastive Loss', color='orange')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Contrastive')
    axes[1, 0].set_title('Contrastive Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    axes[1, 0].set_yscale('log')

    axes[1, 1].plot(history['kl_skill_loss'], label='KL Skill', color='brown')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('KL Divergence')
    axes[1, 1].set_title('KL Loss - Skills')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    axes[1, 1].set_yscale('log')

    axes[1, 2].plot(history['kl_style_loss'], label='KL Style', color='pink')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('KL Divergence')
    axes[1, 2].set_title('KL Loss - Individual Styles')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    axes[1, 2].set_yscale('log')

    plt.suptitle('Beta VAE Training Curves', fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"BetaVAEå­¦ç¿’æ›²ç·šã‚’ä¿å­˜: {save_path}")

def train_beta_vae_generalized_coordinate_model(config_path: str, experiment_id: int, db_path: str):
    """å®Œå…¨ç‰ˆBetaVAEå­¦ç¿’ï¼ˆã‚¹ã‚­ãƒ«è»¸åˆ†æçµ±åˆï¼‰"""
    print("=== BetaVAEå­¦ç¿’é–‹å§‹ ===")

    # 1. è¨­å®šèª­ã¿è¾¼ã¿
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    try:
        config = validate_and_convert_config(config)
        print("BetaVAEè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼: âœ… æ­£å¸¸")
    except ValueError as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        update_db(db_path, experiment_id, {
            'status': 'failed',
            'end_time': datetime.now().isoformat(),
            'notes': f"Config validation error: {str(e)}"
        })
        raise e

    # DBã«å­¦ç¿’é–‹å§‹ã‚’é€šçŸ¥
    update_db(db_path, experiment_id, {
        'status': 'running',
        'start_time': datetime.now().isoformat()
    })

    try:
        # 2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        output_dir = config['logging']['output_dir']
        setup_directories(output_dir)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"-- BetaVAEå®Ÿé¨“ID: {experiment_id} | ãƒ‡ãƒã‚¤ã‚¹: {device} ---")

        # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        try:
            train_loader, val_loader, test_loader, test_df = create_dataloaders(
                config['data']['data_path'],
                config['model']['seq_len'],
                config['training']['batch_size']
            )
            if train_loader is None:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            update_db(db_path, experiment_id, {
                'status': 'failed',
                'end_time': datetime.now().isoformat()
            })
            raise e

        # 4. BetaVAEãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        model = BetaVAEGeneralizedCoordinate(**config['model']).to(device)

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        optimizer = optim.AdamW(
            model.parameters(),
            config['training']['lr'],
            weight_decay=config['training'].get('weight_decay', 1e-5)
        )

        main_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=config['training'].get('scheduler_T_0', 15),
            T_mult=config['training'].get('scheduler_T_mult', 2),
            eta_min=config['training'].get('scheduler_eta_min', 1e-6)
        )
        warmup_scheduler = optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.1,
            total_iters=config['training'].get('warmup_epochs', 10)
        )
        scheduler = ChainedScheduler([warmup_scheduler, main_scheduler])

        # 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        best_val_loss = float('inf')
        epochs_no_improve = 0
        patience = config['training'].get('patience', 20)
        history = {
            'train_loss': [], 'val_loss': [], 'learning_rates': [],
            'recon_loss': [], 'kl_style_loss': [], 'kl_skill_loss': [], 'contrastive_loss': [],
        }

        print(f"å­¦ç¿’é–‹å§‹: {config['training']['num_epochs']}ã‚¨ãƒãƒƒã‚¯, patience={patience}")

        for epoch in range(config['training']['num_epochs']):
            model.train()
            model.update_epoch(epoch, config['training']['num_epochs'])

            epoch_losses = {
                'total': [], 'recon': [], 'kl_skill': [], 'kl_style': [], 'contrastive_loss': []
            }

            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{config["training"]["num_epochs"]} [Train]')
            for trajectories, subject_id, expertise in progress_bar:
                trajectories = trajectories.to(device)

                optimizer.zero_grad()
                outputs = model(trajectories, subject_id)

                loss = outputs['total_loss']
                loss.backward()

                torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    config['training'].get('clip_grad_norm', 1.0)
                )
                optimizer.step()

                # æå¤±è¨˜éŒ²
                epoch_losses['total'].append(outputs['total_loss'].item())
                epoch_losses['recon'].append(outputs['reconstruct_loss'].item())
                epoch_losses['kl_style'].append(outputs['kl_style'].item())
                epoch_losses['kl_skill'].append(outputs['kl_skill'].item())
                epoch_losses['contrastive_loss'].append(outputs['contrastive_loss'].item())

                progress_bar.set_postfix({'Loss': np.mean(epoch_losses['total'])})

            # ã‚¨ãƒãƒƒã‚¯æå¤±è¨˜éŒ²
            history['train_loss'].append(np.mean(epoch_losses['total']))
            history['recon_loss'].append(np.mean(epoch_losses['recon']))
            history['kl_style_loss'].append(np.mean(epoch_losses['kl_style']))
            history['kl_skill_loss'].append(np.mean(epoch_losses['kl_skill']))
            history['contrastive_loss'].append(np.mean(epoch_losses['contrastive_loss']))
            history['learning_rates'].append(optimizer.param_groups[0]['lr'])

            # æ¤œè¨¼ãƒ«ãƒ¼ãƒ—
            model.eval()
            epoch_val_losses = []
            with torch.no_grad():
                for trajectories, subject_id, expertise in val_loader:
                    trajectories = trajectories.to(device)
                    outputs = model(trajectories)
                    epoch_val_losses.append(outputs['total_loss'].item())

            current_val_loss = np.mean(epoch_val_losses)
            history['val_loss'].append(current_val_loss)

            print(f"Epoch {epoch + 1}: Train Loss: {history['train_loss'][-1]:.4f}, "
                  f"Val Loss: {current_val_loss:.4f}")
            print(f"  Recon: {history['recon_loss'][-1]:.4f}, "
                  f"KL(Skill): {history['kl_skill_loss'][-1]:.4f}, "
                  f"KL(Style): {history['kl_style_loss'][-1]:.4f}, "
                  f"Contrastive: {history['contrastive_loss'][-1]:.4f}, ")

            # å­¦ç¿’ç‡æ›´æ–°
            scheduler.step()

            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ & ã‚¢ãƒ¼ãƒªãƒ¼ã‚¹ãƒˆãƒƒãƒ—
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                epochs_no_improve = 0
                best_model_path = os.path.join(output_dir, 'checkpoints',
                                               f'best_hierarchical_model_exp{experiment_id}.pth')
                model.save_model(best_model_path)
                print(f" -> New best model saved! (Loss: {best_val_loss:.4f})")
            else:
                epochs_no_improve += 1

            if epochs_no_improve > patience:
                print(f"\nEarly stopping triggered after {patience} epochs with no improvement.")
                break

        # 6. å­¦ç¿’çµ‚äº†å¾Œã®å‡¦ç†
        final_model_path = os.path.join(output_dir, 'checkpoints', f'final_beta_vae_model_exp{experiment_id}.pth')
        model.save_model(final_model_path)

        plot_path = os.path.join(output_dir, 'plots', f'beta_vae_training_curves_exp{experiment_id}.png')
        plot_hierarchical_training_curves(history, plot_path)

        print("=== å­¦ç¿’å®Œäº†ã€è©•ä¾¡é–‹å§‹ ===")

        # 7. æ”¹è‰¯ç‰ˆè©•ä¾¡å®Ÿè¡Œ
        try:
            eval_results = run_beta_vae_evaluation(model, test_loader, test_df, output_dir, device,
                                                          experiment_id)

            # æœ€é«˜ç›¸é–¢å€¤ã‚’è¨ˆç®—
            best_correlation = 0.0
            best_correlation_metric = 'none'
            if 'best_skill_correlations' in eval_results:
                for metric, corr in eval_results['best_skill_correlations'].items():
                    if abs(corr) > abs(best_correlation):
                        best_correlation = corr
                        best_correlation_metric = metric

            # è©•ä¾¡çµæœã‚’DBã«è¨˜éŒ²
            update_db(db_path, experiment_id, {
                'status': 'completed',
                'end_time': datetime.now().isoformat(),
                'final_total_loss': history['train_loss'][-1],
                'best_val_loss': best_val_loss,
                'final_epoch': len(history['train_loss']),
                'early_stopped': epochs_no_improve > patience,

                # éšå±¤åˆ¥æœ€çµ‚æå¤±
                'final_recon_loss': history['recon_loss'][-1],
                'final_kl_style': history['kl_style_loss'][-1],
                'final_kl_skill': history['kl_skill_loss'][-1],
                'final_contrastive_loss': history['contrastive_loss'][-1],

                # è©•ä¾¡æŒ‡æ¨™
                'reconstruction_mse': eval_results['reconstruction_mse'],
                'style_separation_score': eval_results.get('style_separation_score', 0.0),
                'skill_performance_correlation': best_correlation,
                'best_skill_correlation_metric': best_correlation_metric,

                # ã‚¹ã‚­ãƒ«è»¸åˆ†æçµæœ
                'skill_axis_analysis_completed': eval_results.get('skill_axis_analysis_completed', False),
                'skill_improvement_directions_count': len(
                    eval_results.get('skill_improvement_directions_available', [])),
                'axis_based_improvement_enabled': True,

                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                'model_path': final_model_path,
                'best_model_path': best_model_path if 'best_model_path' in locals() else final_model_path,
                'training_curves_path': plot_path,
                'axis_based_exemplars_path': eval_results.get('axis_based_exemplars_path'),
                'evaluation_results_path': eval_results.get('evaluation_results_path'),

                'notes': f"å®Œå…¨ç‰ˆéšå±¤å‹VAE - MSE: {eval_results['reconstruction_mse']:.6f}, "
                         f"Style ARI: {eval_results.get('style_separation_score', 0.0):.4f}, "
                         f"Best Corr: {best_correlation:.4f} ({best_correlation_metric})"
            })

            print(f"=== å®Ÿé¨“ID: {experiment_id} æ­£å¸¸å®Œäº† ===")
            print(f"æœ€çµ‚çµæœ:")
            print(f"  å†æ§‹æˆMSE: {eval_results['reconstruction_mse']:.6f}")
            print(f"  ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢ARI: {eval_results.get('style_separation_score', 0.0):.4f}")
            print(f"  æœ€é«˜ã‚¹ã‚­ãƒ«ç›¸é–¢: {best_correlation:.4f} ({best_correlation_metric})")

        except Exception as eval_error:
            print(f"è©•ä¾¡æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {eval_error}")
            import traceback
            traceback.print_exc()

            update_db(db_path, experiment_id, {
                'status': 'completed',
                'end_time': datetime.now().isoformat(),
                'final_total_loss': history['train_loss'][-1],
                'best_val_loss': best_val_loss,
                'model_path': final_model_path,
                'training_curves_path': plot_path,
                'notes': f"å­¦ç¿’å®Œäº†ã€ä½†ã—è©•ä¾¡å¤±æ•—: {str(eval_error)}"
            })

    except Exception as e:
        print(f"!!! éšå±¤å‹VAEå®Ÿé¨“ID: {experiment_id} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e} !!!")
        import traceback
        traceback.print_exc()

        update_db(db_path, experiment_id, {
            'status': 'failed',
            'end_time': datetime.now().isoformat(),
            'notes': f"å­¦ç¿’ä¸­ã‚¨ãƒ©ãƒ¼: {str(e)}"
        })
        raise e

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸€èˆ¬åŒ–åº§æ¨™å‹BetaVAEå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument('--config', type=str, required=True, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--experiment_id', type=int, required=True, help='å®Ÿé¨“ç®¡ç†DBã®ID')
    parser.add_argument('--db_path', type=str, required=True, help='å®Ÿé¨“ç®¡ç†DBã®ãƒ‘ã‚¹')

    args = parser.parse_args()

    print(f"éšå±¤å‹VAEå­¦ç¿’é–‹å§‹:")
    print(f"  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {args.config}")
    print(f"  å®Ÿé¨“ID: {args.experiment_id}")
    print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {args.db_path}")

    train_beta_vae_generalized_coordinate_model(config_path=args.config, experiment_id=args.experiment_id, db_path=args.db_path)