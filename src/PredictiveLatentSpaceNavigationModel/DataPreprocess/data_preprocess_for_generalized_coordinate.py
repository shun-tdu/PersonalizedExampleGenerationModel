import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜ã«ä½¿ç”¨


def analyze_contradiction(df):
    """çŸ›ç›¾ã®åŸå› ã‚’è©³ç´°åˆ†æ"""
    print("ğŸ” çŸ›ç›¾ã®åŸå› åˆ†æ")
    print("=" * 50)

    subjects = df['subject_id'].unique()

    # 1. åˆ†é›¢ã‚¹ã‚³ã‚¢è¨ˆç®—ã®è©³ç´°ç¢ºèª
    print("1. åˆ†é›¢ã‚¹ã‚³ã‚¢è¨ˆç®—ã®æ¤œè¨¼:")

    feature_groups = {
        'Position': ['HandlePosDiffX', 'HandlePosDiffY'],
        'Velocity': ['HandleVelDiffX', 'HandleVelDiffY'],
        'Acceleration': ['HandleAccDiffX', 'HandleAccDiffY']
    }

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            print(f"\n{group_name}ã‚°ãƒ«ãƒ¼ãƒ—:")

            # è¢«é¨“è€…ã”ã¨ã®å®Ÿéš›ã®æ•°å€¤ã‚’ç¢ºèª
            subject_stats = {}
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 0:
                    stats = {
                        'mean': np.mean(subject_data, axis=0),
                        'std': np.std(subject_data, axis=0),
                        'median': np.median(subject_data, axis=0),
                        'data_points': len(subject_data)
                    }
                    subject_stats[subject] = stats
                    print(f"  {subject}: å¹³å‡={stats['mean']}, ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°={stats['data_points']}")

            # è¢«é¨“è€…é–“ã®å®Ÿéš›ã®å·®ã‚’è¨ˆç®—
            if len(subject_stats) > 1:
                means = np.array([stats['mean'] for stats in subject_stats.values()])
                print(f"  è¢«é¨“è€…é–“ã®å¹³å‡å€¤ç¯„å›²:")
                for dim in range(means.shape[1]):
                    dim_values = means[:, dim]
                    print(
                        f"    æ¬¡å…ƒ{dim}: {np.min(dim_values):.6f} ~ {np.max(dim_values):.6f} (ç¯„å›²: {np.ptp(dim_values):.6f})")

    # 2. é‹å‹•ç‰¹æ€§ã§ã®å€‹äººå·®ã®å…·ä½“çš„æ•°å€¤
    print(f"\n2. é‹å‹•ç‰¹æ€§ã§ã®å€‹äººå·®ã®å…·ä½“å€¤:")

    def calculate_movement_features(subject_df):
        """è¢«é¨“è€…ã®é‹å‹•ç‰¹æ€§ã‚’æ•°å€¤åŒ–"""
        features = []

        for trial_num, trial_df in subject_df.groupby('trial_num'):
            if len(trial_df) > 10:
                # ç›´ç·šæ€§
                start_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[0].values
                end_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[-1].values
                actual_path = trial_df[['HandlePosX', 'HandlePosY']].values

                straight_distance = np.linalg.norm(end_pos - start_pos)
                path_length = np.sum([np.linalg.norm(actual_path[i + 1] - actual_path[i])
                                      for i in range(len(actual_path) - 1)])
                linearity = straight_distance / path_length if path_length > 0 else 0

                # é€Ÿåº¦ç‰¹æ€§
                vel_x = trial_df['HandleVelX'].values
                vel_y = trial_df['HandleVelY'].values
                speed = np.sqrt(vel_x ** 2 + vel_y ** 2)

                peak_time_ratio = np.argmax(speed) / len(speed) if len(speed) > 0 else 0.5

                features.append({
                    'linearity': linearity,
                    'peak_time_ratio': peak_time_ratio,
                    'max_speed': np.max(speed),
                    'avg_speed': np.mean(speed)
                })

        return features

    # å„è¢«é¨“è€…ã®é‹å‹•ç‰¹æ€§
    subject_movement_features = {}
    for subject in subjects:
        subject_df = df[df['subject_id'] == subject]
        features = calculate_movement_features(subject_df)
        if features:
            # å¹³å‡å€¤ã‚’è¨ˆç®—
            avg_features = {}
            for key in features[0].keys():
                values = [f[key] for f in features]
                avg_features[key] = np.mean(values)
            subject_movement_features[subject] = avg_features

    # é‹å‹•ç‰¹æ€§ã§ã®è¢«é¨“è€…é–“å·®ã‚’å®šé‡åŒ–
    print(f"è¢«é¨“è€…åˆ¥é‹å‹•ç‰¹æ€§:")
    for subject, features in subject_movement_features.items():
        print(f"  {subject}: {features}")

    # é‹å‹•ç‰¹æ€§ã§ã®åˆ†é›¢ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    if len(subject_movement_features) > 1:
        print(f"\né‹å‹•ç‰¹æ€§ã§ã®åˆ†é›¢åº¦:")

        for feature_name in ['linearity', 'peak_time_ratio', 'max_speed', 'avg_speed']:
            values = [features[feature_name] for features in subject_movement_features.values()]

            if len(values) > 1:
                between_var = np.var(values)

                # è¢«é¨“è€…å†…åˆ†æ•£ã‚’æ¦‚ç®—ï¼ˆå…¨ä½“åˆ†æ•£ã‹ã‚‰è¢«é¨“è€…é–“åˆ†æ•£ã‚’å¼•ãï¼‰
                all_values = []
                for subject in subjects:
                    subject_df = df[df['subject_id'] == subject]
                    feature_values = calculate_movement_features(subject_df)
                    if feature_values:
                        subject_feature_values = [f[feature_name] for f in feature_values]
                        all_values.extend(subject_feature_values)

                if all_values:
                    total_var = np.var(all_values)
                    within_var = max(total_var - between_var, 0.001)  # æœ€å°å€¤ã§åˆ¶é™
                    separation_score = between_var / within_var

                    print(f"  {feature_name}: åˆ†é›¢ã‚¹ã‚³ã‚¢ = {separation_score:.4f}")

                    if separation_score > 1.0:
                        print(f"    ğŸŸ¢ ã“ã®ç‰¹å¾´ã§ã¯å€‹äººå·®ã‚ã‚Šï¼")
                    else:
                        print(f"    ğŸ”´ ã“ã®ç‰¹å¾´ã§ã‚‚å€‹äººå·®å°‘ãªã„")


def reconcile_contradiction(df):
    """çŸ›ç›¾ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®çµ±åˆåˆ†æ"""
    print(f"\nğŸ”„ çŸ›ç›¾ã®è§£æ±ºç­–åˆ†æ")
    print("=" * 50)

    print("ä»®èª¬1: ç‰¹å¾´æŠ½å‡ºãƒ¬ãƒ™ãƒ«ã®é•ã„")
    print("- å·®åˆ†å€¤ã§ã®åˆ†æ â†’ å€‹äººå·®æ¤œå‡ºå›°é›£")
    print("- é«˜æ¬¡ç‰¹å¾´ã§ã®åˆ†æ â†’ å€‹äººå·®æ¤œå‡ºå¯èƒ½")

    print(f"\nä»®èª¬2: ã‚¹ã‚±ãƒ¼ãƒ«ã®å•é¡Œ")
    print("- å¾®ç´°ãªå·®ãŒå¤§ããªãƒã‚¤ã‚ºã«åŸ‹ã‚‚ã‚Œã¦ã„ã‚‹")
    print("- é©åˆ‡ãªæ­£è¦åŒ–ã«ã‚ˆã‚Šå€‹äººå·®ãŒæµ®ä¸Šã™ã‚‹å¯èƒ½æ€§")

    print(f"\nä»®èª¬3: éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­˜åœ¨")
    print("- ç·šå½¢åˆ†æã§ã¯æ‰ãˆã‚‰ã‚Œãªã„å€‹äººå·®")
    print("- VAEã®ã‚ˆã†ãªéç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºå¯èƒ½")

    # å®Ÿéš›ã«éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ç´¢
    subjects = df['subject_id'].unique()

    # å„è¢«é¨“è€…ã®ã€Œé‹å‹•ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã€ã‚’ä½œæˆ
    print(f"\nğŸ¯ çµ±åˆçš„é‹å‹•ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ†æ:")

    subject_signatures = {}

    for subject in subjects:
        subject_df = df[df['subject_id'] == subject]

        # è¤‡æ•°ã®ç‰¹å¾´ã‚’çµ±åˆ
        signature_features = []

        for trial_num, trial_df in subject_df.groupby('trial_num'):
            if len(trial_df) > 20:
                # 1. è»Œé“ç‰¹å¾´
                positions = trial_df[['HandlePosX', 'HandlePosY']].values
                velocities = trial_df[['HandleVelX', 'HandleVelY']].values

                # è¤‡åˆç‰¹å¾´ã‚’è¨ˆç®—
                composite_features = {
                    # æ™‚ç©ºé–“ç‰¹å¾´
                    'path_curvature': calculate_path_curvature(positions),
                    'velocity_smoothness': calculate_velocity_smoothness(velocities),
                    'acceleration_jerk': calculate_jerk_metric(trial_df),

                    # å‹•çš„ç‰¹å¾´
                    'movement_rhythm': calculate_movement_rhythm(velocities),
                    'force_modulation': calculate_force_modulation(trial_df),
                }

                signature_features.append(composite_features)

        if signature_features:
            # å¹³å‡ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã‚’è¨ˆç®—
            avg_signature = {}
            for key in signature_features[0].keys():
                values = [f[key] for f in signature_features if not np.isnan(f[key])]
                if values:
                    avg_signature[key] = np.mean(values)
                else:
                    avg_signature[key] = 0.0

            subject_signatures[subject] = avg_signature

    # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã®å€‹äººå·®ã‚’åˆ†æ
    if len(subject_signatures) > 1:
        print(f"çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã®å€‹äººå·®:")

        for subject, signature in subject_signatures.items():
            print(f"  {subject}: {signature}")

        # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼é–“è·é›¢
        subjects_list = list(subject_signatures.keys())
        signatures_matrix = np.array([list(sig.values()) for sig in subject_signatures.values()])

        if signatures_matrix.shape[0] > 1 and signatures_matrix.shape[1] > 0:
            from scipy.spatial.distance import pdist, squareform

            distances = pdist(signatures_matrix, metric='euclidean')
            distance_matrix = squareform(distances)

            print(f"\nè¢«é¨“è€…é–“ã‚·ã‚°ãƒãƒãƒ£ãƒ¼è·é›¢:")
            for i, subj1 in enumerate(subjects_list):
                for j, subj2 in enumerate(subjects_list):
                    if i < j:
                        print(f"  {subj1} vs {subj2}: {distance_matrix[i, j]:.4f}")

            avg_distance = np.mean(distances)
            print(f"å¹³å‡è·é›¢: {avg_distance:.4f}")

            if avg_distance > 0.5:
                print("ğŸŸ¢ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§æ˜ç¢ºãªå€‹äººå·®ã‚ã‚Šï¼")
            elif avg_distance > 0.1:
                print("ğŸŸ¡ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ä¸­ç¨‹åº¦ã®å€‹äººå·®")
            else:
                print("ğŸ”´ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã‚‚å€‹äººå·®å°‘ãªã„")


# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
def calculate_path_curvature(positions):
    """è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—"""
    if len(positions) < 3:
        return 0.0

    # å˜ç´”ãªæ›²ç‡è¨ˆç®—
    curvatures = []
    for i in range(1, len(positions) - 1):
        p1, p2, p3 = positions[i - 1], positions[i], positions[i + 1]

        # 3ç‚¹ã‹ã‚‰æ›²ç‡ã‚’è¨ˆç®—
        a = np.linalg.norm(p2 - p1)
        b = np.linalg.norm(p3 - p2)
        c = np.linalg.norm(p3 - p1)

        if a > 0 and b > 0 and c > 0:
            s = (a + b + c) / 2
            area = np.sqrt(max(0, s * (s - a) * (s - b) * (s - c)))
            curvature = 4 * area / (a * b * c) if (a * b * c) > 0 else 0
            curvatures.append(curvature)

    return np.mean(curvatures) if curvatures else 0.0


def calculate_velocity_smoothness(velocities):
    """é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•ã‚’è¨ˆç®—"""
    if len(velocities) < 2:
        return 0.0

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
    velocity_changes = np.abs(np.diff(vel_magnitude))
    return 1.0 / (1.0 + np.std(velocity_changes))


def calculate_jerk_metric(trial_df):
    """ã‚¸ãƒ£ãƒ¼ã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    if len(trial_df) < 3:
        return 0.0

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values

    jerk_x = np.diff(acc_x)
    jerk_y = np.diff(acc_y)
    jerk_magnitude = np.sqrt(jerk_x ** 2 + jerk_y ** 2)

    return np.mean(jerk_magnitude)


def calculate_movement_rhythm(velocities):
    """é‹å‹•ãƒªã‚ºãƒ ã‚’è¨ˆç®—"""
    if len(velocities) < 10:
        return 0.0

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)

    # FFTã§å‘¨æ³¢æ•°æˆåˆ†ã‚’åˆ†æ
    try:
        fft = np.fft.fft(vel_magnitude)
        freqs = np.fft.fftfreq(len(vel_magnitude))

        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†
        dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft) // 2])) + 1
        dominant_freq = freqs[dominant_freq_idx]

        return abs(dominant_freq)
    except:
        return 0.0


def calculate_force_modulation(trial_df):
    """åŠ›èª¿ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
    if len(trial_df) < 5:
        return 0.0

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values
    force_estimate = np.sqrt(acc_x ** 2 + acc_y ** 2)

    # åŠ›ã®å¤‰èª¿æŒ‡æ¨™
    force_variability = np.std(force_estimate) / (np.mean(force_estimate) + 1e-6)
    return force_variability


# å®Ÿè¡Œé–¢æ•°
def resolve_contradiction_analysis(df):
    """çŸ›ç›¾ã®è§£æ±ºåˆ†æã‚’å®Ÿè¡Œ"""
    print("ğŸ¯ çŸ›ç›¾è§£æ±ºåˆ†æ")
    print("=" * 60)

    analyze_contradiction(df)
    reconcile_contradiction(df)

    print(f"\nğŸ’¡ çµè«–:")
    print(f"å·®åˆ†ãƒ¬ãƒ™ãƒ«ã§ã¯å€‹äººå·®ãŒæ¤œå‡ºå›°é›£ã ãŒã€")
    print(f"é«˜æ¬¡ç‰¹å¾´ã‚„çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã¯å€‹äººå·®ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ã€‚")
    print(f"VAEã®ã‚ˆã†ãªéç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ½œåœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ãŒæœ‰æœ›ã€‚")

# def analyze_movement_constraints(df):
#     """Point-to-Pointå‹•ä½œã®åˆ¶ç´„ã‚’å®šé‡åŒ–"""
#     print("=== Point-to-Pointå‹•ä½œåˆ¶ç´„åˆ†æ ===")
#
#     # 1. è»Œé“ã®ç›´ç·šæ€§åˆ†æ
#     print("1. è»Œé“ã®ç›´ç·šæ€§:")
#     subjects = df['subject_id'].unique()
#
#     for subject in subjects:
#         subject_df = df[df['subject_id'] == subject]
#         linearity_scores = []
#
#         for trial_num, trial_df in subject_df.groupby('trial_num'):
#             if len(trial_df) > 10:
#                 # é–‹å§‹ç‚¹ã¨çµ‚äº†ç‚¹
#                 start_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[0].values
#                 end_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[-1].values
#
#                 # å®Ÿéš›ã®è»Œé“
#                 actual_path = trial_df[['HandlePosX', 'HandlePosY']].values
#
#                 # ç›´ç·šè·é›¢
#                 straight_distance = np.linalg.norm(end_pos - start_pos)
#
#                 # å®Ÿéš›ã®çµŒè·¯é•·
#                 path_length = np.sum([np.linalg.norm(actual_path[i + 1] - actual_path[i])
#                                       for i in range(len(actual_path) - 1)])
#
#                 # ç›´ç·šæ€§ã‚¹ã‚³ã‚¢ (1ã«è¿‘ã„ã»ã©ç›´ç·šçš„)
#                 if path_length > 0:
#                     linearity = straight_distance / path_length
#                     linearity_scores.append(linearity)
#
#         if linearity_scores:
#             avg_linearity = np.mean(linearity_scores)
#             print(f"  {subject}: å¹³å‡ç›´ç·šæ€§ = {avg_linearity:.4f}")
#
#             if avg_linearity > 0.95:
#                 print(f"    ğŸ”´ ã»ã¼ç›´ç·šçš„ - å€‹äººå·®ãŒå‡ºã«ãã„")
#             elif avg_linearity > 0.90:
#                 print(f"    ğŸŸ¡ ã‚„ã‚„ç›´ç·šçš„")
#             else:
#                 print(f"    ğŸŸ¢ æ›²ç·šçš„ - å€‹äººå·®ã®å¯èƒ½æ€§ã‚ã‚Š")
#
#
# def analyze_movement_phases(df):
#     """é‹å‹•ã®ä½ç›¸åˆ†æ"""
#     print("\n2. é‹å‹•ä½ç›¸ã®ä¸€æ§˜æ€§:")
#
#     subjects = df['subject_id'].unique()
#
#     for subject in subjects:
#         subject_df = df[df['subject_id'] == subject]
#         phase_patterns = []
#
#         for trial_num, trial_df in subject_df.groupby('trial_num'):
#             if len(trial_df) > 20:
#                 # é€Ÿåº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
#                 vel_x = trial_df['HandleVelX'].values
#                 vel_y = trial_df['HandleVelY'].values
#                 speed = np.sqrt(vel_x ** 2 + vel_y ** 2)
#
#                 # é‹å‹•ä½ç›¸ã®ç‰¹å®š
#                 # 1. åŠ é€ŸæœŸé–“ã®å‰²åˆ
#                 acceleration_phase = np.sum(np.diff(speed) > 0) / len(speed)
#
#                 # 2. æœ€å¤§é€Ÿåº¦åˆ°é”æ™‚ç‚¹ï¼ˆæ­£è¦åŒ–ï¼‰
#                 if len(speed) > 0:
#                     peak_time_ratio = np.argmax(speed) / len(speed)
#                 else:
#                     peak_time_ratio = 0.5
#
#                 # 3. é€Ÿåº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®å¯¾ç§°æ€§
#                 mid_point = len(speed) // 2
#                 first_half = speed[:mid_point]
#                 second_half = speed[mid_point:mid_point + len(first_half)][::-1]  # åè»¢
#
#                 if len(first_half) > 0 and len(second_half) > 0:
#                     symmetry = np.corrcoef(first_half, second_half)[0, 1]
#                     if np.isnan(symmetry):
#                         symmetry = 0
#                 else:
#                     symmetry = 0
#
#                 phase_patterns.append({
#                     'acceleration_ratio': acceleration_phase,
#                     'peak_time_ratio': peak_time_ratio,
#                     'velocity_symmetry': symmetry
#                 })
#
#         if phase_patterns:
#             avg_accel_ratio = np.mean([p['acceleration_ratio'] for p in phase_patterns])
#             avg_peak_time = np.mean([p['peak_time_ratio'] for p in phase_patterns])
#             avg_symmetry = np.mean([p['velocity_symmetry'] for p in phase_patterns])
#
#             print(f"  {subject}:")
#             print(f"    åŠ é€ŸæœŸé–“å‰²åˆ: {avg_accel_ratio:.3f}")
#             print(f"    é€Ÿåº¦ãƒ”ãƒ¼ã‚¯æ™‚ç‚¹: {avg_peak_time:.3f}")
#             print(f"    é€Ÿåº¦å¯¾ç§°æ€§: {avg_symmetry:.3f}")
#
#             # ãƒ™ãƒ«ã‚·ã‚§ã‚¤ãƒ—ï¼ˆå…¸å‹çš„ãªpoint-to-pointï¼‰ã®åˆ¤å®š
#             is_typical_ptp = (0.4 < avg_peak_time < 0.6 and avg_symmetry > 0.7)
#             if is_typical_ptp:
#                 print(f"    ğŸ”´ å…¸å‹çš„ãªãƒ™ãƒ«ã‚·ã‚§ã‚¤ãƒ—é€Ÿåº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
#             else:
#                 print(f"    ğŸŸ¢ éå…¸å‹çš„ãªé€Ÿåº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ - å€‹æ€§ã‚ã‚Š")
#
#
# def analyze_optimal_control_convergence(df):
#     """æœ€é©åˆ¶å¾¡ã¸ã®åæŸåº¦åˆ†æ"""
#     print("\n3. æœ€é©åˆ¶å¾¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®åæŸ:")
#
#     subjects = df['subject_id'].unique()
#
#     # ç†è«–çš„æœ€é©è»Œé“ã¨ã®æ¯”è¼ƒ
#     for subject in subjects:
#         subject_df = df[df['subject_id'] == subject]
#         optimality_scores = []
#
#         for trial_num, trial_df in subject_df.groupby('trial_num'):
#             if len(trial_df) > 15:
#                 # å®Ÿéš›ã®è»Œé“
#                 positions = trial_df[['HandlePosX', 'HandlePosY']].values
#                 velocities = trial_df[['HandleVelX', 'HandleVelY']].values
#
#                 # æœ€é©åˆ¶å¾¡æŒ‡æ¨™
#                 # 1. èºåº¦æœ€å°åŒ–ï¼ˆæ»‘ã‚‰ã‹ã•ï¼‰
#                 jerks = []
#                 for i in range(2, len(velocities)):
#                     jerk = np.linalg.norm(velocities[i] - 2 * velocities[i - 1] + velocities[i - 2])
#                     jerks.append(jerk)
#
#                 if jerks:
#                     jerk_score = 1.0 / (1.0 + np.mean(jerks))  # èºåº¦ãŒå°ã•ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
#                 else:
#                     jerk_score = 0
#
#                 # 2. ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡
#                 energy = np.sum([np.linalg.norm(v) ** 2 for v in velocities])
#                 path_length = np.sum([np.linalg.norm(positions[i + 1] - positions[i])
#                                       for i in range(len(positions) - 1)])
#
#                 if path_length > 0:
#                     energy_efficiency = 1.0 / (1.0 + energy / path_length)
#                 else:
#                     energy_efficiency = 0
#
#                 optimality_scores.append({
#                     'jerk_optimality': jerk_score,
#                     'energy_efficiency': energy_efficiency
#                 })
#
#         if optimality_scores:
#             avg_jerk_opt = np.mean([s['jerk_optimality'] for s in optimality_scores])
#             avg_energy_eff = np.mean([s['energy_efficiency'] for s in optimality_scores])
#
#             print(f"  {subject}:")
#             print(f"    èºåº¦æœ€é©æ€§: {avg_jerk_opt:.4f}")
#             print(f"    ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡: {avg_energy_eff:.4f}")
#
#             # æœ€é©åˆ¶å¾¡ã¸ã®åæŸåº¦
#             overall_optimality = (avg_jerk_opt + avg_energy_eff) / 2
#             if overall_optimality > 0.8:
#                 print(f"    ğŸ”´ é«˜åº¦ã«æœ€é©åŒ– - å€‹äººå·®å°‘ãªã„")
#             elif overall_optimality > 0.6:
#                 print(f"    ğŸŸ¡ ä¸­ç¨‹åº¦ã«æœ€é©åŒ–")
#             else:
#                 print(f"    ğŸŸ¢ æœ€é©åŒ–ä¸ååˆ† - å€‹äººå·®ã®å¯èƒ½æ€§")
#
#
# def recommend_alternative_tasks():
#     """å€‹äººå·®ãŒç¾ã‚Œã‚„ã™ã„ä»£æ›¿ã‚¿ã‚¹ã‚¯ã®ææ¡ˆ"""
#     print("\n" + "=" * 50)
#     print("å€‹äººå·®ãŒç¾ã‚Œã‚„ã™ã„ä»£æ›¿ã‚¿ã‚¹ã‚¯ææ¡ˆ")
#     print("=" * 50)
#
#     alternative_tasks = [
#         {
#             "ã‚¿ã‚¹ã‚¯": "è‡ªç”±è»Œé“æç”»",
#             "èª¬æ˜": "ç‰¹å®šã®å½¢çŠ¶ã‚’è‡ªç”±ã«æãï¼ˆå††ã€8ã®å­—ãªã©ï¼‰",
#             "å€‹äººå·®è¦å› ": "æç”»ã‚¹ã‚¿ã‚¤ãƒ«ã€é€Ÿåº¦å¤‰èª¿ã€è»Œé“é¸æŠ",
#             "æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢": "> 2.0"
#         },
#         {
#             "ã‚¿ã‚¹ã‚¯": "ãƒªã‚ºãƒŸã‚«ãƒ«é‹å‹•",
#             "èª¬æ˜": "ãƒ¡ãƒˆãƒ­ãƒãƒ¼ãƒ ã«åˆã‚ã›ãŸåå¾©é‹å‹•",
#             "å€‹äººå·®è¦å› ": "ãƒªã‚ºãƒ åŒæœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã€ä½ç›¸é–¢ä¿‚",
#             "æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢": "> 1.5"
#         },
#         {
#             "ã‚¿ã‚¹ã‚¯": "éšœå®³ç‰©å›é¿",
#             "èª¬æ˜": "å‹•çš„éšœå®³ç‰©ã‚’é¿ã‘ãªãŒã‚‰ã®åˆ°é”é‹å‹•",
#             "å€‹äººå·®è¦å› ": "å›é¿æˆ¦ç•¥ã€äºˆæ¸¬çš„åˆ¶å¾¡",
#             "æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢": "> 3.0"
#         },
#         {
#             "ã‚¿ã‚¹ã‚¯": "ãƒãƒ«ãƒã‚¿ãƒ¼ã‚²ãƒƒãƒˆ",
#             "èª¬æ˜": "è¤‡æ•°ç›®æ¨™ã¸ã®é€£ç¶šåˆ°é”",
#             "å€‹äººå·®è¦å› ": "è»Œé“è¨ˆç”»ã€å‹•ä½œã‚·ãƒ¼ã‚±ãƒ³ã‚¹",
#             "æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢": "> 2.5"
#         },
#         {
#             "ã‚¿ã‚¹ã‚¯": "åŠ›åˆ¶å¾¡èª²é¡Œ",
#             "èª¬æ˜": "ç‰¹å®šã®åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®æ“ä½œ",
#             "å€‹äººå·®è¦å› ": "åŠ›ã®èª¿ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚°ãƒªãƒƒãƒ—æˆ¦ç•¥",
#             "æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢": "> 2.0"
#         }
#     ]
#
#     for i, task in enumerate(alternative_tasks, 1):
#         print(f"{i}. {task['ã‚¿ã‚¹ã‚¯']}")
#         print(f"   èª¬æ˜: {task['èª¬æ˜']}")
#         print(f"   å€‹äººå·®è¦å› : {task['å€‹äººå·®è¦å› ']}")
#         print(f"   æœŸå¾…åˆ†é›¢ã‚¹ã‚³ã‚¢: {task['æœŸå¾…ã•ã‚Œã‚‹åˆ†é›¢ã‚¹ã‚³ã‚¢']}")
#         print()
#
#
# # ä½¿ç”¨æ–¹æ³•
# def comprehensive_ptp_analysis(df):
#     """Point-to-Pointå‹•ä½œã®åŒ…æ‹¬çš„åˆ†æ"""
#     print("ğŸ¯ Point-to-Pointå‹•ä½œåˆ¶ç´„åˆ†æ")
#     print("=" * 60)
#
#     analyze_movement_constraints(df)
#     analyze_movement_phases(df)
#     analyze_optimal_control_convergence(df)
#     recommend_alternative_tasks()
#
#     print("\nğŸ’¡ çµè«–:")
#     print("Point-to-Pointå‹•ä½œã¯æœ¬è³ªçš„ã«åˆ¶ç´„ãŒå¼·ãã€")
#     print("å€‹äººå·®ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ï¼‰ãŒç¾ã‚Œã«ãã„é‹å‹•ã‚¿ã‚¹ã‚¯ã§ã™ã€‚")
#     print("ã‚ˆã‚Šè‡ªç”±åº¦ã®é«˜ã„ã‚¿ã‚¹ã‚¯ã§ã®æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ----------------------------------------
def load_rawdata(data_dir: str) -> pd.DataFrame | None:
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ã¦ã®CSVã‚’èª­ã¿è¾¼ã¿ã€ä¸€ã¤ã®DataFrameã«çµåˆã™ã‚‹ã€‚"""
    all_files = glob.glob(os.path.join(data_dir, "*.csv"))
    if not all_files:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{data_dir}' ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    df_list = []
    for filename in all_files:
        try:
            basename = os.path.basename(filename)
            parts = basename.replace('.csv', '').split('_')
            subject_id = parts[0]
            block_num = int(parts[1].replace('Block', ''))

            df = pd.read_csv(filename)
            # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ã—ã€æƒ…å ±ã‚’ä»˜ä¸
            df = df.rename(columns={'SubjectId': 'subject_id', 'CurrentTrial': 'trial_num', 'Block': 'block'})
            df['subject_id'] = subject_id
            df['block'] = block_num
            df_list.append(df)
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    if not df_list:
        return None
    return pd.concat(df_list, ignore_index=True)


# ----------------------------------------
# 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ----------------------------------------
def calculate_jerk(acceleration: np.ndarray, dt: float) -> float:
    if len(acceleration) < 2: return np.nan
    jerk = np.gradient(acceleration, dt, axis=0)
    return np.mean(np.linalg.norm(jerk, axis=1))


def calculate_path_efficiency(trajectory: np.ndarray) -> float:
    start_point, end_point = trajectory[0], trajectory[-1]
    straight_distance = np.linalg.norm(end_point - start_point)
    total_distance = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
    if total_distance < 1e-6: return 1.0
    return straight_distance / total_distance


def calculate_approach_angle(trajectory: np.ndarray, n_points: int = 5) -> float:
    if len(trajectory) < n_points: return np.nan
    vec = trajectory[-1] - trajectory[-n_points]
    return np.arctan2(vec[1], vec[0]) * 180 / np.pi


def calculate_sparc(velocity: np.ndarray, dt: float) -> float:
    if len(velocity) < 2: return np.nan
    velocity_magnitude = np.linalg.norm(velocity, axis=1)
    return 1.0 / (np.std(velocity_magnitude) + 1e-6)


# ----------------------------------------
# 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DataFrameã®ä½œæˆ
# ----------------------------------------
def create_performance_dataframe(raw_df: pd.DataFrame, target_sequence_length: int = 100) -> pd.DataFrame:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€è©¦è¡Œã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DataFrameã‚’ä½œæˆã™ã‚‹ã€‚"""
    # 'TRIAL_RUNNING'çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
    df = raw_df[raw_df['TrialState'] == 'TRIAL_RUNNING'].copy()
    if df.empty:
        print("ã‚¨ãƒ©ãƒ¼: 'TRIAL_RUNNING'çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # --- è©¦è¡Œé–“ã®ã°ã‚‰ã¤ãã‚’å…ˆã«è¨ˆç®— ---
    all_trial_variability = {}
    for subject_id, subject_df in df.groupby('subject_id'):
        resampled_trajs = []
        for _, trial_df in subject_df.groupby('trial_num'):
            traj_points = trial_df[['HandlePosX', 'HandlePosY']].values
            original_indices = np.linspace(0, 1, num=len(traj_points))
            target_indices = np.linspace(0, 1, num=target_sequence_length)
            resampled_x = np.interp(target_indices, original_indices, traj_points[:, 0])
            resampled_y = np.interp(target_indices, original_indices, traj_points[:, 1])
            resampled_trajs.append(np.column_stack([resampled_x, resampled_y]))
        if resampled_trajs:
            variability = np.mean(np.std(np.array(resampled_trajs), axis=0))
            all_trial_variability[subject_id] = variability

    # --- å„è©¦è¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¨ˆç®— ---
    def calculate_metrics_for_trial(group: pd.DataFrame) -> pd.Series:
        trajectory = group[['HandlePosX', 'HandlePosY']].values
        velocity = group[['HandleVelX', 'HandleVelY']].values
        acceleration = group[['HandleAccX', 'HandleAccY']].values
        target_end_pos = group[['TargetEndPosX', 'TargetEndPosY']].iloc[-1].values
        time_stamps = group['Timestamp'].values

        dt = np.mean(np.diff(time_stamps))
        if np.isnan(dt) or dt == 0: dt = 0.01

        return pd.Series({
            'block': group['block'].iloc[0],
            'trial_time': time_stamps[-1] - time_stamps[0],
            'trial_error': np.linalg.norm(trajectory[-1] - target_end_pos),
            'jerk': calculate_jerk(acceleration, dt),
            'path_efficiency': calculate_path_efficiency(trajectory),
            'approach_angle': calculate_approach_angle(trajectory),
            'sparc': calculate_sparc(velocity, dt),
            'trial_variability': all_trial_variability.get(group.name[0], np.nan)
        })

    performance_df = df.groupby(['subject_id', 'trial_num']).apply(calculate_metrics_for_trial).reset_index()
    return performance_df


# ----------------------------------------
# 4. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
# ----------------------------------------
def calculate_skill_scores(perf_df: pd.DataFrame) -> pd.Series:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DFã‹ã‚‰ã€è¢«é¨“è€…ã”ã¨ã®ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    # è¢«é¨“è€…ã”ã¨ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®å¹³å‡ã‚’è¨ˆç®—
    subject_metrics = perf_df.groupby('subject_id').mean()

    # Z-scoreåŒ–
    metrics_zscored = subject_metrics.drop(columns=['block', 'trial_num']).apply(lambda x: (x - x.mean()) / x.std())

    # æ–¹å‘æ€§ã®çµ±ä¸€ (å€¤ãŒä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã®ç¬¦å·ã‚’åè»¢)
    metrics_zscored['trial_time'] *= -1
    metrics_zscored['trial_error'] *= -1
    metrics_zscored['jerk'] *= -1
    metrics_zscored['trial_variability'] *= -1
    # path_efficiency, sparc, approach_angleã¯é«˜ã„æ–¹ãŒè‰¯ã„ã®ã§ãã®ã¾ã¾

    # ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
    skill_scores = metrics_zscored.sum(axis=1)
    return skill_scores


def classify_by_median_split(skill_scores: pd.Series) -> pd.Series:
    """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’ä¸­å¤®å€¤ã§ã€Œç†Ÿé”è€…(1)ã€ã¨ã€Œåˆå¿ƒè€…(0)ã€ã«åˆ†é¡ã™ã‚‹ã€‚"""
    median_score = skill_scores.median()
    return (skill_scores > median_score).astype(int).rename('is_expert')

def add_diffs(group):
    """å„ç¨®ç‰©ç†é‡ã‚’å·®åˆ†è¡¨ç¤ºå¤‰æ›"""
    pos_data = group[['HandlePosX', 'HandlePosY']].values
    pos_diffs = np.diff(pos_data, axis=0, prepend=pos_data[0:1])

    vel_data = group[['HandleVelX', 'HandleVelY']].values
    vel_diffs = np.diff(vel_data, axis=0, prepend=vel_data[0:1])

    acc_data = group[['HandleAccX', 'HandleAccY']].values
    acc_diffs = np.diff(acc_data, axis=0, prepend=acc_data[0:1])

    group = group.copy()
    group['HandlePosDiffX'] = pos_diffs[:, 0]
    group['HandlePosDiffY'] = pos_diffs[:, 1]

    group['HandleVelDiffX'] = vel_diffs[:, 0]
    group['HandleVelDiffY'] = vel_diffs[:, 1]

    group['HandleAccDiffX'] = acc_diffs[:, 0]
    group['HandleAccDiffY'] = acc_diffs[:, 1]

    return group


def prepare_scaler(feature_df:pd.DataFrame):
    """ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã”ã¨ã«å€‹åˆ¥ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""

    # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ç¢ºèª
    available_cols = feature_df.columns.tolist()
    print(f"Available columns: {available_cols}")

    scalers = {}

    # ä½ç½®
    position_cols = ['HandlePosX', 'HandlePosY']
    if all(col in available_cols for col in position_cols):
        scalers['position'] = StandardScaler().fit(feature_df[position_cols])
        print(f"Position scaler created:{position_cols}")

    # ä½ç½®å·®åˆ†
    position_diff_cols = ['HandlePosDiffX', 'HandlePosDiffY']
    if all(col in available_cols for col in position_diff_cols):
        scalers['position_diff'] = StandardScaler().fit(feature_df[position_diff_cols])
        print(f"Position diff scaler created:{position_diff_cols}")

    # é€Ÿåº¦
    velocity_cols = ['HandleVelX', 'HandleVelY']
    if all(col in available_cols for col in velocity_cols):
        scalers['velocity'] = StandardScaler().fit(feature_df[velocity_cols])
        print(f"Velocity scaler created:{velocity_cols}")

    # é€Ÿåº¦å·®åˆ†
    velocity_diff_cols = ['HandleVelDiffX', 'HandleVelDiffY']
    if all(col in available_cols for col in velocity_diff_cols):
        scalers['velocity_diff'] = StandardScaler().fit(feature_df[velocity_diff_cols])
        print(f"Velocity diff scaler created:{velocity_diff_cols}")

    # åŠ é€Ÿåº¦
    acceleration_cols = ['HandleAccX', 'HandleAccY']
    if all(col in available_cols for col in acceleration_cols):
        scalers['acceleration'] = StandardScaler().fit(feature_df[acceleration_cols])
        print(f"acceleration scaler created:{acceleration_cols}")

    # åŠ é€Ÿåº¦å·®åˆ†
    acceleration_diff_cols = ['HandleAccDiffX', 'HandleAccDiffY']
    if all(col in available_cols for col in acceleration_diff_cols):
        scalers['acceleration_diff'] = StandardScaler().fit(feature_df[acceleration_diff_cols])
        print(f"acceleration diff scaler created:{acceleration_diff_cols}")

     # å°†æ¥çš„ãªã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    jerk_cols = ['JerkX', 'JerkY']
    if all(col in available_cols for col in jerk_cols):
        scalers['jerk'] = StandardScaler().fit(feature_df[jerk_cols])
        print(f"Jerk scaler created: {jerk_cols}")

    return scalers


def analyze_style_hierarchy(df):
    """ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤ã®å®šé‡çš„åˆ†æ"""

    # è¢«é¨“è€…ã”ã¨ã®å„ç‰©ç†é‡ã®ç‰¹å¾´ã‚’æŠ½å‡º
    subjects = df['subject_id'].unique()

    # å„ç‰©ç†é‡ã§ã®è¢«é¨“è€…é–“åˆ†é›¢åº¦ã‚’è¨ˆç®—
    separation_scores = {}

    feature_groups = {
        'Position': ['HandlePosDiffX', 'HandlePosDiffY'],
        'Velocity': ['HandleVelDiffX', 'HandleVelDiffY'],
        'Acceleration': ['HandleAccDiffX', 'HandleAccDiffY']
    }

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            # è¢«é¨“è€…ã”ã¨ã®å¹³å‡ã‚’è¨ˆç®—
            subject_means = []
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].mean().values
                subject_means.append(subject_data)

            subject_means = np.array(subject_means)

            # è¢«é¨“è€…é–“åˆ†æ•£ / è¢«é¨“è€…å†…åˆ†æ•£
            between_var = np.var(subject_means.mean(axis=1))

            within_vars = []
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 1:
                    within_var = np.var(subject_data.mean(axis=1))
                    within_vars.append(within_var)

            avg_within_var = np.mean(within_vars) if within_vars else 0.01
            separation_score = between_var / (avg_within_var + 1e-8)

            separation_scores[group_name] = separation_score

            print(f"{group_name}:")
            print(f"  è¢«é¨“è€…é–“åˆ†æ•£: {between_var:.6f}")
            print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.6f}")
            print(f"  åˆ†é›¢ã‚¹ã‚³ã‚¢: {separation_score:.4f}")

    return separation_scores


def analyze_style_hierarchy(df):
    """ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤ã®å®šé‡çš„åˆ†æ"""
    print("\n" + "=" * 50)
    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æé–‹å§‹")
    print("=" * 50)

    # è¢«é¨“è€…ã”ã¨ã®å„ç‰©ç†é‡ã®ç‰¹å¾´ã‚’æŠ½å‡º
    subjects = df['subject_id'].unique()
    print(f"åˆ†æå¯¾è±¡è¢«é¨“è€…æ•°: {len(subjects)}")
    print(f"è¢«é¨“è€…ID: {list(subjects)}")

    # å„ç‰©ç†é‡ã§ã®è¢«é¨“è€…é–“åˆ†é›¢åº¦ã‚’è¨ˆç®—
    separation_scores = {}

    feature_groups = {
        'Position': ['HandlePosX', 'HandlePosY'],
        'Velocity': ['HandleVelX', 'HandleVelY'],
        'Acceleration': ['HandleAccX', 'HandleAccY']
    }

    print(f"\nåˆ†æå¯¾è±¡ç‰©ç†é‡ã‚°ãƒ«ãƒ¼ãƒ—:")
    for group_name, features in feature_groups.items():
        available = all(f in df.columns for f in features)
        print(f"  {group_name}: {features} -> {'âœ…' if available else 'âŒ'}")

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            # è¢«é¨“è€…ã”ã¨ã®å¹³å‡ã‚’è¨ˆç®—
            subject_means = []
            subject_within_vars = []

            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 0:
                    subject_mean = np.mean(subject_data, axis=0)
                    subject_means.append(subject_mean)

                    # è¢«é¨“è€…å†…åˆ†æ•£ã‚‚è¨ˆç®—
                    if len(subject_data) > 1:
                        within_var = np.var(subject_data, axis=0).mean()
                        subject_within_vars.append(within_var)

            if len(subject_means) > 1:
                subject_means = np.array(subject_means)

                # è¢«é¨“è€…é–“åˆ†æ•£ï¼šå„è¢«é¨“è€…ã®å¹³å‡å€¤ã®åˆ†æ•£
                between_var = np.var(subject_means, axis=0).mean()

                # å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£
                avg_within_var = np.mean(subject_within_vars) if subject_within_vars else 0.01

                # åˆ†é›¢ã‚¹ã‚³ã‚¢ï¼šè¢«é¨“è€…é–“åˆ†æ•£ / è¢«é¨“è€…å†…åˆ†æ•£
                separation_score = between_var / (avg_within_var + 1e-8)

                separation_scores[group_name] = separation_score

                print(f"\n{group_name}:")
                print(f"  è¢«é¨“è€…é–“åˆ†æ•£: {between_var:.6f}")
                print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.6f}")
                print(f"  åˆ†é›¢ã‚¹ã‚³ã‚¢: {separation_score:.4f}")

                # åˆ†é›¢åº¦ã®è©•ä¾¡
                if separation_score > 2.0:
                    print(f"  ğŸŸ¢ é«˜ã„åˆ†é›¢åº¦ - ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ãŒè±Šå¯Œ")
                elif separation_score > 1.0:
                    print(f"  ğŸŸ¡ ä¸­ç¨‹åº¦ã®åˆ†é›¢åº¦ - ä¸€éƒ¨ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ã‚ã‚Š")
                else:
                    print(f"  ğŸ”´ ä½ã„åˆ†é›¢åº¦ - ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ãŒå°‘ãªã„")

    print(f"\n" + "=" * 50)
    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æçµæœ")
    print("=" * 50)

    # åˆ†é›¢ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    sorted_scores = sorted(separation_scores.items(), key=lambda x: x[1], reverse=True)
    for i, (group, score) in enumerate(sorted_scores, 1):
        print(f"{i}. {group}: {score:.4f}")

    if sorted_scores:
        best_group, best_score = sorted_scores[0]
        print(f"\nğŸ¯ æœ€ã‚‚æœ‰åŠ¹ãªã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±: {best_group} (åˆ†é›¢ã‚¹ã‚³ã‚¢: {best_score:.4f})")

        if best_score > 2.0:
            print("âœ… è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™")
        elif best_score > 1.0:
            print("âš ï¸  è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯å›°é›£ã§ã™ãŒå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        else:
            print("âŒ è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯éå¸¸ã«å›°é›£ã§ã™")

    return separation_scores

def main():
    TARGET_SEQ_LEN = 100
    RAWDATA_DIR = '../../../data/RawDatas/'
    PROCESSED_DATA_DIR = 'PredictiveLatentSpaceNavigationModel/DataPreprocess/ForGeneralizedCoordinate'
    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

    # --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ ---
    # 1. ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    raw_df = load_rawdata(RAWDATA_DIR)
    if raw_df is None: return

    # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
    performance_df = create_performance_dataframe(raw_df, target_sequence_length=TARGET_SEQ_LEN)
    if performance_df.empty: return

    # 3. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    skill_scores = calculate_skill_scores(performance_df)

    # 4. ç†Ÿé”è€…/åˆå¿ƒè€…ã®åˆ†é¡
    expertise_labels = classify_by_median_split(skill_scores)

    # 5. ç†Ÿç·´åº¦ã®æƒ…å ±ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    filtered_df = raw_df[raw_df['TrialState'] == 'TRIAL_RUNNING'].copy()
    filtered_df['is_expert'] = filtered_df['subject_id'].map(expertise_labels)

    # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è»Œé“ãƒ‡ãƒ¼ã‚¿ã«çµåˆ
    # performance_dfã®åˆ—åã«prefixã‚’è¿½åŠ ã—ã¦ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆã‚’é¿ã‘ã‚‹
    performance_df_renamed = performance_df.add_prefix('perf_').rename(columns={'perf_subject_id': 'subject_id', 'perf_trial_num': 'trial_num'})
    master_df = filtered_df.merge(
        performance_df_renamed,
        on=['subject_id', 'trial_num'],
        how='left'
    )

    # 7. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    subject_ids = master_df['subject_id'].unique()
    train_subjects, test_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)

    train_df = master_df[master_df['subject_id'].isin(train_subjects)]
    test_df = master_df[master_df['subject_id'].isin(test_subjects)]

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿è¢«é¨“è€…æ•°: {len(train_subjects)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¢«é¨“è€…æ•°: {len(test_subjects)}")

    # 8. å·®åˆ†è¡¨ç¤ºã®è¨ˆç®—
    train_df = train_df.groupby(['subject_id', 'trial_num'], group_keys=False).apply(add_diffs)
    test_df = test_df.groupby(['subject_id', 'trial_num'], group_keys=False).apply(add_diffs)
    print("å·®åˆ†è¡¨ç¤ºã®è¨ˆç®—å®Œäº† âœ… ")

    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æå®Ÿè¡Œä¸­...")
    # separation_score = analyze_style_hierarchy(master_df)
    # comprehensive_ptp_analysis(master_df)
    resolve_contradiction_analysis(train_df)
    # print(train_df.columns)

    # 8. ã‚¹ã‚±ãƒ¼ãƒ©ã®ç”¨æ„
    feature_cols = [
        'HandlePosDiffX', 'HandlePosDiffY',
        'HandleVelDiffX', 'HandleVelDiffY',
        # 'HandleAccDiffX', 'HandleAccDiffY',
    ]

    scalers = prepare_scaler(train_df[feature_cols])
    print("StandardScalerã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¾ã—ãŸã€‚âœ…")

    # 9. ã‚¹ã‚±ãƒ¼ãƒ©ã¨ç‰¹å¾´é‡å®šç¾©ã‚’ä¿å­˜
    scaler_path = os.path.join(PROCESSED_DATA_DIR, 'scalers.joblib')
    feature_config_path = os.path.join(PROCESSED_DATA_DIR, 'feature_config.joblib')

    joblib.dump(scalers, scaler_path)
    joblib.dump({'feature_cols': feature_cols}, feature_config_path)

    print(f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ {scaler_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print(f"ç‰¹å¾´é‡è¨­å®šã‚’ {feature_config_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãã‚Œãã‚Œä¿å­˜
    train_df.to_parquet(os.path.join(PROCESSED_DATA_DIR, 'train_data.parquet'))
    test_df.to_parquet(os.path.join(PROCESSED_DATA_DIR, 'test_data.parquet'))
    print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’Parquetå½¢å¼ã§ä¿å­˜ã—ã¾ã—ãŸã€‚")



if __name__ == "__main__":
    main()
