import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import  Dict, List, Tuple, Optional
from pathlib import  Path
from scipy.interpolate import interp1d, UnivariateSpline
from scipy import stats
# from sklearn.decomposition import FactorAnalysis
try:
    from factor_analyzer import FactorAnalyzer
    FACTOR_ANALYZER_AVAILABLE = True
except ImportError:
    FACTOR_ANALYZER_AVAILABLE = False
    FactorAnalyzer = None

from sklearn.preprocessing import StandardScaler, MinMaxScaler  # CLAUDE_ADDED: MinMaxScaler for -1 to 1 normalization

import yaml
import datetime
import joblib

# CLAUDE_ADDED: Academic paper formatting settings
# Set Times New Roman font for academic papers
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman', 'Times', 'Liberation Serif']
plt.rcParams['font.size'] = 9
plt.rcParams['axes.labelsize'] = 11
plt.rcParams['axes.titlesize'] = 13
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9
plt.rcParams['legend.fontsize'] = 9
plt.rcParams['figure.titlesize'] = 15
# Set tick direction to inward for academic papers
plt.rcParams['xtick.direction'] = 'in'
plt.rcParams['ytick.direction'] = 'in'


def anonymize_subjects(subject_ids):
    """Convert subject names to anonymous labels (Subject1, Subject2, etc.)"""
    unique_subjects = sorted(list(set(subject_ids)))
    return {subj: f"Subject{i+1}" for i, subj in enumerate(unique_subjects)}

def save_academic_figure(fig, save_path):
    """Save figure in both PDF and PNG formats for academic use"""
    save_path = Path(save_path)
    pdf_path = save_path.with_suffix('.pdf')
    png_path = save_path.with_suffix('.png')

    # Save PDF (vector format, scalable, preferred for academic papers)
    fig.savefig(pdf_path, format='pdf', bbox_inches='tight', dpi=300,
               facecolor='white', edgecolor='none')

    # Save PNG (raster format, 300 DPI for print quality)
    fig.savefig(png_path, format='png', bbox_inches='tight', dpi=300,
               facecolor='white', edgecolor='none')

    print(f"âœ… Saved: {pdf_path.name} and {png_path.name}")


class DataPreprocessConfigLoader:
    def __init__(self, config_dir: str):
        # ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        self.config_dir = Path(config_dir)
        if not self.config_dir.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with self.config_dir.open() as f:
            config = yaml.safe_load(f)

        is_valid, error_list = self.validate_config(config)
        if not is_valid:
            error_details = "\n - ".join(error_list)
            raise ValueError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™: \n - {error_details}")
        else:
            self.config =config

    def validate_config(self, config: Dict) -> Tuple[bool, List[str]]:
        """è¨­å®šã‚’æ¤œè¨¼ã—ã€æˆåŠŸ/å¤±æ•—ã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™"""

        all_errors = []
        all_errors.extend(self._validate_paths(config))
        all_errors.extend(self._validate_preprocessing(config))
        all_errors.extend(self._validate_analysis_flags(config))

        return len(all_errors) == 0, all_errors

    def _validate_paths(self, config: Dict) -> List[str]:
        """ãƒ‘ã‚¹ç³»è¨­å®šã®æ¤œè¨¼"""
        errors = []

        data_section = config.get('data')
        if data_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'data' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        raw_data_dir_section = data_section.get('raw_data_dir')
        if  raw_data_dir_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'raw_data_dir' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return errors
        else:
            raw_data_dir = Path(raw_data_dir_section)
            if not raw_data_dir.exists():
                errors.append(f"raw_data_dirã«æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{raw_data_dir}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        return errors

    def _validate_preprocessing(self, config:Dict) -> List[str]:
        """å‰å‡¦ç†è¨­å®šã®æ¤œè¨¼"""
        errors = []

        pre_process_section = config.get('pre_process')
        if pre_process_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'pre_process' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        target_seq_len_section = pre_process_section.get('target_seq_len')
        if target_seq_len_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'target_seq_len' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        interpolate_method_section = pre_process_section.get('interpolate_method')
        if interpolate_method_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'interpolate_method' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors
        else:
            if interpolate_method_section not in ['linear', 'spline', 'none']:
                errors.append("interpolate_methodã®å€¤ãŒä¸é©åˆ‡ã§ã™ã€‚['linear', 'spline', 'none'] ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")

        # CLAUDE_ADDED: normalization_method ã®æ¤œè¨¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        normalization_method_section = pre_process_section.get('normalization_method')
        if normalization_method_section is not None:
            if normalization_method_section not in ['standard', 'minmax']:
                errors.append("normalization_methodã®å€¤ãŒä¸é©åˆ‡ã§ã™ã€‚['standard', 'minmax'] ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")

        return errors

    def _validate_analysis_flags(self, config:Dict) -> List[str]:
        """åˆ†æãƒ•ãƒ©ã‚°ã®æ¤œè¨¼"""
        errors = []

        analysis_section = config.get('analysis')
        if analysis_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'analysis' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        anova_skill_metrics_section = analysis_section.get('anova_skill_metrics')
        if anova_skill_metrics_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'anova_skill_metrics' ãŒã‚ã‚Šã¾ã›ã‚“")

        factorize_skill_metrics_section = analysis_section.get('factorize_skill_metrics')
        if factorize_skill_metrics_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'factorize_skill_metrics' ãŒã‚ã‚Šã¾ã›ã‚“")

        return errors

    def get_config(self) -> Optional[Dict] :
        """æ¤œè¨¼æ¸ˆã¿è¨­å®šã‚’å–å¾—"""
        if self.config is not None:
            return self.config.copy()
        else:
            return None

    def get_data_paths(self) -> Tuple[str, str]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’å–å¾—"""
        return (self.config['data']['raw_data_dir'],
                self.config['data']['output_dir'])


class OutputManager:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã‚’ç®¡ç†ã™ã‚‹"""
    def __init__(self, validated_config: Dict):
        self.config = validated_config
        self.base_output_dir = self.config['data']['output_dir']
        self.process_name = self.config['information']['name']

        self._skill_analyzer_output_dir, self._dataset_builder_output_path, self._skill_score_calculator_output_path = self._make_unique_output_paths()


    def _make_unique_output_paths(self) -> tuple[Path, Path, Path]:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä½œæˆ"""
        base_path = Path(self.base_output_dir)
        validated_process_name = self.process_name.replace(' ', '_')
        time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

        dir_name = f"{validated_process_name}_{time_stamp}"

        skill_analyzer_result_dir = base_path / dir_name / 'skill_analyze_result'
        data_set_builder_output_dir = base_path / dir_name / 'dataset'
        skill_score_calculator_output_dir =base_path/ dir_name /'skill_score_result'

        return skill_analyzer_result_dir, data_set_builder_output_dir, skill_score_calculator_output_dir

    @property
    def skill_analyzer_output_dir_path(self):
        return self._skill_analyzer_output_dir

    @property
    def dataset_builder_output_dir_path(self):
        return self._dataset_builder_output_path

    @property
    def skill_score_calculator_output_dir_path(self):
        return self._skill_score_calculator_output_path


class TrajectoryDataLoader:
    """è»Œé“ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ã‚’æ‹…å½“"""
    def __init__(self, config:Dict):
        """æ¤œè¨¼æ¸ˆã¿è¨­å®šã‚’å—ã‘å–ã‚Š"""
        self.config = config
        self.raw_data_dir = Path(config['data']['raw_data_dir'])
        self.raw_data_df = self._load_raw_data()
        self.preprocessed_data_df = self._preprocess_trajectories(self.raw_data_df)

    def _load_raw_data(self) -> Optional[pd.DataFrame]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®CSVã‚’èª­ã¿è¾¼ã¿ã€ä¸€ã¤ã®DataFrameã«çµåˆã™ã‚‹"""
        all_files = list(self.raw_data_dir.glob('*.csv'))

        if not all_files:
            print(f"ã‚¨ãƒ©ãƒ¼:ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {str(self.raw_data_dir)} ã«csvãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼")
            return None

        df_list = []
        for file_path in all_files:
            try:
                parts = file_path.stem.split('_')
                subject_id = parts[0]
                block_num = int(parts[1].replace('Block', ''))

                df = pd.read_csv(file_path)

                # CLAUDE_ADDED: ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ã—ã€æƒ…å ±ã‚’ä»˜ä¸ (ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ )
                print(f"Processing file: {file_path.name}, original columns: {df.columns.tolist()}")
                
                # CurrentTrialãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒªãƒãƒ¼ãƒ 
                rename_dict = {}
                if 'SubjectId' in df.columns:
                    rename_dict['SubjectId'] = 'subject_id'
                if 'CurrentTrial' in df.columns:
                    rename_dict['CurrentTrial'] = 'trial_num'
                
                if rename_dict:
                    df = df.rename(columns=rename_dict)
                    print(f"Renamed columns: {rename_dict}")
                
                df['subject_id'] = subject_id
                df['block'] = block_num
                
                # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¾Œã®ã‚«ãƒ©ãƒ ã¨åŸºæœ¬çµ±è¨ˆã‚’ç¢ºèª
                print(f"Final columns: {df.columns.tolist()}")
                if 'trial_num' in df.columns:
                    print(f"trial_num unique values: {df['trial_num'].unique()}")
                print(f"Data shape for {subject_id} Block{block_num}: {df.shape}")
                
                df_list.append(df)
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        if not df_list:
            return None

        concatenated_df = pd.concat(df_list, ignore_index=True)
        
        # CLAUDE_ADDED: çµåˆå¾Œã®ç¢ºèª
        print(f"Concatenated data shape: {concatenated_df.shape}")
        trial_running_df = concatenated_df[concatenated_df['TrialState'] == 'TRIAL_RUNNING'].copy()
        print(f"TRIAL_RUNNING data shape: {trial_running_df.shape}")
        
        return trial_running_df

    def _preprocess_trajectories(self, data: pd.DataFrame) -> pd.DataFrame:
        """è»Œé“ã®å‰å‡¦ç†(è£œå®Œã€é•·ã•èª¿æ•´)"""
        target_seq_len = self.config['pre_process']['target_seq_len']
        method = self.config['pre_process']['interpolate_method']

        if method == 'none':
            print("ğŸ’¡ æ™‚é–“è£œé–“ãªã—ã§ã€å…ƒã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
        else:
            target_seq_len = self.config['pre_process']['target_seq_len']

        processed_trajectories = [] #å…¨ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’è“„ç©ã™ã‚‹ãƒªã‚¹ãƒˆ

        print(f"Input data shape: {data.shape}")
        print(f"Available columns: {data.columns.tolist()}")
        
        for subject_id, subject_df in data.groupby('subject_id'):
            for (trial_num, block_num), trial_df in subject_df.groupby(['trial_num', 'block']):
                try:
                    original_length = len(trial_df)

                    # æœ€å°ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
                    if original_length < 2:
                        print(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num} ({original_length}ç‚¹)")
                        continue

                    # è£œå®Œå‡¦ç†ãªã—ã®å ´åˆ
                    if method == 'none':
                        trajectory_df = trial_df.copy()
                        trajectory_df['time_step'] = range(original_length)
                        trajectory_df['original_length'] = original_length

                        processed_trajectories.append(trajectory_df)
                        continue

                    # è£œå®Œæ–¹æ³•ã«å¿œã˜ãŸå‡¦ç†
                    traj_positions = trial_df[['HandlePosX','HandlePosY']].values
                    traj_velocities = trial_df[['HandleVelX','HandleVelY']].values
                    traj_acceleration = trial_df[['HandleAccX','HandleAccY']].values

                    # å¤‰æ›å…ˆã®æ™‚é–“è»¸ã®ä½œæˆ
                    original_length = len(traj_positions)
                    original_time = np.linspace(0, 1, original_length)
                    target_time = np.linspace(0,1, target_seq_len)

                    interp_pos_x = None
                    interp_pos_y = None
                    interp_vel_x = None
                    interp_vel_y = None
                    interp_acc_x = None
                    interp_acc_y = None

                    if method == 'linear':
                        # ç·šå½¢è£œé–“é–¢æ•°ã®ä½œæˆ
                        interp_pos_x = interp1d(original_time, traj_positions[:, 0], kind ='linear')
                        interp_pos_y = interp1d(original_time, traj_positions[:, 1], kind ='linear')
                        interp_vel_x = interp1d(original_time, traj_velocities[:, 0], kind='linear')
                        interp_vel_y = interp1d(original_time, traj_velocities[:, 1], kind='linear')
                        interp_acc_x = interp1d(original_time, traj_acceleration[:, 0], kind='linear')
                        interp_acc_y = interp1d(original_time, traj_acceleration[:, 1], kind='linear')
                    elif method == 'spline':
                        # ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“é–¢æ•°ã®ä½œæˆ
                        interp_pos_x = UnivariateSpline(original_time, traj_positions[:, 0], s=0)
                        interp_pos_y = UnivariateSpline(original_time, traj_positions[:, 1], s=0)
                        interp_vel_x = UnivariateSpline(original_time, traj_velocities[:, 0], s=0)
                        interp_vel_y = UnivariateSpline(original_time, traj_velocities[:, 1], s=0)
                        interp_acc_x = UnivariateSpline(original_time, traj_acceleration[:, 0], s=0)
                        interp_acc_y = UnivariateSpline(original_time, traj_acceleration[:, 1], s=0)
                    else:
                        print(f"æœªå¯¾å¿œã®è£œå®Œæ–¹æ³•: {method}")
                        continue

                    # æ–°ã—ã„æ™‚é–“è»¸ã§è£œå®Œ
                    resampled_pos_x = interp_pos_x(target_time)
                    resampled_pos_y = interp_pos_y(target_time)
                    resampled_vel_x = interp_vel_x(target_time)
                    resampled_vel_y = interp_vel_y(target_time)
                    resampled_acc_x = interp_acc_x(target_time)
                    resampled_acc_y = interp_acc_y(target_time)

                    # å…ƒã®Timestampæƒ…å ±ã‚’å–å¾—ã—ã¦æ–°ã—ã„æ™‚é–“è»¸ã‚’ä½œæˆ
                    original_start_time = trial_df['Timestamp'].iloc[0]
                    original_end_time = trial_df['Timestamp'].iloc[-1]
                    resampled_timestamps = np.linspace(original_start_time, original_end_time, target_seq_len)
                    
                    # ãƒªã‚µãƒ³ãƒ—ãƒ«å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
                    trajectory_df = pd.DataFrame({
                        'subject_id': subject_id,
                        'trial_num': trial_num,
                        'block': block_num, # CLAUDE_ADDED: ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‹ã‚‰å–å¾—ã—ãŸæ­£ã—ã„blockç•ªå·ã‚’ä½¿ç”¨
                        'time_step':range(target_seq_len),
                        'Timestamp': resampled_timestamps,
                        'HandlePosX': resampled_pos_x,
                        'HandlePosY': resampled_pos_y,
                        'HandleVelX': resampled_vel_x,
                        'HandleVelY': resampled_vel_y,
                        'HandleAccX': resampled_acc_x,
                        'HandleAccY': resampled_acc_y,
                        'TargetEndPosX': trial_df['TargetEndPosX'].iloc[0],
                        'TargetEndPosY': trial_df['TargetEndPosY'].iloc[0],
                        'original_length': original_length
                    })

                    processed_trajectories.append(trajectory_df)

                except ValueError as e:
                    print(f"è£œå®Œã‚¨ãƒ©ãƒ¼: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num}: {e}")
                    continue

        # å…¨ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’çµåˆ
        if processed_trajectories:
            return pd.concat(processed_trajectories, ignore_index=True)
        else:
            return pd.DataFrame()   #ç©ºã®DataFrameã‚’è¿”ã™

    def get_subjects(self) -> List[str]:
        """è¢«é¨“è€…IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.df['subject_id'].unique().to_list()

    def get_raw_data(self) -> Optional[pd.DataFrame]:
        if self.raw_data_df is not None:
            return self.raw_data_df
        else:
            return None

    def get_preprocessed_data(self, block: int) -> Optional[pd.DataFrame]:
        """æŒ‡å®šã—ãŸãƒ–ãƒ­ãƒƒã‚¯ã®å‰å‡¦ç†æ¸ˆã¿DataFrameã‚’è¿”ã™"""
        if self.preprocessed_data_df is not None:
            if block== 0:
                filtered_df = self.preprocessed_data_df
            else:
                filtered_df = self.preprocessed_data_df[self.preprocessed_data_df['block'] == block]

            return filtered_df
        else:
            return None


class SkillMetricCalculator:
    """å„ç¨®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®è¨ˆç®—ã‚’æ‹…å½“"""
    def __init__(self, config: Dict):
        """è¨­å®šã‚’å—ã‘å–ã‚ŠåˆæœŸåŒ–"""
        self.config = config
        self.target_seq_len = config['pre_process']['target_seq_len']

    
    def calculate_skill_metrics(self, preprocessed_data: pd.DataFrame) -> pd.DataFrame:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ç¨®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦æ¨™æº–åŒ–"""
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if preprocessed_data is None or len(preprocessed_data) == 0:
            print("âš ï¸ ã‚¹ã‚­ãƒ«æŒ‡æ¨™è¨ˆç®—: å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return pd.DataFrame()  # ç©ºã®DataFrameã‚’è¿”ã™
        
        skill_metrics = []
        
        for (subject_id, trial_num, block), trial_group in preprocessed_data.groupby(['subject_id', 'trial_num', 'block']):
            try:
                # å„ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‹ã‚‰è»Œé“æ›²ç‡ã€é€Ÿåº¦æ»‘ã‚‰ã‹ã•ã€åŠ é€Ÿåº¦æ»‘ã‚‰ã‹ã•ã€ã‚¸ãƒ£ãƒ¼ã‚¯ã€åˆ¶å¾¡å®‰å®šæ€§ã€æ™‚é–“çš„ä¸€è²«æ€§ã€å‹•ä½œæ™‚é–“ã€çµ‚ç‚¹èª¤å·®ã‚’è¨ˆç®—
                curvature = self.calculate_curvature(trial_group)
                velocity_smoothness = self.calculate_velocity_smoothness(trial_group)
                acceleration_smoothness = self.calculate_acceleration_smoothness(trial_group)
                jerk_score = self.calculate_jerk(trial_group)
                control_stability = self.calculate_control_stability(trial_group)
                temporal_consistency = self.calculate_temporal_consistency(trial_group)
                trial_time = self.calculate_trial_time(trial_group)
                endpoint_error = self.calculate_endpoint_error(trial_group)

                # çµæœã‚’ã¾ã¨ã‚ã‚‹
                metrics = {
                    'subject_id': subject_id,
                    'trial_num': trial_num,
                    'block': block,
                    'curvature': curvature,
                    'velocity_smoothness': velocity_smoothness,
                    'acceleration_smoothness': acceleration_smoothness,
                    'jerk_score': jerk_score,
                    'control_stability': control_stability,
                    'temporal_consistency': temporal_consistency,
                    'trial_time': trial_time,
                    'endpoint_error': endpoint_error,
                }
                skill_metrics.append(metrics)
                
            except Exception as e:
                print(f"ã‚¹ã‚­ãƒ«æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num}: {e}")
                continue
        
        skill_metrics_df =  pd.DataFrame(skill_metrics)

        # Z-scoreæ¨™æº–åŒ–ã®å®Ÿè¡Œ
        # if self.config.get('analysis', {}).get('standardize_metrics', True):
        #     skill_metrics_df = self._standardize_metrics(skill_metrics_df)

        return skill_metrics_df

    @staticmethod
    def standardize_metrics(metrics_df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’æ¨™æº–åŒ–ã™ã‚‹"""
        standardized_df = metrics_df.copy()

        # é€šçŸ¥åˆ—ã®ã¿ã‚’æ¨™æº–åŒ–å¯¾è±¡ã¨ã™ã‚‹
        metric_directions = {'curvature': False,
                              'velocity_smoothness': True,
                              'acceleration_smoothness': True,
                              'jerk_score': False,
                              'control_stability': True,
                              'temporal_consistency': True,
                              'trial_time': False,
                              'endpoint_error': False,
                              }

        for col in metric_directions.keys():
            if col in standardized_df.columns:
                values = standardized_df[col].dropna()
                if len(values) > 1:
                    mean_val = values.mean()
                    std_val = values.std()
                    if std_val > 0:
                        z_scores = (standardized_df[col] - mean_val) / std_val
                        # ä½ã„å€¤ãŒè‰¯ã„æŒ‡æ¨™ã¯ç¬¦å·ã‚’åè»¢
                        if not metric_directions[col]:
                            z_scores = -z_scores
                        standardized_df[col] = z_scores

        return standardized_df

    @staticmethod
    def calculate_curvature(trial_data: pd.DataFrame):
        """è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—"""
        positions = trial_data[['HandlePosX','HandlePosY']].values

        if len(positions) < 3:
            return np.nan

        curvatures = []
        for i in range(1,len(positions) - 1):
            p1, p2, p3 = positions[i - 1], positions[i], positions[i + 1]

            # 3ç‚¹ã‹ã‚‰æ›²ç‡ã‚’è¨ˆç®—
            a = np.linalg.norm(p2 - p1)
            b = np.linalg.norm(p3 - p2)
            c = np.linalg.norm(p3 - p1)

            if a > 0 and b > 0 and c > 0:
                s = (a + b + c) / 2
                area = np.sqrt(max(0, s * (s - a) * (s - b) * (s - c)))
                curvature = 4 * area / (a * b * c) if (a * b * c) > 0 else 0
                curvatures.append(curvature)

        return np.mean(curvatures) if curvatures else 0.0

    @staticmethod
    def calculate_velocity_smoothness(trial_data: pd.DataFrame):
        """é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•ã‚’è¨ˆç®—"""
        velocities = trial_data[['HandleVelX','HandleVelY']].values

        if len(velocities) < 2:
            return np.nan

        vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
        velocity_changes = np.abs(np.diff(vel_magnitude))

        # æ­£è¦åŒ–ã•ã‚ŒãŸæ»‘ã‚‰ã‹ã•æŒ‡æ¨™
        smoothness = 1.0 / (1.0 + np.std(velocity_changes))
        return smoothness

    @staticmethod
    def calculate_acceleration_smoothness(trial_data: pd.DataFrame):
        """åŠ é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•"""

        if len(trial_data) < 3:
            return np.nan

        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values
        acc_magnitude = np.sqrt(acc_x ** 2 + acc_y ** 2)

        acc_changes = np.abs(np.diff(acc_magnitude))
        return 1.0 / (1.0 + np.std(acc_changes))

    @staticmethod
    def calculate_control_stability(trial_data: pd.DataFrame):
        """åˆ¶å¾¡å®‰å®šæ€§æŒ‡æ¨™"""
        if len(trial_data) < 5:
            return np.nan

        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values

        # åŠ é€Ÿåº¦ã®æ¨™æº–åå·®ï¼ˆåˆ¶å¾¡ã®å®‰å®šæ€§ã®é€†æŒ‡æ¨™ï¼‰
        acc_std = np.std(np.sqrt(acc_x ** 2 + acc_y ** 2))
        stability = 1.0 / (1.0 + acc_std)

        return stability

    @staticmethod
    def calculate_temporal_consistency(trial_data: pd.DataFrame):
        """æ™‚é–“çš„ä¸€è²«æ€§"""
        if len(trial_data) < 10:
            return np.nan

        timestamps = trial_data['Timestamp'].values
        time_intervals = np.diff(timestamps)

        # æ™‚é–“é–“éš”ã®ä¸€è²«æ€§
        consistency = 1.0 / (1.0 + np.std(time_intervals) / np.mean(time_intervals))
        return consistency

    @staticmethod
    def calculate_trial_time(trial_data: pd.DataFrame) -> float:
        """å‹•ä½œæ™‚é–“ã®è¨ˆç®— - è»Œé“ã®é•·ã•ã‹ã‚‰æ¨å®š"""
        time_stamps = trial_data['Timestamp'].values

        return time_stamps[-1] - time_stamps[0]

    @staticmethod
    def calculate_endpoint_error(trial_data: pd.DataFrame) -> float:
        """çµ‚ç‚¹èª¤å·®ã®è¨ˆç®— - ç›®æ¨™ä½ç½®ã‹ã‚‰ã®è·é›¢"""
        # æœ€å¾Œã®ä½ç½®ã‚’çµ‚ç‚¹ã¨ã—ã¦ä½¿ç”¨
        final_x = trial_data['HandlePosX'].iloc[-1]
        final_y = trial_data['HandlePosY'].iloc[-1]

        target_x = trial_data['TargetEndPosX'].iloc[-1]
        target_y = trial_data['TargetEndPosY'].iloc[-1]
        
        endpoint_error = np.sqrt((final_x - target_x)**2 + (final_y - target_y)**2)
        return endpoint_error

    @staticmethod
    def calculate_jerk(trial_data: pd.DataFrame) -> float:
        """ã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆåŠ é€Ÿåº¦ã®å¤‰åŒ–ç‡ï¼‰ã®è¨ˆç®—"""
        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values
        
        # åŠ é€Ÿåº¦ã®å¾®åˆ†ã§ã‚¸ãƒ£ãƒ¼ã‚¯ã‚’è¨ˆç®—
        jerk_x = np.diff(acc_x)
        jerk_y = np.diff(acc_y)
        
        # ã‚¸ãƒ£ãƒ¼ã‚¯ã®å¤§ãã•ã‚’è¨ˆç®—ã—ã€å¹³å‡ã‚’å–ã‚‹
        jerk_magnitude = np.sqrt(jerk_x**2 + jerk_y**2)
        jerk_score = np.mean(jerk_magnitude)
        
        return jerk_score


class SkillAnalyzer:
    """ANOVA,å› å­è§£æãªã©ã®çµ±è¨ˆè§£æã‚’æ‹…å½“"""
    def __init__(self, config: Dict, output_manager: OutputManager):
        """è¨­å®šã‚’å—ã‘å–ã‚ŠåˆæœŸåŒ–"""
        self.config = config
        self.output_manager = output_manager
        self.analysis_config = config.get('analysis', {})

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—ç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.used_standard_scaler = None
        self.used_factor_analysis = None
    
    def analyze_skill_metrics(self, skill_metrics_df: pd.DataFrame, rotation: str = 'promax') -> Dict:
        """ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®çµ±è¨ˆè§£æã‚’å®Ÿè¡Œ"""
        results = {}
        
        if self.analysis_config.get('anova_skill_metrics', False):
            print("ANOVAè§£æã‚’å®Ÿè¡Œä¸­...")
            results['anova'] = self._perform_anova_analysis(skill_metrics_df)

            # ANOVAè§£æçµæœã®å¯è¦–åŒ–
            self._save_anova_plots(skill_metrics_df, results['anova'])

        if self.analysis_config.get('factorize_skill_metrics', False):
            print("å› å­åˆ†æã‚’å®Ÿè¡Œä¸­...")
            results['factor_analysis'] = self._perform_factor_analysis(skill_metrics_df, rotation)

            if 'error' not in results['factor_analysis']:
                self._save_factor_analysis_plots(skill_metrics_df, results['factor_analysis'])
            else:
                print(f"âš ï¸ å› å­åˆ†æãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ: {results['factor_analysis']['error']}")

        return results
    
    def _perform_anova_analysis(self, skill_metrics_df: pd.DataFrame) -> Dict:
        """ANOVAè§£æã®å®Ÿè¡Œ"""
        anova_results = {}
        
        # æ•°å€¤ã‚¹ã‚­ãƒ«æŒ‡æ¨™åˆ—ã‚’å–å¾—
        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                        'jerk_score', 'control_stability', 'temporal_consistency', 
                        'trial_time', 'endpoint_error']
        
        for skill_metric in skill_columns:
            if skill_metric in skill_metrics_df.columns:
                try:
                    # è¢«é¨“è€…é–“ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™å·®ç•°ã‚’ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æã§æ¤œå®š
                    anova_result = self._one_way_anova(skill_metrics_df, skill_metric)
                    anova_results[skill_metric] = anova_result
                    
                except Exception as e:
                    print(f"ANOVAè§£æã‚¨ãƒ©ãƒ¼ ({skill_metric}): {e}")
                    anova_results[skill_metric] = {'error': str(e)}
        
        return anova_results
    
    def _one_way_anova(self, data: pd.DataFrame, metric_name: str) -> Dict:
        """ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æã®å®Ÿè¡Œ"""
        # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups = []
        subject_ids = []
        
        for subject_id, subject_data in data.groupby('subject_id'):
            metric_values = subject_data[metric_name].dropna()
            if len(metric_values) > 0:
                groups.append(metric_values.values)
                subject_ids.append(subject_id)
        
        if len(groups) < 2:
            return {'error': 'åˆ†æã«ååˆ†ãªã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒã‚ã‚Šã¾ã›ã‚“'}
        
        # ANOVAæ¤œå®šã®å®Ÿè¡Œ
        f_statistic, p_value = stats.f_oneway(*groups)
        
        # åŠ¹æœé‡ï¼ˆeta squaredï¼‰ã®è¨ˆç®—
        total_variance = np.var(np.concatenate(groups))
        within_variance = np.mean([np.var(group) for group in groups])
        eta_squared = (total_variance - within_variance) / total_variance
        
        return {
            'f_statistic': f_statistic,
            'p_value': p_value,
            'eta_squared': eta_squared,
            'significant': p_value < 0.05,
            'groups_count': len(groups),
            'subject_ids': subject_ids
        }
    
    def _perform_factor_analysis(self, skill_metrics_df: pd.DataFrame, rotation: str) -> Dict:
        """å› å­åˆ†æã®å®Ÿè¡Œ"""
        try:
            # æ•°å€¤ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®ã¿ã‚’æŠ½å‡º
            skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                            'jerk_score', 'control_stability', 'temporal_consistency', 
                            'trial_time', 'endpoint_error']
            
            # æ¬ æå€¤ã‚’é™¤å»ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            analysis_data = skill_metrics_df[skill_columns].dropna()
            
            if len(analysis_data) < 10:
                return {'error': 'å› å­åˆ†æã«ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°ãŒã‚ã‚Šã¾ã›ã‚“'}
            
            # æ¨™æº–åŒ–ï¼ˆå¿µã®ãŸã‚å†åº¦å®Ÿè¡Œï¼‰
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(analysis_data)
            
            # å› å­æ•°ã®æ±ºå®šï¼ˆå›ºæœ‰å€¤>1ã®åŸºæº–ï¼‰
            correlation_matrix = np.corrcoef(scaled_data.T)
            eigenvalues = np.linalg.eigvals(correlation_matrix)
            n_factors = max(1, np.sum(eigenvalues > 1))
            
            # å› å­åˆ†æã®å®Ÿè¡Œ
            if not FACTOR_ANALYZER_AVAILABLE:
                return {'error': 'factor_analyzer ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“'}
            
            fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)
            fa.fit(scaled_data)
            
            # å› å­è² è·é‡ã®è¨ˆç®—
            # factor_loadings = fa.components_.T
            factor_loadings = fa.loadings_
            
            # å› å­å¾—ç‚¹ã®è¨ˆç®—
            factor_scores = fa.transform(scaled_data)

            # ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¿å­˜
            self.used_standard_scaler = scaler
            self.used_factor_analysis = fa

            result = {
                'n_factors': n_factors,
                'factor_loadings': factor_loadings,
                'factor_scores': factor_scores,
                'explained_variance': eigenvalues[:n_factors],
                'skill_columns': skill_columns,
                'sample_size': len(analysis_data)
            }

            if rotation == 'promax':
                factor_correlation = fa.phi_
                result['factor_correlations'] = factor_correlation

            return result
            
        except Exception as e:
            return {'error': f'å› å­åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}'}

    def _save_anova_plots(self, skill_metrics_df: pd.DataFrame, anova_results: Dict):
        output_dir = self.output_manager.skill_analyzer_output_dir_path
        output_dir.mkdir(parents=True, exist_ok=True)

        # æœ‰æ„å·®ã®ã‚ã‚‹æŒ‡æ¨™ã¨ç„¡ã„æŒ‡æ¨™ã§åˆ†ã‘ã¦è¡¨ç¤º
        significant_metrics = []
        non_significant_metrics = []

        metrics = []
        eta_values = []
        p_values = []

        for metric, result  in anova_results.items():
            if 'error' not in result:
                if result.get('significant', False):
                    significant_metrics.append(metric)
                else:
                    non_significant_metrics.append(metric)

                metrics.append(metric)
                eta_values.append(result['eta_squared'])
                p_values.append(result['p_value'])

        # æœ‰æ„å·®ã‚ã‚Šã®æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if significant_metrics:
            self._create_boxplot_grid(skill_metrics_df, significant_metrics, anova_results, output_dir / 'significant_results.png', "Significant Skill Metrics")

        # æœ‰æ„å·®ãªã—ã®æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if non_significant_metrics:
            self._create_boxplot_grid(skill_metrics_df, non_significant_metrics, anova_results, output_dir / 'non_significant_results.png',
                                      "Non Significant Skill Metrics")

        # å„æŒ‡æ¨™ã®åŠ¹æœé‡ã€på€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if metrics:
            self._create_bar_plot(metrics, eta_values, p_values, output_dir/ 'eta_result.png', "Comparison Effectiveness")

    def _create_boxplot_grid(self, data: pd.DataFrame, metrics: List[str], anova_results: Dict, save_path: Path, title: str):
        """è¤‡æ•°æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º"""
        n_metrics = len(metrics)
        n_cols = min(3, n_metrics)
        n_rows = (n_metrics + n_cols - 1) // n_cols

        # 8.5cm = 3.35 inches width, adjust height based on number of rows
        fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize = (3.35, 2.5 * n_rows))

        if n_metrics == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.flatten() if n_cols > 1 else [axes]
        else:
            axes = axes.flatten()

        for i, metric in enumerate(metrics):
            ax = axes[i]

            # CLAUDE_ADDED: è¢«é¨“è€…åã‚’åŒ¿ååŒ–
            data_copy = data.copy()
            subject_mapping = anonymize_subjects(data_copy['subject_id'])
            data_copy['subject_id'] = data_copy['subject_id'].map(subject_mapping)

            # ç®±ã²ã’å›³ã®ä½œæˆ
            sns.boxplot(data=data_copy, x='subject_id', y=metric, ax=ax)
            ax.set_title(f'{metric}\n(p={anova_results[metric]["p_value"]:.3f})',
                         fontsize=6)
            ax.set_xlabel('Subject ID', fontsize=6)
            ax.set_ylabel('Skill Metrics', fontsize=6)
            ax.tick_params(axis='x', rotation=45, labelsize=4)

            # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for j in range(i + 1, len(axes)):
            axes[j].set_visible(False)

        fig.suptitle(title, fontsize=16, y=1.02)
        fig.tight_layout()

        try:
            # CLAUDE_ADDED: Use academic figure saving function
            save_academic_figure(fig, save_path)
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_bar_plot(self, metrics: List, eta_values: List, p_values:List, save_path: Path, title: str):
        """åŠ¹æœé‡ã¨på€¤ã®æ£’ã‚°ãƒ©ãƒ•ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        data = {
            'metrics': metrics,
            'eta': eta_values,
            'p_value': p_values
        }
        df = pd.DataFrame(data)

        # 8.5cm = 3.35 inches width for dual subplot
        fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (3.35, 2.0))

        # åŠ¹æœé‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
        sns.barplot(data=df, x='metrics', y='eta', ax=axes[0])
        axes[0].set_title('Effectiveness', fontsize=8)
        axes[0].set_xlabel('Metrics', fontsize=6)
        axes[0].set_ylabel('Eta Squared', fontsize=6)
        axes[0].tick_params(axis='x', rotation=45, labelsize=4)
        axes[0].tick_params(axis='y', labelsize=6)

        # på€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ
        sns.barplot(data=df, x='metrics', y='p_value', ax=axes[1])
        axes[1].set_title('p-value', fontsize=8)
        axes[1].set_xlabel('Metrics', fontsize=6)
        axes[1].set_ylabel('p-value', fontsize=6)
        axes[1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Î±=0.05')
        axes[1].legend(fontsize=5)
        axes[1].tick_params(axis='x', rotation=45, labelsize=4)
        axes[1].tick_params(axis='y', labelsize=6)

        fig.suptitle(title, fontsize=10, y=1.02)
        fig.tight_layout()

        try:
            # CLAUDE_ADDED: Use academic figure saving function
            save_academic_figure(fig, save_path)
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _save_factor_analysis_plots(self, skill_metrics_df: pd.DataFrame, factor_analysis_results: Dict):
        """å› å­åˆ†æçµæœã®å¯è¦–åŒ–"""
        output_dir = self.output_manager.skill_analyzer_output_dir_path
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        self._create_factor_loading_heatmap(factor_analysis_results, output_dir / 'factor_loadings_heatmap.png')

        # 2. å› å­ç‰¹å…¸æ•£å¸ƒå›³(æœ€åˆã®2å› å­)
        if factor_analysis_results['n_factors'] >= 2:
            self._create_factor_score_scatter(skill_metrics_df, factor_analysis_results,
                                              output_dir / 'factor_scores_scatter.png')
        # 3. å› å­å¯„ä¸ç‡æ£’ã‚°ãƒ©ãƒ•
        self._create_factor_variance_plot(factor_analysis_results, output_dir/ 'factor_variance_explained.png')

        # 4. promax å›è»¢ã®å ´åˆã¯å› å­ç›¸é–¢è¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        self._create_factor_correlation_heatmap(factor_analysis_results, output_dir / 'factor_correlation_heatmap.png')

    def analyze_block_wise_factor_analysis(self, skill_metrics_df: pd.DataFrame, rotation: str = 'promax'):
        """ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æã‚’å®Ÿè¡Œã—ã€2x2ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
        print("ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æã‚’å®Ÿè¡Œä¸­...")

        output_dir = self.output_manager.skill_analyzer_output_dir_path
        output_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ–ãƒ­ãƒƒã‚¯1-4ã®å› å­åˆ†æçµæœã‚’æ ¼ç´
        block_results = {}

        for block in range(1, 5):
            print(f"ãƒ–ãƒ­ãƒƒã‚¯{block}ã®å› å­åˆ†æã‚’å®Ÿè¡Œä¸­...")
            block_data = skill_metrics_df[skill_metrics_df['block'] == block]

            if len(block_data) < 10:
                print(f"âš ï¸ ãƒ–ãƒ­ãƒƒã‚¯{block}: ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(block_data)})")
                continue

            # ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æã‚’å®Ÿè¡Œ
            factor_result = self._perform_factor_analysis(block_data, rotation)

            if 'error' not in factor_result:
                # ãƒ–ãƒ­ãƒƒã‚¯æƒ…å ±ã‚’è¿½åŠ 
                factor_result['block'] = block
                factor_result['block_data'] = block_data
                block_results[block] = factor_result
                print(f"âœ… ãƒ–ãƒ­ãƒƒã‚¯{block}ã®å› å­åˆ†æå®Œäº†")
            else:
                print(f"âŒ ãƒ–ãƒ­ãƒƒã‚¯{block}ã®å› å­åˆ†æã‚¨ãƒ©ãƒ¼: {factor_result['error']}")

        if len(block_results) < 4:
            print(f"âš ï¸ 4ãƒ–ãƒ­ãƒƒã‚¯å…¨ã¦ã®å› å­åˆ†æãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚å®Œäº†æ•°: {len(block_results)}")

        # 2x2ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ
        if block_results:
            self._create_block_wise_2x2_plots(block_results, output_dir / 'block_wise_factor_analysis.png')

        return block_results

    def _create_block_wise_2x2_plots(self, block_results: Dict, save_path: Path):
        """ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æçµæœã‚’2x2ãƒ—ãƒ­ãƒƒãƒˆã§è¡¨ç¤º"""
        print("ãƒ–ãƒ­ãƒƒã‚¯æ¯2x2ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆä¸­...")

        # å…¨è¢«é¨“è€…ã®çµ±ä¸€ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        all_subjects = set()
        for block_data in block_results.values():
            all_subjects.update(block_data['block_data']['subject_id'].unique())
        global_subject_mapping = anonymize_subjects(list(all_subjects))

        # 2x2ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ (8.5cm = 3.35 inches width)
        fig, axes = plt.subplots(2, 2, figsize=(3.35, 3.35))
        axes = axes.flatten()

        for idx, block in enumerate(range(1, 5)):
            ax = axes[idx]

            if block not in block_results:
                ax.text(0.5, 0.5, f'Block {block}\nNo Data', ha='center', va='center',
                       transform=ax.transAxes, fontsize=2)
                ax.set_title(f'Block {block}', fontsize=4)
                continue

            result = block_results[block]
            block_data = result['block_data']

            # å› å­å¾—ç‚¹ãŒ2å› å­ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿æ•£å¸ƒå›³ã‚’æç”»
            if result['n_factors'] >= 2:
                factor_scores = result['factor_scores']

                # è¢«é¨“è€…æƒ…å ±ã‚’å–å¾—
                analysis_data = block_data[result['skill_columns']].dropna()
                subject_info = block_data.loc[analysis_data.index, ['subject_id']]

                # è¢«é¨“è€…ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦æ•£å¸ƒå›³ã‚’ä½œæˆ
                unique_subjects = subject_info['subject_id'].unique()
                colors = plt.cm.tab10(np.linspace(0, 1, len(unique_subjects)))

                for i, subject in enumerate(unique_subjects):
                    mask = subject_info['subject_id'] == subject
                    if np.sum(mask) > 0:
                        subject_scores = factor_scores[mask]
                        ax.scatter(subject_scores[:, 0], subject_scores[:, 1],
                                 c=[colors[i]], label=global_subject_mapping[subject],
                                 alpha=0.7, s=1)

                # è»¸ã®è¨­å®š
                ax.set_xlabel('Factor 1 Score',fontsize=8)
                ax.set_ylabel('Factor 2 Score',fontsize=8)
                # ax.grid(True, alpha=0.3, linewidth=0.2)

                ax.tick_params(axis='both', labelsize=6)

                # è»¸ã®äº¤ç‚¹ã«ç·šã‚’è¿½åŠ 
                # ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
                # ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)

                # å·¦ä¸Šã®æ•£å¸ƒå›³ï¼ˆBlock 1ï¼‰ã«ã®ã¿å‡¡ä¾‹ã‚’è¿½åŠ 
                if block == 1:
                    ax.legend(loc='upper left',
                             fontsize=4, frameon=True, fancybox=True,
                             shadow=True, framealpha=0.8, markerscale=2)
            else:
                ax.text(0.5, 0.5, f'Block {block}\nInsufficient Factors\n({result["n_factors"]} factor)',
                       ha='center', va='center', transform=ax.transAxes, fontsize=1)
                ax.set_title(f'Block {block}', fontsize=6)

        plt.tight_layout()

        # å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close(fig)

        # è¿½åŠ ï¼šãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚‚2x2ã§ä½œæˆ
        self._create_block_wise_heatmaps(block_results, save_path.parent / 'block_wise_factor_loadings.png')

    def _create_block_wise_heatmaps(self, block_results: Dict, save_path: Path):
        """ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’2x2ã§è¡¨ç¤º"""
        print("ãƒ–ãƒ­ãƒƒã‚¯æ¯ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆä¸­...")

        # 2x2ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ (8.5cm = 3.35 inches width)
        fig, axes = plt.subplots(2, 2, figsize=(3.35, 3.35))
        axes = axes.flatten()

        for idx, block in enumerate(range(1, 5)):
            ax = axes[idx]

            if block not in block_results:
                ax.text(0.5, 0.5, f'Block {block}\nNo Data', ha='center', va='center',
                       transform=ax.transAxes, fontsize=2)
                ax.set_title(f'Block {block}', fontsize=4)
                ax.set_xticks([])
                ax.set_yticks([])
                continue

            result = block_results[block]

            # å› å­è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
            loadings = result['factor_loadings']
            skill_columns = result['skill_columns']
            n_factors = result['n_factors']

            # DataFrameã«å¤‰æ›
            loading_df = pd.DataFrame(
                loadings,
                index=skill_columns,
                columns=[f'F{i+1}' for i in range(n_factors)]
            )

            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»
            heatmap = sns.heatmap(loading_df, annot=True, cmap='RdBu_r', center=0,
                       fmt='.2f', ax=ax, cbar=True,
                       cbar_kws={'shrink': 0.8}, annot_kws={'fontsize': 4})

            # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´
            if heatmap.collections:
                colorbar = heatmap.collections[0].colorbar
                if colorbar:
                    colorbar.ax.tick_params(labelsize=4)

            ax.set_title(f'Block {block}', fontsize=6)
            ax.set_xlabel('Factor', fontsize=6)
            ax.set_ylabel('Skill Metrics', fontsize=6)
            # ç›®ç››ã‚Šç·šã‚’ç´°ãã—ã€ã‚¹ã‚­ãƒ«æŒ‡æ¨™åã‚’æ–œã‚ã«å›è»¢
            ax.tick_params(axis='both', labelsize=4, width=0.5, length=2)
            ax.tick_params(axis='y', rotation=45)

        # plt.suptitle('Block-wise Factor Analysis: Factor Loading Matrix', fontsize=18, y=0.98)
        plt.tight_layout()

        # å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close(fig)

    def _create_factor_loading_heatmap(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ"""
        loadings = factor_analysis_results['factor_loadings']
        skill_columns = factor_analysis_results['skill_columns']
        n_factors = factor_analysis_results['n_factors']

        # DataFrameã«å¤‰æ›
        loading_df = pd.DataFrame(
            loadings,
            index=skill_columns,
            columns=[f'Factor {i+1}' for i in range(n_factors)]
        )

        # 8.5cm = 3.35 inches width for factor loading heatmap
        fig, axes = plt.subplots(figsize = (3.35, 2.5))
        heatmap = sns.heatmap(loading_df, annot=True, cmap='RdBu_r', center=0,
                    fmt ='.2f', ax=axes, cbar_kws={'label': 'Factor Loading'}, annot_kws={'fontsize': 6})

        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if heatmap.collections:
            colorbar = heatmap.collections[0].colorbar
            if colorbar:
                colorbar.ax.tick_params(labelsize=6)
                colorbar.set_label('Factor Loading', fontsize=8)

        axes.set_title('Factor Loading Matrix', fontsize=10)
        axes.set_xlabel('Factor', fontsize=8)
        axes.set_ylabel('Skill Metrics', fontsize=8)
        axes.tick_params(axis='both', labelsize=6)

        try:
            # CLAUDE_ADDED: Use academic figure saving function
            save_academic_figure(fig, save_path)
        except Exception as e:
            print(f"âŒ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_factor_score_scatter(self, skill_metrics_df: pd.DataFrame, factor_analysis_results: Dict, save_path: Path):
        """å› å­ç‰¹å…¸ã®æ•£å¸ƒå›³ä½œæˆ(ç¬¬ä¸€å› å­ vs ç¬¬äºŒå› å­)"""
        factor_scores = factor_analysis_results['factor_scores']

        # è¢«é¨“è€…æƒ…å ±ã‚’è¿½åŠ 
        analysis_data = skill_metrics_df[factor_analysis_results['skill_columns']].dropna()
        subject_info = skill_metrics_df.loc[analysis_data.index, ['subject_id', 'block']]

        # CLAUDE_ADDED: è¢«é¨“è€…åã®åŒ¿ååŒ–
        subject_mapping = anonymize_subjects(subject_info['subject_id'])

        # 8.5cm = 3.35 inches width for factor score scatter plot
        fig, axes = plt.subplots(figsize=(3.35, 2.8))

        # è¢«é¨“è€…ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦æ•£å¸ƒå›³ã‚’ä½œæˆ
        unique_subjects = subject_info['subject_id'].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_subjects)))

        for i, subject in enumerate(unique_subjects):
            mask = subject_info['subject_id'] == subject
            if np.sum(mask) > 0:
                subject_scores = factor_scores[mask]
                # CLAUDE_ADDED: åŒ¿ååŒ–ã•ã‚ŒãŸè¢«é¨“è€…åã‚’ä½¿ç”¨
                axes.scatter(subject_scores[:, 0], subject_scores[:, 1],
                           c=[colors[i]], label=subject_mapping[subject], alpha=0.7, s=50)

        axes.set_xlabel('Factor 1 score', fontsize=8)
        axes.set_ylabel('Factor 2 score', fontsize=8)
        axes.set_title('Factor Scatter Plotï¼ˆPer subjectsï¼‰', fontsize=10)
        axes.grid(True, alpha=0.3)
        axes.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=6)
        axes.tick_params(axis='both', labelsize=6)

        # è»¸ã®äº¤ç‚¹ã«ç·šã‚’è¿½åŠ 
        axes.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        axes.axvline(x=0, color='k', linestyle='-', alpha=0.3)

        # CLAUDE_ADDED: å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close(fig)

    def _create_factor_variance_plot(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­ã®å¯„ä¸ç‡ï¼ˆèª¬æ˜åˆ†æ•£ï¼‰æ£’ã‚°ãƒ©ãƒ•"""
        eigenvalues = factor_analysis_results['explained_variance']
        n_factors = factor_analysis_results['n_factors']

        # å¯„ä¸ç‡ã®è¨ˆç®—
        total_variance = np.sum(eigenvalues)
        contribution_ratios = eigenvalues / total_variance * 100
        cumulative_ratios = np.cumsum(contribution_ratios)

        factor_names = [f'Factor {i + 1}' for i in range(n_factors)]

        # 8.5cm = 3.35 inches width for factor variance plot (dual subplot)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(3.35, 2.5))

        # å¯„ä¸ç‡æ£’ã‚°ãƒ©ãƒ•
        bars = ax1.bar(factor_names, contribution_ratios, alpha=0.7, color='steelblue')
        ax1.set_title('Explained Variance', fontsize=8)
        ax1.set_ylabel('Explained Variance (%)', fontsize=6)
        ax1.set_xlabel('Factor', fontsize=6)
        ax1.tick_params(axis='both', labelsize=6)

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, ratio in zip(bars, contribution_ratios):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{ratio:.1f}%', ha='center', va='bottom', fontsize=5)

        # ç´¯ç©å¯„ä¸ç‡æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
        ax2.plot(factor_names, cumulative_ratios, marker='o', color='red', linewidth=2)
        ax2.set_title(' Cumulative Explained Variance ', fontsize=8)
        ax2.set_ylabel(' Cumulative Explained Variance  (%)', fontsize=6)
        ax2.set_xlabel('Factor', fontsize=6)
        ax2.grid(True, alpha=0.3)
        ax2.tick_params(axis='both', labelsize=6)

        # å€¤ã‚’ç‚¹ã®ä¸Šã«è¡¨ç¤º
        for i, ratio in enumerate(cumulative_ratios):
            ax2.text(i, ratio + 2, f'{ratio:.1f}%', ha='center', va='bottom', fontsize=5)

        fig.suptitle('Factor Analysisï¼šExplained Variance', fontsize=10, y=1.02)
        fig.tight_layout()

        # CLAUDE_ADDED: å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close(fig)

    def _create_factor_correlation_heatmap(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ"""

        # å› å­ç›¸é–¢è¡Œåˆ—ã®å­˜åœ¨ç¢ºèª
        fa_correlation = factor_analysis_results.get('factor_correlations')
        if fa_correlation is None:
            print("ğŸ’¡ å› å­é–“ç›¸é–¢è¡Œåˆ—ã¯æ–œäº¤å›è»¢ï¼ˆpromaxãªã©ï¼‰ã®å ´åˆã®ã¿ãƒ—ãƒ­ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚")
            return

        # 8.5cm = 3.35 inches width for factor correlation heatmap
        fig = plt.figure(figsize=(3.35, 2.8))
        heatmap = sns.heatmap(fa_correlation,
                    annot=True,
                    fmt='.2f',
                    cmap='RdBu_r',
                    vmin=-1,
                    vmax=1,
                    annot_kws={'fontsize': 6})

        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if heatmap.collections:
            colorbar = heatmap.collections[0].colorbar
            if colorbar:
                colorbar.ax.tick_params(labelsize=6)

        plt.title('Factor Correlation Matrix', fontsize=10)
        plt.xlabel('Factor', fontsize=8)
        plt.ylabel('Factor', fontsize=8)
        plt.tick_params(axis='both', labelsize=6)

        # CLAUDE_ADDED: å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close()


    @property
    def factorize_artifact(self):
        if self.used_standard_scaler is not None and self.used_factor_analysis is not None:
            return self.used_standard_scaler, self.used_factor_analysis
        else:
            print("å­¦ç¿’æ¸ˆã¿Standard Scalerã¨Factor Analysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return None

    # CLAUDE_ADDED: å› å­å¯„ä¸ç‡ã‚’å–å¾—ã™ã‚‹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    @property
    def factor_contribution_ratios(self):
        """å› å­åˆ†æçµæœã‹ã‚‰å› å­å¯„ä¸ç‡ã‚’è¨ˆç®—ã—ã¦è¿”ã™"""
        if hasattr(self, 'used_factor_analysis') and self.used_factor_analysis is not None:
            # å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å›ºæœ‰å€¤ã‚’å–å¾—
            # FactorAnalyzerã®å ´åˆã€get_eigenvalues()ãƒ¡ã‚½ãƒƒãƒ‰ã§å›ºæœ‰å€¤ã‚’å–å¾—
            try:
                eigenvalues, _ = self.used_factor_analysis.get_eigenvalues()
                n_factors = self.used_factor_analysis.n_factors

                # ä½¿ç”¨ã—ã¦ã„ã‚‹å› å­æ•°åˆ†ã®å›ºæœ‰å€¤ã‚’å–å¾—
                factor_eigenvalues = eigenvalues[:n_factors]

                # å¯„ä¸ç‡ã®è¨ˆç®—: å„å›ºæœ‰å€¤ / å…¨å›ºæœ‰å€¤ã®åˆè¨ˆ
                total_variance = np.sum(eigenvalues)
                contribution_ratios = factor_eigenvalues / total_variance

                print(f"ğŸ’¡ å› å­å¯„ä¸ç‡ã‚’è‡ªå‹•è¨ˆç®—ã—ã¾ã—ãŸ:")
                for i, ratio in enumerate(contribution_ratios):
                    print(f"   Factor {i+1}: {ratio:.4f} ({ratio*100:.2f}%)")

                return contribution_ratios
            except Exception as e:
                print(f"âš ï¸ å› å­å¯„ä¸ç‡ã®è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                return None
        else:
            print("âš ï¸ å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å¯„ä¸ç‡ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“")
            return None


class SkillScoreCalculator:
    """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
    def __init__(self, config: Dict, output_manager: OutputManager, factor_weights: Optional[np.ndarray] = None):
        self.config = config
        self.output = output_manager
        # CLAUDE_ADDED: factor_weightsãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        if factor_weights is not None:
            self.factor_weights = factor_weights
            print(f"ğŸ’¡ æŒ‡å®šã•ã‚ŒãŸå› å­é‡ã¿ã‚’ä½¿ç”¨: {self.factor_weights}")
        else:
            self.factor_weights = np.array([-0.565, 0.245, -0.19])
            print(f"âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å› å­é‡ã¿ã‚’ä½¿ç”¨: {self.factor_weights}")

    def calc_skill_score(self, skill_metrics_df: pd.DataFrame, expert_scaler: StandardScaler, expert_fa) -> pd.DataFrame:
        """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
        plot_data_list = []

        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                         'jerk_score', 'control_stability', 'temporal_consistency',
                         'trial_time', 'endpoint_error']

        for subject_id, subject_df in skill_metrics_df.groupby('subject_id'):
            sorted_trials = subject_df.sort_values(by=['block', 'trial_num']).reset_index()

            for i, trial_row in sorted_trials.iterrows():
                try:
                    # CLAUDE_ADDED: 1ãƒˆãƒ©ã‚¤ã‚¢ãƒ«åˆ†ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’DataFrameã¨ã—ã¦æŠ½å‡ºï¼ˆç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    trial_metrics_df = trial_row[skill_columns].to_frame().T

                    # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿å‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ•°å€¤å‹ã«å¤‰æ›
                    # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦
                    for col in skill_columns:
                        trial_metrics_df[col] = pd.to_numeric(trial_metrics_df[col], errors='coerce')

                    # æ¬ æå€¤ç¢ºèª
                    if trial_metrics_df.isna().any().any():
                        continue

                    # CLAUDE_ADDED: å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã§æ¨™æº–åŒ–ï¼ˆDataFrameå½¢å¼ã§æ¸¡ã—ã¦ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    scaled_metrics = expert_scaler.transform(trial_metrics_df)

                    # å­¦ç¿’æ¸ˆã¿FAãƒ¢ãƒ‡ãƒ«ã§å› å­å¾—ç‚¹ã‚’è¨ˆç®—
                    factor_scores = expert_fa.transform(scaled_metrics)

                    # å› å­å¾—ç‚¹ã‚’é‡ã¿ä»˜ã‘ã—ã¦å˜ä¸€ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«åˆç®—
                    skill_score = np.dot(factor_scores[0], self.factor_weights)

                    plot_data_list.append({
                        'subject_id': subject_id,
                        'trial_order': i + 1,
                        'block': trial_row['block'],
                        'trial_num_in_block': trial_row['trial_num'],
                        'skill_score': skill_score
                    })
                except Exception as e:
                    print(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€… {subject_id}, trial_index {i}: {e}")

        # ãƒªã‚¹ãƒˆã‹ã‚‰æœ€çµ‚çš„ãªDataFrameã‚’ä½œæˆ
        skill_score_df = pd.DataFrame(plot_data_list)

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if skill_score_df is not None:
            self._save_skill_score_plots(skill_score_df)

        return skill_score_df

    # CLAUDE_ADDED: å› å­ã‚¹ã‚³ã‚¢ã‚’å«ã‚€ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ¡ã‚½ãƒƒãƒ‰
    def calc_skill_score_with_factors(self, skill_metrics_df: pd.DataFrame, expert_scaler: StandardScaler, expert_fa) -> pd.DataFrame:
        """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã¨å› å­ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
        plot_data_list = []

        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                         'jerk_score', 'control_stability', 'temporal_consistency',
                         'trial_time', 'endpoint_error']

        n_factors = expert_fa.n_factors

        for subject_id, subject_df in skill_metrics_df.groupby('subject_id'):
            sorted_trials = subject_df.sort_values(by=['block', 'trial_num']).reset_index()

            for i, trial_row in sorted_trials.iterrows():
                try:
                    # 1ãƒˆãƒ©ã‚¤ã‚¢ãƒ«åˆ†ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’DataFrameã¨ã—ã¦æŠ½å‡ºï¼ˆç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    trial_metrics_df = trial_row[skill_columns].to_frame().T

                    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ•°å€¤å‹ã«å¤‰æ›
                    for col in skill_columns:
                        trial_metrics_df[col] = pd.to_numeric(trial_metrics_df[col], errors='coerce')

                    # æ¬ æå€¤ç¢ºèª
                    if trial_metrics_df.isna().any().any():
                        continue

                    # å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã§æ¨™æº–åŒ–ï¼ˆDataFrameå½¢å¼ã§æ¸¡ã—ã¦ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    scaled_metrics = expert_scaler.transform(trial_metrics_df)

                    # å­¦ç¿’æ¸ˆã¿FAãƒ¢ãƒ‡ãƒ«ã§å› å­å¾—ç‚¹ã‚’è¨ˆç®—
                    factor_scores = expert_fa.transform(scaled_metrics)

                    # å› å­å¾—ç‚¹ã‚’é‡ã¿ä»˜ã‘ã—ã¦å˜ä¸€ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«åˆç®—
                    skill_score = np.dot(factor_scores[0], self.factor_weights)

                    # CLAUDE_ADDED: å› å­ã‚¹ã‚³ã‚¢ã‚’å€‹åˆ¥ã«ä¿å­˜
                    data_dict = {
                        'subject_id': subject_id,
                        'trial_order': i + 1,
                        'block': trial_row['block'],
                        'trial_num_in_block': trial_row['trial_num'],
                        'skill_score': skill_score
                    }

                    # å„å› å­ã‚¹ã‚³ã‚¢ã‚’å€‹åˆ¥ã®ã‚«ãƒ©ãƒ ã¨ã—ã¦è¿½åŠ 
                    for f_idx in range(n_factors):
                        data_dict[f'factor_{f_idx+1}_score'] = factor_scores[0][f_idx]

                    plot_data_list.append(data_dict)

                except Exception as e:
                    print(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€… {subject_id}, trial_index {i}: {e}")

        # ãƒªã‚¹ãƒˆã‹ã‚‰æœ€çµ‚çš„ãªDataFrameã‚’ä½œæˆ
        skill_score_df = pd.DataFrame(plot_data_list)

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if skill_score_df is not None:
            self._save_skill_score_plots(skill_score_df)

        print(f"âœ… è¨ˆç®—å®Œäº†: skill_score + {n_factors}å€‹ã®å› å­ã‚¹ã‚³ã‚¢")
        return skill_score_df

    def calculate_stable_skill_score(self, skill_metrics_df: pd.DataFrame,expert_scaler: StandardScaler, expert_fa, window_size=10):
        """å®‰å®šã—ãŸã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        stable_scores = []

        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                         'jerk_score', 'control_stability', 'temporal_consistency',
                         'trial_time', 'endpoint_error']

        for subject_id, subject_df in skill_metrics_df.groupby('subject_id'):
            sorted_trials = subject_df.sort_values(by=['block', 'trial_num']).reset_index()

            for i, trial_row in sorted_trials.iterrows():
                try:
                    trial_metrics_df = trial_row[skill_columns].to_frame().T

                    # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦
                    for col in skill_columns:
                        trial_metrics_df[col] = pd.to_numeric(trial_metrics_df[col], errors='coerce')

                    # æ¬ æå€¤ç¢ºèª
                    if trial_metrics_df.isna().any().any():
                        continue

                    # CLAUDE_ADDED: å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã§æ¨™æº–åŒ–ï¼ˆDataFrameå½¢å¼ã§æ¸¡ã—ã¦ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    scaled_metrics = expert_scaler.transform(trial_metrics_df)

                    # å­¦ç¿’æ¸ˆã¿FAãƒ¢ãƒ‡ãƒ«ã§å› å­å¾—ç‚¹ã‚’è¨ˆç®—
                    factor_scores = expert_fa.transform(scaled_metrics)

                    # å› å­å¾—ç‚¹ã‚’é‡ã¿ä»˜ã‘ã—ã¦å˜ä¸€ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«åˆç®—
                    skill_score = np.dot(factor_scores[0], self.factor_weights)

                    stable_scores.append({
                        'subject_id': subject_id,
                        'trial_order': i + 1,
                        'block': trial_row['block'],
                        'trial_num_in_block': trial_row['trial_num'],
                        'skill_score': skill_score
                    })

                except Exception as e:
                    print(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€… {subject_id}, trial_index {i}: {e}")

        skill_score_df = pd.DataFrame(stable_scores)

        skill_score_df['smoothed_skill_score'] = skill_score_df.groupby('subject_id')['skill_score'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if skill_score_df is not None:
            self._save_skill_score_plots(skill_score_df)

        return pd.DataFrame(stable_scores)



    def _save_skill_score_plots(self, skill_scores: pd.DataFrame):
        """è¢«é¨“è€…ã”ã¨ã«ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹"""
        self.output.skill_score_calculator_output_dir_path.mkdir(parents=True, exist_ok=True)

        save_path = self.output.skill_score_calculator_output_dir_path / 'skill_score_transition'

        # CLAUDE_ADDED: è¢«é¨“è€…åã®åŒ¿ååŒ–
        anonymized_scores = skill_scores.copy()
        subject_mapping = anonymize_subjects(skill_scores['subject_id'])
        anonymized_scores['subject_id'] = anonymized_scores['subject_id'].map(subject_mapping)

        # 12cm = 4.72 inches width for skill score plot
        fig = plt.figure(figsize=(4.72, 2.8))

        # CLAUDE_ADDED: smoothed_skill_scoreãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¦é©åˆ‡ãªã‚«ãƒ©ãƒ ã‚’é¸æŠ
        y_column = 'smoothed_skill_score' if 'smoothed_skill_score' in skill_scores.columns else 'skill_score'
        plot_title = 'Smoothed Skill Score Improvement' if y_column == 'smoothed_skill_score' else 'Skill Score Improvement'

        sns.lineplot(
            data=anonymized_scores,
            x='trial_order',
            y=y_column,
            hue='subject_id',
        )

        plt.xlabel('Trial Order [-]', fontsize=9)
        plt.ylabel('Calculated Skill Score [-]', fontsize=9)
        plt.grid(True, alpha=0.3)
        plt.legend(title='Subject ID', fontsize=5, title_fontsize=6, loc='best', ncol=2)
        plt.tick_params(axis='both', labelsize=8)

        plt.tight_layout()

        # CLAUDE_ADDED: å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close()

        # CLAUDE_ADDED: æ¨™æº–åŒ–ã—ãŸã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ãƒ—ãƒ­ãƒƒãƒˆ
        self._save_standardized_skill_score_plot(skill_scores, subject_mapping, y_column)

    def _save_standardized_skill_score_plot(self, skill_scores: pd.DataFrame, subject_mapping: Dict, original_y_column: str):
        """CLAUDE_ADDED: StandardScalerã§æ¨™æº–åŒ–ã—ãŸã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹"""
        # æ¨™æº–åŒ–å‡¦ç†: StandardScaler (DatasetBuilderã¨åŒã˜å‡¦ç†)
        standardized_scores = skill_scores.copy()

        # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
        score_column = original_y_column

        # StandardScalerã§æ¨™æº–åŒ– (å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦)
        scaler = StandardScaler()
        skill_values = standardized_scores[score_column].values.reshape(-1, 1)
        standardized_values = scaler.fit_transform(skill_values)

        standardized_column_name = f'standardized_{score_column}'
        standardized_scores[standardized_column_name] = standardized_values.flatten()

        # è¢«é¨“è€…åã‚’åŒ¿ååŒ–
        standardized_scores['subject_id'] = standardized_scores['subject_id'].map(subject_mapping)

        # ãƒ—ãƒ­ãƒƒãƒˆã®ä¿å­˜ãƒ‘ã‚¹
        save_path = self.output.skill_score_calculator_output_dir_path / 'standardized_skill_score_transition'

        # 12cm = 4.72 inches width for skill score plot
        fig = plt.figure(figsize=(4.72, 2.8))

        sns.lineplot(
            data=standardized_scores,
            x='trial_order',
            y=standardized_column_name,
            hue='subject_id',
        )

        plt.xlabel('Trial Order [-]', fontsize=9)
        plt.ylabel('Standardized Skill Score (Z-score) [-]', fontsize=9)
        plt.grid(True, alpha=0.3)
        plt.legend(title='Subject ID', fontsize=5, title_fontsize=6, loc='best', ncol=2)
        plt.tick_params(axis='both', labelsize=8)

        plt.tight_layout()

        # å­¦è¡“è«–æ–‡ç”¨ã®ä¿å­˜
        save_academic_figure(fig, save_path)
        plt.close()


class DatasetBuilder:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹ - å‰å‡¦ç†ã¨ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’æ‹…å½“"""
    
    def __init__(self, config: Dict, output_manager: OutputManager):
        self.config = config
        self.output_manager = output_manager
        self.target_seq_len = config['pre_process']['target_seq_len']
        
    def build_skill_trajectory_dataset(self,
                                     skill_metrics_df: pd.DataFrame,
                                     preprocessed_trajectory_df: pd.DataFrame,
                                     trained_scaler: StandardScaler,
                                     trained_fa) -> str:
        """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ä»˜ãè»Œé“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã€ä¿å­˜ã™ã‚‹"""

        print("ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ä»˜ãè»Œé“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­...")

        # CLAUDE_ADDED: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        dataset_output_dir = self.output_manager.dataset_builder_output_dir_path
        dataset_output_dir.mkdir(parents=True, exist_ok=True)

        # 1. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã¨å› å­ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        try:
            eigenvalues, _ = trained_fa.get_eigenvalues()
            n_factors = trained_fa.n_factors
            factor_eigenvalues = eigenvalues[:n_factors]
            total_variance = np.sum(eigenvalues)
            factor_weights = factor_eigenvalues / total_variance
            print(f"ğŸ’¡ DatasetBuilder: å› å­å¯„ä¸ç‡ã‚’è‡ªå‹•å–å¾—ã—ã¾ã—ãŸ: {factor_weights}")
        except Exception as e:
            print(f"âš ï¸ DatasetBuilder: å› å­å¯„ä¸ç‡ã®å–å¾—ã«å¤±æ•—ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            factor_weights = None

        skill_score_calculator = SkillScoreCalculator(self.config, self.output_manager, factor_weights)
        use_factor_scores = self.config.get('analysis', {}).get('use_factor_scores', True)

        if use_factor_scores:
            print("ğŸ’¡ å› å­ã‚¹ã‚³ã‚¢ã‚’å«ã‚ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™")
            skill_score_df = skill_score_calculator.calc_skill_score_with_factors(
                skill_metrics_df, trained_scaler, trained_fa
            )
        else:
            print("ğŸ’¡ skill_scoreã®ã¿ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™")
            skill_score_df = skill_score_calculator.calc_skill_score(
                skill_metrics_df, trained_scaler, trained_fa
            )

        # 2. è»Œé“ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’çµåˆ
        merged_df = self._merge_trajectory_and_skill_data(
            preprocessed_trajectory_df, skill_score_df
        )

        # 3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        train_df, test_df = self._split_train_test(merged_df)

        # 4. ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaled_train_df, scaled_test_df, scalers, feature_config = self._scale_features(
            train_df, test_df
        )

        # 5. ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self._save_dataset_files(
            scaled_train_df, scaled_test_df, scalers, feature_config, dataset_output_dir
        )

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {dataset_output_dir}")
        return str(dataset_output_dir)
    
    def _merge_trajectory_and_skill_data(self, trajectory_df: pd.DataFrame,
                                       skill_score_df: pd.DataFrame) -> pd.DataFrame:
        """è»Œé“ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""

        # CLAUDE_ADDED: ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        print(f"CLAUDE_DEBUG: Trajectory data shape: {trajectory_df.shape}")
        print(f"CLAUDE_DEBUG: Skill score data shape: {skill_score_df.shape}")
        print(f"CLAUDE_DEBUG: Skill score data columns: {skill_score_df.columns.tolist()}")

        # CLAUDE_ADDED: å„è©¦è¡Œã®ãƒ‡ãƒ¼ã‚¿é•·ã‚’ç¢ºèª
        if 'time_step' in trajectory_df.columns:
            trial_lengths = trajectory_df.groupby(['subject_id', 'trial_num', 'block']).size()
            print(f"CLAUDE_DEBUG: Sample trajectory lengths: {trial_lengths.head(5)}")
            print(f"CLAUDE_DEBUG: Min length: {trial_lengths.min()}, Max length: {trial_lengths.max()}")

        # CLAUDE_ADDED: ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºï¼ˆskill_score + factor_i_scoreï¼‰
        # å› å­ã‚¹ã‚³ã‚¢ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å› å­ã‚¹ã‚³ã‚¢ã®ã¿ã€å­˜åœ¨ã—ãªã„å ´åˆã¯skill_scoreã‚’ä½¿ç”¨
        factor_score_cols = [col for col in skill_score_df.columns if col.startswith('factor_') and col.endswith('_score')]

        if len(factor_score_cols) > 0:
            # å› å­ã‚¹ã‚³ã‚¢ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å› å­ã‚¹ã‚³ã‚¢ã®ã¿ã‚’ä½¿ç”¨ï¼ˆskill_scoreã¯é™¤å¤–ï¼‰
            score_columns = factor_score_cols
            print(f"ğŸ’¡ å› å­ã‚¹ã‚³ã‚¢ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™: {score_columns}")
        else:
            # å› å­ã‚¹ã‚³ã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯skill_scoreã‚’ä½¿ç”¨
            score_columns = ['skill_score']
            print(f"ğŸ’¡ skill_scoreã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")

        print(f"CLAUDE_DEBUG: Detected score columns to merge: {score_columns}")

        # CLAUDE_ADDED: ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ©ã‚¤ã‚¢ãƒ«å˜ä½ã§çµåˆ
        merged_data = []
        successful_merges = 0
        failed_merges = 0

        for _, skill_row in skill_score_df.iterrows():
            subject_id = skill_row['subject_id']
            block = skill_row['block']
            trial_num = skill_row['trial_num_in_block']

            # è©²å½“ã™ã‚‹è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            trajectory_subset = trajectory_df[
                (trajectory_df['subject_id'] == subject_id) &
                (trajectory_df['block'] == block) &
                (trajectory_df['trial_num'] == trial_num)
            ].copy()

            if not trajectory_subset.empty:
                # CLAUDE_ADDED: çµåˆå‰ã«è»Œé“ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
                print(f"CLAUDE_DEBUG: Merging {subject_id} trial {trial_num} block {block}: length = {len(trajectory_subset)}")

                # CLAUDE_ADDED: å…¨ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ ã‚’å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«è¿½åŠ 
                for score_col in score_columns:
                    if score_col in skill_row:
                        trajectory_subset[score_col] = skill_row[score_col]

                merged_data.append(trajectory_subset)
                successful_merges += 1
            else:
                failed_merges += 1

        print(f"CLAUDE_DEBUG: Successful merges: {successful_merges}, Failed merges: {failed_merges}")

        if merged_data:
            merged_df = pd.concat(merged_data, ignore_index=True)
            print(f"CLAUDE_DEBUG: Final merged data shape: {merged_df.shape}")
            print(f"CLAUDE_DEBUG: Final merged data columns: {merged_df.columns.tolist()}")
            return merged_df
        else:
            return pd.DataFrame()
    
    def _split_train_test(self, merged_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """è¢«é¨“è€…ãƒ¬ãƒ™ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²"""
        
        subjects = merged_df['subject_id'].unique()
        test_size = self.config.get('data', {}).get('test_split', 0.2)
        random_seed = self.config.get('data', {}).get('random_seed', 42)
        
        from sklearn.model_selection import train_test_split
        train_subjects, test_subjects = train_test_split(
            subjects, test_size=test_size, random_state=random_seed
        )
        
        train_df = merged_df[merged_df['subject_id'].isin(train_subjects)]
        test_df = merged_df[merged_df['subject_id'].isin(test_subjects)]
        
        print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨è¢«é¨“è€…={len(train_subjects)}äºº, ãƒ†ã‚¹ãƒˆç”¨è¢«é¨“è€…={len(test_subjects)}äºº")
        
        return train_df, test_df
    
    def _scale_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[
        pd.DataFrame, pd.DataFrame, Dict, Dict]:
        """ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""

        # CLAUDE_ADDED: ã‚³ãƒ³ãƒ•ã‚£ã‚°ã‹ã‚‰æ­£è¦åŒ–æ–¹æ³•ã‚’å–å¾— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'standard')
        normalization_method = self.config.get('pre_process', {}).get('normalization_method', 'standard')
        print(f"CLAUDE_DEBUG: Using normalization method: {normalization_method}")

        # CLAUDE_ADDED: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡ã®ç‰¹å¾´é‡ã‚’å®šç¾©
        trajectory_features = ['HandlePosX', 'HandlePosY', 'HandleVelX',
                             'HandleVelY', 'HandleAccX', 'HandleAccY']

        scalers = {}
        scaled_train_df = train_df.copy()
        scaled_test_df = test_df.copy()

        # CLAUDE_ADDED: æ­£è¦åŒ–æ–¹æ³•ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆã™ã‚‹é–¢æ•°
        def create_scaler(method: str):
            """æ­£è¦åŒ–æ–¹æ³•ã«å¿œã˜ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’è¿”ã™"""
            if method == 'minmax':
                return MinMaxScaler(feature_range=(-1, 1))
            else:  # default: 'standard'
                return StandardScaler()

        # ç‰¹å¾´é‡ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        for feature in trajectory_features:
            if feature in train_df.columns:
                scaler = create_scaler(normalization_method)

                # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒƒãƒˆ
                train_values = train_df[feature].values.reshape(-1, 1)
                scaled_train_values = scaler.fit_transform(train_values)
                scaled_train_df[feature] = scaled_train_values.flatten()

                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ç”¨
                test_values = test_df[feature].values.reshape(-1, 1)
                scaled_test_values = scaler.transform(test_values)
                scaled_test_df[feature] = scaled_test_values.flatten()

                scalers[feature] = scaler

        # CLAUDE_ADDED: ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ ï¼ˆskill_score + factor_i_scoreï¼‰ã‚’æ¤œå‡ºã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        score_columns = []

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚‚æ­£è¦åŒ–
        if 'skill_score' in train_df.columns:
            skill_scaler = create_scaler(normalization_method)
            train_skill = train_df['skill_score'].values.reshape(-1, 1)
            scaled_train_df['skill_score'] = skill_scaler.fit_transform(train_skill).flatten()

            test_skill = test_df['skill_score'].values.reshape(-1, 1)
            scaled_test_df['skill_score'] = skill_scaler.transform(test_skill).flatten()

            scalers['skill_score'] = skill_scaler
            score_columns.append('skill_score')

        # CLAUDE_ADDED: å› å­ã‚¹ã‚³ã‚¢ã‚‚æ­£è¦åŒ–
        factor_score_cols = [col for col in train_df.columns if col.startswith('factor_') and col.endswith('_score')]
        for factor_col in factor_score_cols:
            factor_scaler = create_scaler(normalization_method)
            train_factor = train_df[factor_col].values.reshape(-1, 1)
            scaled_train_df[factor_col] = factor_scaler.fit_transform(train_factor).flatten()

            test_factor = test_df[factor_col].values.reshape(-1, 1)
            scaled_test_df[factor_col] = factor_scaler.transform(test_factor).flatten()

            scalers[factor_col] = factor_scaler
            score_columns.append(factor_col)

        print(f"CLAUDE_DEBUG: Scaled score columns: {score_columns}")

        # CLAUDE_ADDED: ç‰¹å¾´é‡è¨­å®šï¼ˆå› å­ã‚¹ã‚³ã‚¢ã‚‚å«ã‚ã‚‹ï¼‰
        feature_config = {
            'feature_cols': trajectory_features + score_columns,
            'trajectory_features': trajectory_features,
            'score_columns': score_columns,  # CLAUDE_ADDED: ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆã‚‚ä¿å­˜
            'target_seq_len': self.target_seq_len
        }

        return scaled_train_df, scaled_test_df, scalers, feature_config
    
    def _save_dataset_files(self, train_df: pd.DataFrame, test_df: pd.DataFrame, 
                          scalers: Dict, feature_config: Dict, output_dir: Path):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        
        # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜
        train_df.to_parquet(output_dir / 'train_data.parquet', index=False)
        test_df.to_parquet(output_dir / 'test_data.parquet', index=False)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã¨è¨­å®šã‚’ä¿å­˜
        joblib.dump(scalers, output_dir / 'scalers.joblib')
        joblib.dump(feature_config, output_dir / 'feature_config.joblib')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ä¿å­˜
        dataset_info = {
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'train_subjects': train_df['subject_id'].nunique(),
            'test_subjects': test_df['subject_id'].nunique(),
            'feature_columns': feature_config['feature_cols'],
            'target_seq_len': self.target_seq_len,
            'created_at': datetime.datetime.now().isoformat()
        }
        
        import json
        with open(output_dir / 'dataset_info.json', 'w') as f:
            json.dump(dataset_info, f, indent=2)
            
        print(f"ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - train_data.parquet: {len(train_df):,} ã‚µãƒ³ãƒ—ãƒ«") 
        print(f"  - test_data.parquet: {len(test_df):,} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"  - scalers.joblib: {len(scalers)} ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼")
        print(f"  - feature_config.joblib: è¨­å®šæƒ…å ±")
        print(f"  - dataset_info.json: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")



if __name__ == '__main__':
    config_dir = 'PredictiveLatentSpaceNavigationModel/DataPreprocess/data_preprocess_default_config.yaml'

    try:
        # ã‚³ãƒ³ãƒ•ã‚£ã‚°ã®èª­ã¿è¾¼ã¿
        config_loader = DataPreprocessConfigLoader(config_dir)
        validated_config = config_loader.get_config()

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        output_manager = OutputManager(validated_config)

        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿ã“ã¿
        trajectory_loader = TrajectoryDataLoader(validated_config)

        print("============ ç†Ÿé”ã—ãŸãƒ‡ãƒ¼ã‚¿ã§å› å­åˆ†æã‚’å®Ÿè¡Œ (block_num = 4) ============")

        block_num = 4

        # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®è¨ˆç®—
        skill_metrics_calculator = SkillMetricCalculator(validated_config)
        preprocessed_data = trajectory_loader.get_preprocessed_data(block_num)
        skill_metrics_df = skill_metrics_calculator.calculate_skill_metrics(preprocessed_data)

        print(skill_metrics_df['block'].unique())
        print(skill_metrics_df['trial_num'].unique())

        # ANOVA and å› å­åˆ†æ
        skill_analyzer = SkillAnalyzer(validated_config, output_manager)
        skill_analyzer.analyze_skill_metrics(skill_metrics_df, 'promax')

        # å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        trained_scaler, trained_fa = skill_analyzer.factorize_artifact

        # CLAUDE_ADDED: å› å­å¯„ä¸ç‡ã‚’è‡ªå‹•å–å¾—
        factor_weights = skill_analyzer.factor_contribution_ratios
        if factor_weights is None:
            print("âš ï¸ å› å­å¯„ä¸ç‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        print("============ å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ–ãƒ­ãƒƒã‚¯æ¯å› å­åˆ†æã‚’å®Ÿè¡Œ ============")
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æã‚’å®Ÿè¡Œ
        all_preprocess_data_for_blockwise = trajectory_loader.get_preprocessed_data(0)
        all_skill_metrics_df_for_blockwise = skill_metrics_calculator.calculate_skill_metrics(all_preprocess_data_for_blockwise)

        # ãƒ–ãƒ­ãƒƒã‚¯æ¯ã®å› å­åˆ†æã‚’å®Ÿè¡Œ
        skill_analyzer.analyze_block_wise_factor_analysis(all_skill_metrics_df_for_blockwise, 'promax')

        print("============ å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (block_num = 0) ============")
        block_num = 0

        # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
        all_preprocess_data = trajectory_loader.get_preprocessed_data(block_num)
        all_skill_metrics_df = skill_metrics_calculator.calculate_skill_metrics(all_preprocess_data)

        print(all_skill_metrics_df['block'].unique())
        print(all_skill_metrics_df['trial_num'].unique())

        if validated_config.get('data').get('make_dataset'):
            print("============ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¦ä¿å­˜ ============")

            dataset_builder = DatasetBuilder(validated_config, output_manager)
            dataset_path = dataset_builder.build_skill_trajectory_dataset(
                all_skill_metrics_df,
                all_preprocess_data,
                trained_scaler,
                trained_fa
            )
            print(f"ğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {dataset_path}")

        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢æ¨ç§»ã‚‚åˆ¥é€”è¨ˆç®—ãƒ»ä¿å­˜
        # CLAUDE_ADDED: å› å­å¯„ä¸ç‡ã‚’æ¸¡ã—ã¦ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—
        skill_score_calculator = SkillScoreCalculator(validated_config, output_manager, factor_weights)
        skill_score_calculator.calculate_stable_skill_score(all_skill_metrics_df, trained_scaler, trained_fa, 5)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

