import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import  Dict, List, Tuple, Optional
from pathlib import  Path
from scipy.interpolate import interp1d, UnivariateSpline
from scipy import stats
from factor_analyzer import FactorAnalyzer
# from sklearn.decomposition import FactorAnalysis

from sklearn.preprocessing import StandardScaler

import yaml
import datetime
import joblib


class DataPreprocessConfigLoader:
    def __init__(self, config_dir: str):
        # ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        self.config_dir = Path(config_dir)
        if not self.config_dir.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with self.config_dir.open() as f:
            config = yaml.safe_load(f)

        is_valid, error_list = self.validate_config(config)
        if not is_valid:
            error_details = "\n - ".join(error_list)
            raise ValueError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™: \n - {error_details}")
        else:
            self.config =config

    def validate_config(self, config: Dict) -> Tuple[bool, List[str]]:
        """è¨­å®šã‚’æ¤œè¨¼ã—ã€æˆåŠŸ/å¤±æ•—ã¨ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™"""

        all_errors = []
        all_errors.extend(self._validate_paths(config))
        all_errors.extend(self._validate_preprocessing(config))
        all_errors.extend(self._validate_analysis_flags(config))

        return len(all_errors) == 0, all_errors

    def _validate_paths(self, config: Dict) -> List[str]:
        """ãƒ‘ã‚¹ç³»è¨­å®šã®æ¤œè¨¼"""
        errors = []

        data_section = config.get('data')
        if data_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'data' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        raw_data_dir_section = data_section.get('raw_data_dir')
        if  raw_data_dir_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'raw_data_dir' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return errors
        else:
            raw_data_dir = Path(raw_data_dir_section)
            if not raw_data_dir.exists():
                errors.append(f"raw_data_dirã«æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{raw_data_dir}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        return errors

    def _validate_preprocessing(self, config:Dict) -> List[str]:
        """å‰å‡¦ç†è¨­å®šã®æ¤œè¨¼"""
        errors = []

        pre_process_section = config.get('pre_process')
        if pre_process_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'pre_process' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        target_seq_len_section = pre_process_section.get('target_seq_len')
        if target_seq_len_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'target_seq_len' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        interpolate_method_section = pre_process_section.get('interpolate_method')
        if interpolate_method_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'interpolate_method' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors
        else:
            if interpolate_method_section not in ['linear', 'spline']:
                errors.append("interpolate_methodã®å€¤ãŒä¸é©åˆ‡ã§ã™ã€‚['linear', 'spline'] ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")

        return errors

    def _validate_analysis_flags(self, config:Dict) -> List[str]:
        """åˆ†æãƒ•ãƒ©ã‚°ã®æ¤œè¨¼"""
        errors = []

        analysis_section = config.get('analysis')
        if analysis_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'analysis' ãŒã‚ã‚Šã¾ã›ã‚“")
            return errors

        anova_skill_metrics_section = analysis_section.get('anova_skill_metrics')
        if anova_skill_metrics_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'anova_skill_metrics' ãŒã‚ã‚Šã¾ã›ã‚“")

        factorize_skill_metrics_section = analysis_section.get('factorize_skill_metrics')
        if factorize_skill_metrics_section is None:
            errors.append("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ 'factorize_skill_metrics' ãŒã‚ã‚Šã¾ã›ã‚“")

        return errors

    def get_config(self) -> Optional[Dict] :
        """æ¤œè¨¼æ¸ˆã¿è¨­å®šã‚’å–å¾—"""
        if self.config is not None:
            return self.config.copy()
        else:
            return None

    def get_data_paths(self) -> Tuple[str, str]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’å–å¾—"""
        return (self.config['data']['raw_data_dir'],
                self.config['data']['output_dir'])


class OutputManager:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã‚’ç®¡ç†ã™ã‚‹"""
    def __init__(self, validated_config: Dict):
        self.config = validated_config
        self.base_output_dir = self.config['data']['output_dir']
        self.process_name = self.config['information']['name']

        self._skill_analyzer_output_dir, self._dataset_builder_output_path, self._skill_score_calculator_output_path = self._make_unique_output_paths()


    def _make_unique_output_paths(self) -> tuple[Path, Path, Path]:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä½œæˆ"""
        base_path = Path(self.base_output_dir)
        validated_process_name = self.process_name.replace(' ', '_')
        time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

        dir_name = f"{validated_process_name}_{time_stamp}"

        skill_analyzer_result_dir = base_path / dir_name / 'skill_analyze_result'
        data_set_builder_output_dir = base_path / dir_name / 'dataset'
        skill_score_calculator_output_dir =base_path/ dir_name /'skill_score_result'

        return skill_analyzer_result_dir, data_set_builder_output_dir, skill_score_calculator_output_dir

    @property
    def skill_analyzer_output_dir_path(self):
        return self._skill_analyzer_output_dir

    @property
    def dataset_builder_output_dir_path(self):
        return self._dataset_builder_output_path

    @property
    def skill_score_calculator_output_dir_path(self):
        return self._skill_score_calculator_output_path


class TrajectoryDataLoader:
    """è»Œé“ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ã‚’æ‹…å½“"""
    def __init__(self, config:Dict):
        """æ¤œè¨¼æ¸ˆã¿è¨­å®šã‚’å—ã‘å–ã‚Š"""
        self.config = config
        self.raw_data_dir = Path(config['data']['raw_data_dir'])
        self.raw_data_df = self._load_raw_data()
        self.preprocessed_data_df = self._preprocess_trajectories(self.raw_data_df)

    def _load_raw_data(self) -> Optional[pd.DataFrame]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®CSVã‚’èª­ã¿è¾¼ã¿ã€ä¸€ã¤ã®DataFrameã«çµåˆã™ã‚‹"""
        all_files = list(self.raw_data_dir.glob('*.csv'))

        if not all_files:
            print(f"ã‚¨ãƒ©ãƒ¼:ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {str(self.raw_data_dir)} ã«csvãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼")
            return None

        df_list = []
        for file_path in all_files:
            try:
                parts = file_path.stem.split('_')
                subject_id = parts[0]
                block_num = int(parts[1].replace('Block', ''))

                df = pd.read_csv(file_path)

                # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ã—ã€æƒ…å ±ã‚’ä»˜ä¸
                df = df.rename(columns={'SubjectId': 'subject_id', 'CurrentTrial': 'trial_num'})
                df['subject_id'] = subject_id
                df['block'] = block_num
                df_list.append(df)
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        if not df_list:
            return None

        concatenated_df = pd.concat(df_list, ignore_index=True)

        return concatenated_df[concatenated_df['TrialState'] == 'TRIAL_RUNNING'].copy()

    def _preprocess_trajectories(self, data: pd.DataFrame) -> pd.DataFrame:
        """è»Œé“ã®å‰å‡¦ç†(è£œå®Œã€é•·ã•èª¿æ•´)"""
        target_seq_len = self.config['pre_process']['target_seq_len']
        method = self.config['pre_process']['interpolate_method']

        processed_trajectories = [] #å…¨ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’è“„ç©ã™ã‚‹ãƒªã‚¹ãƒˆ

        for subject_id, subject_df in data.groupby('subject_id'):
            for (trial_num, block_num), trial_df in subject_df.groupby(['trial_num', 'block']): # CLAUDE_ADDED: blockã‚‚ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã«å«ã‚ã‚‹
                try:
                    traj_positions = trial_df[['HandlePosX','HandlePosY']].values
                    traj_velocities = trial_df[['HandleVelX','HandleVelY']].values
                    traj_acceleration = trial_df[['HandleAccX','HandleAccY']].values

                    # å¤‰æ›å…ˆã®æ™‚é–“è»¸ã®ä½œæˆ
                    original_length = len(traj_positions)
                    original_time = np.linspace(0, 1, original_length)
                    target_time = np.linspace(0,1, target_seq_len)

                    # æœ€å°ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
                    if original_length < 2:
                        print(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num} ({original_length}ç‚¹)")
                        continue

                    interp_pos_x = None
                    interp_pos_y = None
                    interp_vel_x = None
                    interp_vel_y = None
                    interp_acc_x = None
                    interp_acc_y = None

                    if method == 'linear':
                        # ç·šå½¢è£œé–“é–¢æ•°ã®ä½œæˆ
                        interp_pos_x = interp1d(original_time, traj_positions[:, 0], kind ='linear')
                        interp_pos_y = interp1d(original_time, traj_positions[:, 1], kind ='linear')
                        interp_vel_x = interp1d(original_time, traj_velocities[:, 0], kind='linear')
                        interp_vel_y = interp1d(original_time, traj_velocities[:, 1], kind='linear')
                        interp_acc_x = interp1d(original_time, traj_acceleration[:, 0], kind='linear')
                        interp_acc_y = interp1d(original_time, traj_acceleration[:, 1], kind='linear')
                    elif method == 'spline':
                        # ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“é–¢æ•°ã®ä½œæˆ
                        interp_pos_x = UnivariateSpline(original_time, traj_positions[:, 0], s=0)
                        interp_pos_y = UnivariateSpline(original_time, traj_positions[:, 1], s=0)
                        interp_vel_x = UnivariateSpline(original_time, traj_velocities[:, 0], s=0)
                        interp_vel_y = UnivariateSpline(original_time, traj_velocities[:, 1], s=0)
                        interp_acc_x = UnivariateSpline(original_time, traj_acceleration[:, 0], s=0)
                        interp_acc_y = UnivariateSpline(original_time, traj_acceleration[:, 1], s=0)
                    else:
                        print(f"æœªå¯¾å¿œã®è£œå®Œæ–¹æ³•: {method}")
                        continue

                    # æ–°ã—ã„æ™‚é–“è»¸ã§è£œå®Œ
                    resampled_pos_x = interp_pos_x(target_time)
                    resampled_pos_y = interp_pos_y(target_time)
                    resampled_vel_x = interp_vel_x(target_time)
                    resampled_vel_y = interp_vel_y(target_time)
                    resampled_acc_x = interp_acc_x(target_time)
                    resampled_acc_y = interp_acc_y(target_time)

                    # å…ƒã®Timestampæƒ…å ±ã‚’å–å¾—ã—ã¦æ–°ã—ã„æ™‚é–“è»¸ã‚’ä½œæˆ
                    original_start_time = trial_df['Timestamp'].iloc[0]
                    original_end_time = trial_df['Timestamp'].iloc[-1]
                    resampled_timestamps = np.linspace(original_start_time, original_end_time, target_seq_len)
                    
                    # ãƒªã‚µãƒ³ãƒ—ãƒ«å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
                    trajectory_df = pd.DataFrame({
                        'subject_id': subject_id,
                        'trial_num': trial_num,
                        'block': block_num, # CLAUDE_ADDED: ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‹ã‚‰å–å¾—ã—ãŸæ­£ã—ã„blockç•ªå·ã‚’ä½¿ç”¨
                        'time_step':range(target_seq_len),
                        'Timestamp': resampled_timestamps,
                        'HandlePosX': resampled_pos_x,
                        'HandlePosY': resampled_pos_y,
                        'HandleVelX': resampled_vel_x,
                        'HandleVelY': resampled_vel_y,
                        'HandleAccX': resampled_acc_x,
                        'HandleAccY': resampled_acc_y,
                        'TargetEndPosX': trial_df['TargetEndPosX'].iloc[0],
                        'TargetEndPosY': trial_df['TargetEndPosY'].iloc[0],
                        'original_length': original_length
                    })

                    processed_trajectories.append(trajectory_df)

                except ValueError as e:
                    print(f"è£œå®Œã‚¨ãƒ©ãƒ¼: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num}: {e}")
                    continue

        # å…¨ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’çµåˆ
        if processed_trajectories:
            return pd.concat(processed_trajectories, ignore_index=True)
        else:
            return pd.DataFrame()   #ç©ºã®DataFrameã‚’è¿”ã™

    def get_subjects(self) -> List[str]:
        """è¢«é¨“è€…IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.df['subject_id'].unique().to_list()

    def get_raw_data(self) -> Optional[pd.DataFrame]:
        if self.raw_data_df is not None:
            return self.raw_data_df
        else:
            return None

    def get_preprocessed_data(self, block: int) -> Optional[pd.DataFrame]:
        """æŒ‡å®šã—ãŸãƒ–ãƒ­ãƒƒã‚¯ã®å‰å‡¦ç†æ¸ˆã¿DataFrameã‚’è¿”ã™"""
        if self.preprocessed_data_df is not None:
            if block== 0:
                filtered_df = self.preprocessed_data_df
            else:
                filtered_df = self.preprocessed_data_df[self.preprocessed_data_df['block'] == block]

            return filtered_df
        else:
            return None


class SkillMetricCalculator:
    """å„ç¨®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®è¨ˆç®—ã‚’æ‹…å½“"""
    def __init__(self, config: Dict):
        """è¨­å®šã‚’å—ã‘å–ã‚ŠåˆæœŸåŒ–"""
        self.config = config
        self.target_seq_len = config['pre_process']['target_seq_len']

    
    def calculate_skill_metrics(self, preprocessed_data: pd.DataFrame) -> pd.DataFrame:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ç¨®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦æ¨™æº–åŒ–"""
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if preprocessed_data is None or len(preprocessed_data) == 0:
            print("âš ï¸ ã‚¹ã‚­ãƒ«æŒ‡æ¨™è¨ˆç®—: å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return pd.DataFrame()  # ç©ºã®DataFrameã‚’è¿”ã™
        
        skill_metrics = []
        
        for (subject_id, trial_num, block), trial_group in preprocessed_data.groupby(['subject_id', 'trial_num', 'block']):
            try:
                # å„ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‹ã‚‰è»Œé“æ›²ç‡ã€é€Ÿåº¦æ»‘ã‚‰ã‹ã•ã€åŠ é€Ÿåº¦æ»‘ã‚‰ã‹ã•ã€ã‚¸ãƒ£ãƒ¼ã‚¯ã€åˆ¶å¾¡å®‰å®šæ€§ã€æ™‚é–“çš„ä¸€è²«æ€§ã€å‹•ä½œæ™‚é–“ã€çµ‚ç‚¹èª¤å·®ã‚’è¨ˆç®—
                curvature = self._calculate_curvature(trial_group)
                velocity_smoothness = self._calculate_velocity_smoothness(trial_group)
                acceleration_smoothness = self._calculate_acceleration_smoothness(trial_group)
                jerk_score = self._calculate_jerk(trial_group)
                control_stability = self._calculate_control_stability(trial_group)
                temporal_consistency = self._calculate_temporal_consistency(trial_group)
                trial_time = self._calculate_trial_time(trial_group)
                endpoint_error = self._calculate_endpoint_error(trial_group)

                # çµæœã‚’ã¾ã¨ã‚ã‚‹
                metrics = {
                    'subject_id': subject_id,
                    'trial_num': trial_num,
                    'block': block,
                    'curvature': curvature,
                    'velocity_smoothness': velocity_smoothness,
                    'acceleration_smoothness': acceleration_smoothness,
                    'jerk_score': jerk_score,
                    'control_stability': control_stability,
                    'temporal_consistency': temporal_consistency,
                    'trial_time': trial_time,
                    'endpoint_error': endpoint_error,
                }
                skill_metrics.append(metrics)
                
            except Exception as e:
                print(f"ã‚¹ã‚­ãƒ«æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€…{subject_id}, ãƒˆãƒ©ã‚¤ã‚¢ãƒ«{trial_num}: {e}")
                continue
        
        skill_metrics_df =  pd.DataFrame(skill_metrics)

        # Z-scoreæ¨™æº–åŒ–ã®å®Ÿè¡Œ
        # if self.config.get('analysis', {}).get('standardize_metrics', True):
        #     skill_metrics_df = self._standardize_metrics(skill_metrics_df)

        return skill_metrics_df

    def _standardize_metrics(self, metrics_df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’æ¨™æº–åŒ–ã™ã‚‹"""
        standardized_df = metrics_df.copy()

        # é€šçŸ¥åˆ—ã®ã¿ã‚’æ¨™æº–åŒ–å¯¾è±¡ã¨ã™ã‚‹
        metric_directions = {'curvature': False,
                              'velocity_smoothness': True,
                              'acceleration_smoothness': True,
                              'jerk_score': False,
                              'control_stability': True,
                              'temporal_consistency': True,
                              'trial_time': False,
                              'endpoint_error': False,
                              }

        for col in metric_directions.keys():
            if col in standardized_df.columns:
                values = standardized_df[col].dropna()
                if len(values) > 1:
                    mean_val = values.mean()
                    std_val = values.std()
                    if std_val > 0:
                        z_scores = (standardized_df[col] - mean_val) / std_val
                        # ä½ã„å€¤ãŒè‰¯ã„æŒ‡æ¨™ã¯ç¬¦å·ã‚’åè»¢
                        if not metric_directions[col]:
                            z_scores = -z_scores
                        standardized_df[col] = z_scores

        return standardized_df


    def _calculate_curvature(self, trial_data: pd.DataFrame):
        """è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—"""
        positions = trial_data[['HandlePosX','HandlePosY']].values

        if len(positions) < 3:
            return np.nan

        curvatures = []
        for i in range(1,len(positions) - 1):
            p1, p2, p3 = positions[i - 1], positions[i], positions[i + 1]

            # 3ç‚¹ã‹ã‚‰æ›²ç‡ã‚’è¨ˆç®—
            a = np.linalg.norm(p2 - p1)
            b = np.linalg.norm(p3 - p2)
            c = np.linalg.norm(p3 - p1)

            if a > 0 and b > 0 and c > 0:
                s = (a + b + c) / 2
                area = np.sqrt(max(0, s * (s - a) * (s - b) * (s - c)))
                curvature = 4 * area / (a * b * c) if (a * b * c) > 0 else 0
                curvatures.append(curvature)

        return np.mean(curvatures) if curvatures else 0.0

    def _calculate_velocity_smoothness(self, trial_data: pd.DataFrame):
        """é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•ã‚’è¨ˆç®—"""
        velocities = trial_data[['HandleVelX','HandleVelY']].values

        if len(velocities) < 2:
            return np.nan

        vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
        velocity_changes = np.abs(np.diff(vel_magnitude))

        # æ­£è¦åŒ–ã•ã‚ŒãŸæ»‘ã‚‰ã‹ã•æŒ‡æ¨™
        smoothness = 1.0 / (1.0 + np.std(velocity_changes))
        return smoothness

    def _calculate_acceleration_smoothness(self, trial_data: pd.DataFrame):
        """åŠ é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•"""

        if len(trial_data) < 3:
            return np.nan

        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values
        acc_magnitude = np.sqrt(acc_x ** 2 + acc_y ** 2)

        acc_changes = np.abs(np.diff(acc_magnitude))
        return 1.0 / (1.0 + np.std(acc_changes))

    def _calculate_control_stability(self, trial_data: pd.DataFrame):
        """åˆ¶å¾¡å®‰å®šæ€§æŒ‡æ¨™"""
        if len(trial_data) < 5:
            return np.nan

        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values

        # åŠ é€Ÿåº¦ã®æ¨™æº–åå·®ï¼ˆåˆ¶å¾¡ã®å®‰å®šæ€§ã®é€†æŒ‡æ¨™ï¼‰
        acc_std = np.std(np.sqrt(acc_x ** 2 + acc_y ** 2))
        stability = 1.0 / (1.0 + acc_std)

        return stability

    def _calculate_temporal_consistency(self, trial_data: pd.DataFrame):
        """æ™‚é–“çš„ä¸€è²«æ€§"""
        if len(trial_data) < 10:
            return np.nan

        timestamps = trial_data['Timestamp'].values
        time_intervals = np.diff(timestamps)

        # æ™‚é–“é–“éš”ã®ä¸€è²«æ€§
        consistency = 1.0 / (1.0 + np.std(time_intervals) / np.mean(time_intervals))
        return consistency

    def _calculate_trial_time(self, trial_data: pd.DataFrame) -> float:
        """å‹•ä½œæ™‚é–“ã®è¨ˆç®— - è»Œé“ã®é•·ã•ã‹ã‚‰æ¨å®š"""
        time_stamps = trial_data['Timestamp'].values

        return time_stamps[-1] - time_stamps[0]
    
    def _calculate_endpoint_error(self, trial_data: pd.DataFrame) -> float:
        """çµ‚ç‚¹èª¤å·®ã®è¨ˆç®— - ç›®æ¨™ä½ç½®ã‹ã‚‰ã®è·é›¢"""
        # æœ€å¾Œã®ä½ç½®ã‚’çµ‚ç‚¹ã¨ã—ã¦ä½¿ç”¨
        final_x = trial_data['HandlePosX'].iloc[-1]
        final_y = trial_data['HandlePosY'].iloc[-1]

        target_x = trial_data['TargetEndPosX'].iloc[-1]
        target_y = trial_data['TargetEndPosY'].iloc[-1]
        
        endpoint_error = np.sqrt((final_x - target_x)**2 + (final_y - target_y)**2)
        return endpoint_error
    
    def _calculate_jerk(self, trial_data: pd.DataFrame) -> float:
        """ã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆåŠ é€Ÿåº¦ã®å¤‰åŒ–ç‡ï¼‰ã®è¨ˆç®—"""
        acc_x = trial_data['HandleAccX'].values
        acc_y = trial_data['HandleAccY'].values
        
        # åŠ é€Ÿåº¦ã®å¾®åˆ†ã§ã‚¸ãƒ£ãƒ¼ã‚¯ã‚’è¨ˆç®—
        jerk_x = np.diff(acc_x)
        jerk_y = np.diff(acc_y)
        
        # ã‚¸ãƒ£ãƒ¼ã‚¯ã®å¤§ãã•ã‚’è¨ˆç®—ã—ã€å¹³å‡ã‚’å–ã‚‹
        jerk_magnitude = np.sqrt(jerk_x**2 + jerk_y**2)
        jerk_score = np.mean(jerk_magnitude)
        
        return jerk_score


class SkillAnalyzer:
    """ANOVA,å› å­è§£æãªã©ã®çµ±è¨ˆè§£æã‚’æ‹…å½“"""
    def __init__(self, config: Dict, output_manager: OutputManager):
        """è¨­å®šã‚’å—ã‘å–ã‚ŠåˆæœŸåŒ–"""
        self.config = config
        self.output_manager = output_manager
        self.analysis_config = config.get('analysis', {})

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—ç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.used_standard_scaler = None
        self.used_factor_analysis = None
    
    def analyze_skill_metrics(self, skill_metrics_df: pd.DataFrame, rotation: str = 'promax') -> Dict:
        """ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®çµ±è¨ˆè§£æã‚’å®Ÿè¡Œ"""
        results = {}
        
        if self.analysis_config.get('anova_skill_metrics', False):
            print("ANOVAè§£æã‚’å®Ÿè¡Œä¸­...")
            results['anova'] = self._perform_anova_analysis(skill_metrics_df)

            # ANOVAè§£æçµæœã®å¯è¦–åŒ–
            self._save_anova_plots(skill_metrics_df, results['anova'])

        if self.analysis_config.get('factorize_skill_metrics', False):
            print("å› å­åˆ†æã‚’å®Ÿè¡Œä¸­...")
            results['factor_analysis'] = self._perform_factor_analysis(skill_metrics_df, rotation)

            if 'error' not in results['factor_analysis']:
                self._save_factor_analysis_plots(skill_metrics_df, results['factor_analysis'])
            else:
                print(f"âš ï¸ å› å­åˆ†æãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ: {results['factor_analysis']['error']}")

        return results
    
    def _perform_anova_analysis(self, skill_metrics_df: pd.DataFrame) -> Dict:
        """ANOVAè§£æã®å®Ÿè¡Œ"""
        anova_results = {}
        
        # æ•°å€¤ã‚¹ã‚­ãƒ«æŒ‡æ¨™åˆ—ã‚’å–å¾—
        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                        'jerk_score', 'control_stability', 'temporal_consistency', 
                        'trial_time', 'endpoint_error']
        
        for skill_metric in skill_columns:
            if skill_metric in skill_metrics_df.columns:
                try:
                    # è¢«é¨“è€…é–“ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™å·®ç•°ã‚’ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æã§æ¤œå®š
                    anova_result = self._one_way_anova(skill_metrics_df, skill_metric)
                    anova_results[skill_metric] = anova_result
                    
                except Exception as e:
                    print(f"ANOVAè§£æã‚¨ãƒ©ãƒ¼ ({skill_metric}): {e}")
                    anova_results[skill_metric] = {'error': str(e)}
        
        return anova_results
    
    def _one_way_anova(self, data: pd.DataFrame, metric_name: str) -> Dict:
        """ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æã®å®Ÿè¡Œ"""
        # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups = []
        subject_ids = []
        
        for subject_id, subject_data in data.groupby('subject_id'):
            metric_values = subject_data[metric_name].dropna()
            if len(metric_values) > 0:
                groups.append(metric_values.values)
                subject_ids.append(subject_id)
        
        if len(groups) < 2:
            return {'error': 'åˆ†æã«ååˆ†ãªã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒã‚ã‚Šã¾ã›ã‚“'}
        
        # ANOVAæ¤œå®šã®å®Ÿè¡Œ
        f_statistic, p_value = stats.f_oneway(*groups)
        
        # åŠ¹æœé‡ï¼ˆeta squaredï¼‰ã®è¨ˆç®—
        total_variance = np.var(np.concatenate(groups))
        within_variance = np.mean([np.var(group) for group in groups])
        eta_squared = (total_variance - within_variance) / total_variance
        
        return {
            'f_statistic': f_statistic,
            'p_value': p_value,
            'eta_squared': eta_squared,
            'significant': p_value < 0.05,
            'groups_count': len(groups),
            'subject_ids': subject_ids
        }
    
    def _perform_factor_analysis(self, skill_metrics_df: pd.DataFrame, rotation: str) -> Dict:
        """å› å­åˆ†æã®å®Ÿè¡Œ"""
        try:
            # æ•°å€¤ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®ã¿ã‚’æŠ½å‡º
            skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                            'jerk_score', 'control_stability', 'temporal_consistency', 
                            'trial_time', 'endpoint_error']
            
            # æ¬ æå€¤ã‚’é™¤å»ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            analysis_data = skill_metrics_df[skill_columns].dropna()
            
            if len(analysis_data) < 10:
                return {'error': 'å› å­åˆ†æã«ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°ãŒã‚ã‚Šã¾ã›ã‚“'}
            
            # æ¨™æº–åŒ–ï¼ˆå¿µã®ãŸã‚å†åº¦å®Ÿè¡Œï¼‰
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(analysis_data)
            
            # å› å­æ•°ã®æ±ºå®šï¼ˆå›ºæœ‰å€¤>1ã®åŸºæº–ï¼‰
            correlation_matrix = np.corrcoef(scaled_data.T)
            eigenvalues = np.linalg.eigvals(correlation_matrix)
            n_factors = max(1, np.sum(eigenvalues > 1))
            
            # å› å­åˆ†æã®å®Ÿè¡Œ
            # fa = FactorAnalysis(n_components=n_factors, random_state=42)
            fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)
            fa.fit(scaled_data)
            
            # å› å­è² è·é‡ã®è¨ˆç®—
            # factor_loadings = fa.components_.T
            factor_loadings = fa.loadings_
            
            # å› å­å¾—ç‚¹ã®è¨ˆç®—
            factor_scores = fa.transform(scaled_data)

            # ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¿å­˜
            self.used_standard_scaler = scaler
            self.used_factor_analysis = fa

            result = {
                'n_factors': n_factors,
                'factor_loadings': factor_loadings,
                'factor_scores': factor_scores,
                'explained_variance': eigenvalues[:n_factors],
                'skill_columns': skill_columns,
                'sample_size': len(analysis_data)
            }

            if rotation == 'promax':
                factor_correlation = fa.phi_
                result['factor_correlations'] = factor_correlation

            return result
            
        except Exception as e:
            return {'error': f'å› å­åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}'}

    def _save_anova_plots(self, skill_metrics_df: pd.DataFrame, anova_results: Dict):
        output_dir = self.output_manager.skill_analyzer_output_dir_path
        output_dir.mkdir(parents=True, exist_ok=True)

        # æœ‰æ„å·®ã®ã‚ã‚‹æŒ‡æ¨™ã¨ç„¡ã„æŒ‡æ¨™ã§åˆ†ã‘ã¦è¡¨ç¤º
        significant_metrics = []
        non_significant_metrics = []

        metrics = []
        eta_values = []
        p_values = []

        for metric, result  in anova_results.items():
            if 'error' not in result:
                if result.get('significant', False):
                    significant_metrics.append(metric)
                else:
                    non_significant_metrics.append(metric)

                metrics.append(metric)
                eta_values.append(result['eta_squared'])
                p_values.append(result['p_value'])

        # æœ‰æ„å·®ã‚ã‚Šã®æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if significant_metrics:
            self._create_boxplot_grid(skill_metrics_df, significant_metrics, anova_results, output_dir / 'significant_results.png', "Significant Skill Metrics")

        # æœ‰æ„å·®ãªã—ã®æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if non_significant_metrics:
            self._create_boxplot_grid(skill_metrics_df, non_significant_metrics, anova_results, output_dir / 'non_significant_results.png',
                                      "Non Significant Skill Metrics")

        # å„æŒ‡æ¨™ã®åŠ¹æœé‡ã€på€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if metrics:
            self._create_bar_plot(metrics, eta_values, p_values, output_dir/ 'eta_result.png', "Comparison Effectiveness")

    def _create_boxplot_grid(self, data: pd.DataFrame, metrics: List[str], anova_results: Dict, save_path: Path, title: str):
        """è¤‡æ•°æŒ‡æ¨™ã®ç®±ã²ã’å›³ã‚’ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º"""
        n_metrics = len(metrics)
        n_cols = min(3, n_metrics)
        n_rows = (n_metrics + n_cols - 1) // n_cols

        fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize = (5 * n_cols, 4 * n_rows))

        if n_metrics == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.flatten() if n_cols > 1 else [axes]
        else:
            axes = axes.flatten()

        for i, metric in enumerate(metrics):
            ax = axes[i]

            # ç®±ã²ã’å›³ã®ä½œæˆ
            sns.boxplot(data=data, x='subject_id', y=metric, ax=ax)
            ax.set_title(f'{metric}\n(p={anova_results[metric]["p_value"]:.3f})',
                         fontsize=12)
            ax.set_xlabel('Subject ID')
            ax.set_ylabel('Skill Metrics')
            ax.tick_params(axis='x', rotation=45)

            # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for j in range(i + 1, len(axes)):
            axes[j].set_visible(False)

        fig.suptitle(title, fontsize=16, y=1.02)
        fig.tight_layout()

        try:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ANOVAç®±ã²ã’å›³ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_bar_plot(self, metrics: List, eta_values: List, p_values:List, save_path: Path, title: str):
        """åŠ¹æœé‡ã¨på€¤ã®æ£’ã‚°ãƒ©ãƒ•ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        data = {
            'metrics': metrics,
            'eta': eta_values,
            'p_value': p_values
        }
        df = pd.DataFrame(data)

        fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (12, 5))

        # åŠ¹æœé‡ã®ãƒ—ãƒ­ãƒƒãƒˆ
        sns.barplot(data=df, x='metrics', y='eta', ax=axes[0])
        axes[0].set_title('Effectiveness')
        axes[0].tick_params(axis='x', rotation=45, labelsize=6)

        # på€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ
        sns.barplot(data=df, x='metrics', y='p_value', ax=axes[1])
        axes[1].set_title('p-value')
        axes[1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Î±=0.05')
        axes[1].legend()
        axes[1].tick_params(axis='x', rotation=45, labelsize=6)

        fig.suptitle(title, fontsize=16, y=1.02)
        fig.tight_layout()

        try:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ANOVAçµæœåŠ¹æœé‡ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _save_factor_analysis_plots(self, skill_metrics_df: pd.DataFrame, factor_analysis_results: Dict):
        """å› å­åˆ†æçµæœã®å¯è¦–åŒ–"""
        output_dir = self.output_manager.skill_analyzer_output_dir_path
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        self._create_factor_loading_heatmap(factor_analysis_results, output_dir / 'factor_loadings_heatmap.png')

        # 2. å› å­ç‰¹å…¸æ•£å¸ƒå›³(æœ€åˆã®2å› å­)
        if factor_analysis_results['n_factors'] >= 2:
            self._create_factor_score_scatter(skill_metrics_df, factor_analysis_results,
                                              output_dir / 'factor_scores_scatter.png')
        # 3. å› å­å¯„ä¸ç‡æ£’ã‚°ãƒ©ãƒ•
        self._create_factor_variance_plot(factor_analysis_results, output_dir/ 'factor_variance_explained.png')

        # 4. promax å›è»¢ã®å ´åˆã¯å› å­ç›¸é–¢è¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        self._create_factor_correlation_heatmap(factor_analysis_results, output_dir / 'factor_correlation_heatmap.png')

    def _create_factor_loading_heatmap(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ"""
        loadings = factor_analysis_results['factor_loadings']
        skill_columns = factor_analysis_results['skill_columns']
        n_factors = factor_analysis_results['n_factors']

        # DataFrameã«å¤‰æ›
        loading_df = pd.DataFrame(
            loadings,
            index=skill_columns,
            columns=[f'Factor {i+1}' for i in range(n_factors)]
        )

        fig, axes = plt.subplots(figsize = (8, 6))
        sns.heatmap(loading_df, annot=True, cmap='RdBu_r', center=0,
                    fmt ='.2f', ax=axes, cbar_kws={'label': 'Factor Loading'})
        axes.set_title('Factor Loading Matrix', fontsize=14)
        axes.set_xlabel('Factor')
        axes.set_ylabel('Skill Metrics')

        try:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å› å­è² è·é‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_factor_score_scatter(self, skill_metrics_df: pd.DataFrame, factor_analysis_results: Dict, save_path: Path):
        """å› å­ç‰¹å…¸ã®æ•£å¸ƒå›³ä½œæˆ(ç¬¬ä¸€å› å­ vs ç¬¬äºŒå› å­)"""
        factor_scores = factor_analysis_results['factor_scores']

        # è¢«é¨“è€…æƒ…å ±ã‚’è¿½åŠ 
        analysis_data = skill_metrics_df[factor_analysis_results['skill_columns']].dropna()
        subject_info = skill_metrics_df.loc[analysis_data.index, ['subject_id', 'block']]

        fig, axes = plt.subplots(figsize=(10, 8))

        # è¢«é¨“è€…ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦æ•£å¸ƒå›³ã‚’ä½œæˆ
        unique_subjects = subject_info['subject_id'].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_subjects)))

        for i, subject in enumerate(unique_subjects):
            mask = subject_info['subject_id'] == subject
            if np.sum(mask) > 0:
                subject_scores = factor_scores[mask]
                axes.scatter(subject_scores[:, 0], subject_scores[:, 1],
                           c=[colors[i]], label=f'Subject {subject}', alpha=0.7, s=50)

        axes.set_xlabel('Factor 1 score')
        axes.set_ylabel('Factor 2 score')
        axes.set_title('Factor Scatter Plotï¼ˆPer subjectsï¼‰', fontsize=14)
        axes.grid(True, alpha=0.3)
        axes.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # è»¸ã®äº¤ç‚¹ã«ç·šã‚’è¿½åŠ 
        axes.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        axes.axvline(x=0, color='k', linestyle='-', alpha=0.3)

        try:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å› å­å¾—ç‚¹æ•£å¸ƒå›³ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ æ•£å¸ƒå›³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_factor_variance_plot(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­ã®å¯„ä¸ç‡ï¼ˆèª¬æ˜åˆ†æ•£ï¼‰æ£’ã‚°ãƒ©ãƒ•"""
        eigenvalues = factor_analysis_results['explained_variance']
        n_factors = factor_analysis_results['n_factors']

        # å¯„ä¸ç‡ã®è¨ˆç®—
        total_variance = np.sum(eigenvalues)
        contribution_ratios = eigenvalues / total_variance * 100
        cumulative_ratios = np.cumsum(contribution_ratios)

        factor_names = [f'Factor {i + 1}' for i in range(n_factors)]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # å¯„ä¸ç‡æ£’ã‚°ãƒ©ãƒ•
        bars = ax1.bar(factor_names, contribution_ratios, alpha=0.7, color='steelblue')
        ax1.set_title('Explained Variance', fontsize=12)
        ax1.set_ylabel('Explained Variance (%)')
        ax1.set_xlabel('Factor')

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, ratio in zip(bars, contribution_ratios):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2., height,
                     f'{ratio:.1f}%', ha='center', va='bottom')

        # ç´¯ç©å¯„ä¸ç‡æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
        ax2.plot(factor_names, cumulative_ratios, marker='o', color='red', linewidth=2)
        ax2.set_title(' Cumulative Explained Variance ', fontsize=12)
        ax2.set_ylabel(' Cumulative Explained Variance  (%)')
        ax2.set_xlabel('Factor')
        ax2.grid(True, alpha=0.3)

        # å€¤ã‚’ç‚¹ã®ä¸Šã«è¡¨ç¤º
        for i, ratio in enumerate(cumulative_ratios):
            ax2.text(i, ratio + 2, f'{ratio:.1f}%', ha='center', va='bottom')

        fig.suptitle('Factor Analysisï¼šExplained Variance', fontsize=14, y=1.02)
        fig.tight_layout()

        try:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å› å­å¯„ä¸ç‡ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ å¯„ä¸ç‡ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close(fig)

    def _create_factor_correlation_heatmap(self, factor_analysis_results: Dict, save_path: Path):
        """å› å­ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ"""

        # å› å­ç›¸é–¢è¡Œåˆ—ã®å­˜åœ¨ç¢ºèª
        fa_correlation = factor_analysis_results.get('factor_correlations')
        if fa_correlation is None:
            print("ğŸ’¡ å› å­é–“ç›¸é–¢è¡Œåˆ—ã¯æ–œäº¤å›è»¢ï¼ˆpromaxãªã©ï¼‰ã®å ´åˆã®ã¿ãƒ—ãƒ­ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚")
            return

        plt.figure(figsize=(8, 6))
        sns.heatmap(fa_correlation,
                    annot=True,
                    fmt='.2f',
                    cmap='RdBu_r',
                    vmin=-1,
                    vmax=1)
        plt.title('Factor Correlation Matrix')
        plt.xlabel('Factor')
        plt.ylabel('Factor')

        try:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å› å­é–“ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close()


    @property
    def factorize_artifact(self):
        if self.used_standard_scaler is not None and self.used_factor_analysis is not None:
            return self.used_standard_scaler, self.used_factor_analysis
        else:
            print("å­¦ç¿’æ¸ˆã¿Standard Scalerã¨Factor Analysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return None


class SkillScoreCalculator:
    """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
    def __init__(self, config: Dict, output_manager: OutputManager):
        self.config = config
        self.output = output_manager
        self.factor_weights = np.array([-0.565, 0.245, -0.19])

    def calc_skill_score(self, skill_metrics_df: pd.DataFrame, expert_scaler: StandardScaler, expert_fa: FactorAnalyzer ) -> pd.DataFrame:
        """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
        plot_data_list = []

        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                         'jerk_score', 'control_stability', 'temporal_consistency',
                         'trial_time', 'endpoint_error']

        for subject_id, subject_df in skill_metrics_df.groupby('subject_id'):
            sorted_trials = subject_df.sort_values(by=['block', 'trial_num']).reset_index()

            for i, trial_row in sorted_trials.iterrows():
                try:
                    # CLAUDE_ADDED: 1ãƒˆãƒ©ã‚¤ã‚¢ãƒ«åˆ†ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’DataFrameã¨ã—ã¦æŠ½å‡ºï¼ˆç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    trial_metrics_df = trial_row[skill_columns].to_frame().T
                    
                    # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿å‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ•°å€¤å‹ã«å¤‰æ›
                    # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦
                    for col in skill_columns:
                        trial_metrics_df[col] = pd.to_numeric(trial_metrics_df[col], errors='coerce')
                    
                    # æ¬ æå€¤ç¢ºèª
                    if trial_metrics_df.isna().any().any():
                        continue

                    # CLAUDE_ADDED: å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã§æ¨™æº–åŒ–ï¼ˆDataFrameå½¢å¼ã§æ¸¡ã—ã¦ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    scaled_metrics = expert_scaler.transform(trial_metrics_df)

                    # å­¦ç¿’æ¸ˆã¿FAãƒ¢ãƒ‡ãƒ«ã§å› å­å¾—ç‚¹ã‚’è¨ˆç®—
                    factor_scores = expert_fa.transform(scaled_metrics)

                    # å› å­å¾—ç‚¹ã‚’é‡ã¿ä»˜ã‘ã—ã¦å˜ä¸€ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«åˆç®—
                    skill_score = np.dot(factor_scores[0], self.factor_weights)

                    plot_data_list.append({
                        'subject_id': subject_id,
                        'trial_order': i + 1,
                        'block': trial_row['block'],
                        'trial_num_in_block': trial_row['trial_num'],
                        'skill_score': skill_score
                    })
                except Exception as e:
                    print(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€… {subject_id}, trial_index {i}: {e}")

        # ãƒªã‚¹ãƒˆã‹ã‚‰æœ€çµ‚çš„ãªDataFrameã‚’ä½œæˆ
        skill_score_df = pd.DataFrame(plot_data_list)

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if skill_score_df is not None:
            self._save_skill_score_plots(skill_score_df)

        return skill_score_df

    def calculate_stable_skill_score(self, skill_metrics_df: pd.DataFrame,expert_scaler: StandardScaler, expert_fa: FactorAnalyzer, window_size=10):
        """å®‰å®šã—ãŸã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        stable_scores = []

        skill_columns = ['curvature', 'velocity_smoothness', 'acceleration_smoothness',
                         'jerk_score', 'control_stability', 'temporal_consistency',
                         'trial_time', 'endpoint_error']

        for subject_id, subject_df in skill_metrics_df.groupby('subject_id'):
            sorted_trials = subject_df.sort_values(by=['block', 'trial_num']).reset_index()

            for i, trial_row in sorted_trials.iterrows():
                try:
                    trial_metrics_df = trial_row[skill_columns].to_frame().T

                    # éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦
                    for col in skill_columns:
                        trial_metrics_df[col] = pd.to_numeric(trial_metrics_df[col], errors='coerce')

                    # æ¬ æå€¤ç¢ºèª
                    if trial_metrics_df.isna().any().any():
                        continue

                    # CLAUDE_ADDED: å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã§æ¨™æº–åŒ–ï¼ˆDataFrameå½¢å¼ã§æ¸¡ã—ã¦ç‰¹å¾´é‡åã‚’ä¿æŒï¼‰
                    scaled_metrics = expert_scaler.transform(trial_metrics_df)

                    # å­¦ç¿’æ¸ˆã¿FAãƒ¢ãƒ‡ãƒ«ã§å› å­å¾—ç‚¹ã‚’è¨ˆç®—
                    factor_scores = expert_fa.transform(scaled_metrics)

                    # å› å­å¾—ç‚¹ã‚’é‡ã¿ä»˜ã‘ã—ã¦å˜ä¸€ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«åˆç®—
                    skill_score = np.dot(factor_scores[0], self.factor_weights)

                    stable_scores.append({
                        'subject_id': subject_id,
                        'trial_order': i + 1,
                        'block': trial_row['block'],
                        'trial_num_in_block': trial_row['trial_num'],
                        'skill_score': skill_score
                    })

                except Exception as e:
                    print(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: è¢«é¨“è€… {subject_id}, trial_index {i}: {e}")

        skill_score_df = pd.DataFrame(stable_scores)

        skill_score_df['smoothed_skill_score'] = skill_score_df.groupby('subject_id')['skill_score'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        if skill_score_df is not None:
            self._save_skill_score_plots(skill_score_df)

        return pd.DataFrame(stable_scores)



    def _save_skill_score_plots(self, skill_scores: pd.DataFrame):
        """è¢«é¨“è€…ã”ã¨ã«ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹"""
        self.output.skill_score_calculator_output_dir_path.mkdir(parents=True, exist_ok=True)

        save_path = self.output.skill_score_calculator_output_dir_path / 'stable_skill_score_transition.png'

        plt.figure(figsize=(12, 7))

        sns.lineplot(
            data =skill_scores,
            x='trial_order',
            y='smoothed_skill_score',
            hue='subject_id',
        )

        plt.title('Skill Score Improvement Over Trials per Subject')
        plt.xlabel('Trial Order')
        plt.ylabel('Calculated Skill Score')
        plt.grid(True)
        plt.legend(title='Subject ID')

        plt.tight_layout()

        try:
            plt.savefig(save_path, dpi=300)
            print(f"âœ… ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢æ¨ç§»ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        plt.close()


class DatasetBuilder:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹ - å‰å‡¦ç†ã¨ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’æ‹…å½“"""
    
    def __init__(self, config: Dict, output_manager: OutputManager):
        self.config = config
        self.output_manager = output_manager
        self.target_seq_len = config['pre_process']['target_seq_len']
        
    def build_skill_trajectory_dataset(self, 
                                     skill_metrics_df: pd.DataFrame, 
                                     preprocessed_trajectory_df: pd.DataFrame,
                                     trained_scaler: StandardScaler,
                                     trained_fa) -> str:
        """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ä»˜ãè»Œé“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã€ä¿å­˜ã™ã‚‹"""
        
        print("ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ä»˜ãè»Œé“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­...")
        
        # CLAUDE_ADDED: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        dataset_output_dir = self.output_manager.dataset_builder_output_dir_path
        dataset_output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        skill_score_calculator = SkillScoreCalculator(self.config, self.output_manager)
        skill_score_df = skill_score_calculator.calc_skill_score(
            skill_metrics_df, trained_scaler, trained_fa
        )
        
        # 2. è»Œé“ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’çµåˆ
        merged_df = self._merge_trajectory_and_skill_data(
            preprocessed_trajectory_df, skill_score_df
        )
        
        # 3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        train_df, test_df = self._split_train_test(merged_df)
        
        # 4. ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaled_train_df, scaled_test_df, scalers, feature_config = self._scale_features(
            train_df, test_df
        )
        
        # 5. ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self._save_dataset_files(
            scaled_train_df, scaled_test_df, scalers, feature_config, dataset_output_dir
        )
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {dataset_output_dir}")
        return str(dataset_output_dir)
    
    def _merge_trajectory_and_skill_data(self, trajectory_df: pd.DataFrame, 
                                       skill_score_df: pd.DataFrame) -> pd.DataFrame:
        """è»Œé“ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""
        
        # CLAUDE_ADDED: ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ©ã‚¤ã‚¢ãƒ«å˜ä½ã§çµåˆ
        merged_data = []
        
        for _, skill_row in skill_score_df.iterrows():
            subject_id = skill_row['subject_id'] 
            block = skill_row['block']
            trial_num = skill_row['trial_num_in_block']
            skill_score = skill_row['skill_score']
            
            # è©²å½“ã™ã‚‹è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            trajectory_subset = trajectory_df[
                (trajectory_df['subject_id'] == subject_id) & 
                (trajectory_df['block'] == block) & 
                (trajectory_df['trial_num'] == trial_num)
            ].copy()
            
            if not trajectory_subset.empty:
                # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«è¿½åŠ 
                trajectory_subset['skill_score'] = skill_score
                merged_data.append(trajectory_subset)
        
        if merged_data:
            return pd.concat(merged_data, ignore_index=True)
        else:
            return pd.DataFrame()
    
    def _split_train_test(self, merged_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """è¢«é¨“è€…ãƒ¬ãƒ™ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²"""
        
        subjects = merged_df['subject_id'].unique()
        test_size = self.config.get('data', {}).get('test_split', 0.2)
        random_seed = self.config.get('data', {}).get('random_seed', 42)
        
        from sklearn.model_selection import train_test_split
        train_subjects, test_subjects = train_test_split(
            subjects, test_size=test_size, random_state=random_seed
        )
        
        train_df = merged_df[merged_df['subject_id'].isin(train_subjects)]
        test_df = merged_df[merged_df['subject_id'].isin(test_subjects)]
        
        print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨è¢«é¨“è€…={len(train_subjects)}äºº, ãƒ†ã‚¹ãƒˆç”¨è¢«é¨“è€…={len(test_subjects)}äºº")
        
        return train_df, test_df
    
    def _scale_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[
        pd.DataFrame, pd.DataFrame, Dict, Dict]:
        """ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
        
        # CLAUDE_ADDED: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡ã®ç‰¹å¾´é‡ã‚’å®šç¾©
        trajectory_features = ['HandlePosX', 'HandlePosY', 'HandleVelX', 
                             'HandleVelY', 'HandleAccX', 'HandleAccY']
        
        scalers = {}
        scaled_train_df = train_df.copy()
        scaled_test_df = test_df.copy()
        
        # ç‰¹å¾´é‡ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        for feature in trajectory_features:
            if feature in train_df.columns:
                scaler = StandardScaler()
                
                # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒƒãƒˆ
                train_values = train_df[feature].values.reshape(-1, 1)
                scaled_train_values = scaler.fit_transform(train_values)
                scaled_train_df[feature] = scaled_train_values.flatten()
                
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ç”¨
                test_values = test_df[feature].values.reshape(-1, 1)
                scaled_test_values = scaler.transform(test_values)
                scaled_test_df[feature] = scaled_test_values.flatten()
                
                scalers[feature] = scaler
        
        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚‚æ­£è¦åŒ–
        if 'skill_score' in train_df.columns:
            skill_scaler = StandardScaler()
            train_skill = train_df['skill_score'].values.reshape(-1, 1)
            scaled_train_df['skill_score'] = skill_scaler.fit_transform(train_skill).flatten()
            
            test_skill = test_df['skill_score'].values.reshape(-1, 1) 
            scaled_test_df['skill_score'] = skill_scaler.transform(test_skill).flatten()
            
            scalers['skill_score'] = skill_scaler
        
        # ç‰¹å¾´é‡è¨­å®š
        feature_config = {
            'feature_cols': trajectory_features + ['skill_score'],
            'trajectory_features': trajectory_features,
            'target_seq_len': self.target_seq_len
        }
        
        return scaled_train_df, scaled_test_df, scalers, feature_config
    
    def _save_dataset_files(self, train_df: pd.DataFrame, test_df: pd.DataFrame, 
                          scalers: Dict, feature_config: Dict, output_dir: Path):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        
        # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜
        train_df.to_parquet(output_dir / 'train_data.parquet', index=False)
        test_df.to_parquet(output_dir / 'test_data.parquet', index=False)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã¨è¨­å®šã‚’ä¿å­˜
        joblib.dump(scalers, output_dir / 'scalers.joblib')
        joblib.dump(feature_config, output_dir / 'feature_config.joblib')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ä¿å­˜
        dataset_info = {
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'train_subjects': train_df['subject_id'].nunique(),
            'test_subjects': test_df['subject_id'].nunique(),
            'feature_columns': feature_config['feature_cols'],
            'target_seq_len': self.target_seq_len,
            'created_at': datetime.datetime.now().isoformat()
        }
        
        import json
        with open(output_dir / 'dataset_info.json', 'w') as f:
            json.dump(dataset_info, f, indent=2)
            
        print(f"ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - train_data.parquet: {len(train_df):,} ã‚µãƒ³ãƒ—ãƒ«") 
        print(f"  - test_data.parquet: {len(test_df):,} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"  - scalers.joblib: {len(scalers)} ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼")
        print(f"  - feature_config.joblib: è¨­å®šæƒ…å ±")
        print(f"  - dataset_info.json: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")



if __name__ == '__main__':
    config_dir = 'PredictiveLatentSpaceNavigationModel/DataPreprocess/data_preprocess_default_config.yaml'

    try:
        # ã‚³ãƒ³ãƒ•ã‚£ã‚°ã®èª­ã¿è¾¼ã¿
        config_loader = DataPreprocessConfigLoader(config_dir)
        validated_config = config_loader.get_config()

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        output_manager = OutputManager(validated_config)

        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿ã“ã¿
        trajectory_loader = TrajectoryDataLoader(validated_config)

        # ============ ç†Ÿé”ã—ãŸãƒ‡ãƒ¼ã‚¿ã§å› å­åˆ†æã‚’å®Ÿè¡Œ (block_num = 4) ============
        print("============ ç†Ÿé”ã—ãŸãƒ‡ãƒ¼ã‚¿ã§å› å­åˆ†æã‚’å®Ÿè¡Œ (block_num = 4) ============")

        block_num = 4

        # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®è¨ˆç®—
        skill_metrics_calculator = SkillMetricCalculator(validated_config)
        preprocessed_data = trajectory_loader.get_preprocessed_data(block_num)
        skill_metrics_df = skill_metrics_calculator.calculate_skill_metrics(preprocessed_data)

        print(skill_metrics_df['block'].unique())
        print(skill_metrics_df['trial_num'].unique())

        # ANOVA and å› å­åˆ†æ
        skill_analyzer = SkillAnalyzer(validated_config, output_manager)
        skill_analyzer.analyze_skill_metrics(skill_metrics_df, 'promax')

        # å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ã¨å› å­åˆ†æã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        trained_scaler, trained_fa = skill_analyzer.factorize_artifact

        # ============ å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (block_num = 0) ============
        print("============ å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (block_num = 0) ============")
        block_num = 0

        # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
        all_preprocess_data = trajectory_loader.get_preprocessed_data(block_num)
        all_skill_metrics_df = skill_metrics_calculator.calculate_skill_metrics(all_preprocess_data)

        print(all_skill_metrics_df['block'].unique())
        print(all_skill_metrics_df['trial_num'].unique())

        # ============ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¦ä¿å­˜ ============
        print("============ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¦ä¿å­˜ ============")
        
        dataset_builder = DatasetBuilder(validated_config, output_manager)
        dataset_path = dataset_builder.build_skill_trajectory_dataset(
            all_skill_metrics_df, 
            all_preprocess_data,
            trained_scaler, 
            trained_fa
        )
        
        print(f"ğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {dataset_path}")
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢æ¨ç§»ã‚‚åˆ¥é€”è¨ˆç®—ãƒ»ä¿å­˜
        skill_score_calculator = SkillScoreCalculator(validated_config, output_manager)
        skill_score_calculator.calculate_stable_skill_score(all_skill_metrics_df, trained_scaler, trained_fa, 5)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

