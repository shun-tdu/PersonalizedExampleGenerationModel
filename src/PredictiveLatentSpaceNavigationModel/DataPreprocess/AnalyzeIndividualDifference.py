import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.stats import f_oneway
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols
import joblib # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜ã«ä½¿ç”¨
plt.switch_backend('Agg')

def analyze_contradiction(df):
    """çŸ›ç›¾ã®åŸå› ã‚’è©³ç´°åˆ†æ"""
    print("ğŸ” çŸ›ç›¾ã®åŸå› åˆ†æ")
    print("=" * 50)

    subjects = df['subject_id'].unique()

    # 1. åˆ†é›¢ã‚¹ã‚³ã‚¢è¨ˆç®—ã®è©³ç´°ç¢ºèª
    print("1. åˆ†é›¢ã‚¹ã‚³ã‚¢è¨ˆç®—ã®æ¤œè¨¼:")

    feature_groups = {
        'Position': ['HandlePosDiffX', 'HandlePosDiffY'],
        'Velocity': ['HandleVelDiffX', 'HandleVelDiffY'],
        'Acceleration': ['HandleAccDiffX', 'HandleAccDiffY']
    }

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            print(f"\n{group_name}ã‚°ãƒ«ãƒ¼ãƒ—:")

            # è¢«é¨“è€…ã”ã¨ã®å®Ÿéš›ã®æ•°å€¤ã‚’ç¢ºèª
            subject_stats = {}
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 0:
                    stats = {
                        'mean': np.mean(subject_data, axis=0),
                        'std': np.std(subject_data, axis=0),
                        'median': np.median(subject_data, axis=0),
                        'data_points': len(subject_data)
                    }
                    subject_stats[subject] = stats
                    print(f"  {subject}: å¹³å‡={stats['mean']}, ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°={stats['data_points']}")

            # è¢«é¨“è€…é–“ã®å®Ÿéš›ã®å·®ã‚’è¨ˆç®—
            if len(subject_stats) > 1:
                means = np.array([stats['mean'] for stats in subject_stats.values()])
                print(f"  è¢«é¨“è€…é–“ã®å¹³å‡å€¤ç¯„å›²:")
                for dim in range(means.shape[1]):
                    dim_values = means[:, dim]
                    print(
                        f"    æ¬¡å…ƒ{dim}: {np.min(dim_values):.6f} ~ {np.max(dim_values):.6f} (ç¯„å›²: {np.ptp(dim_values):.6f})")

    # 2. é‹å‹•ç‰¹æ€§ã§ã®å€‹äººå·®ã®å…·ä½“çš„æ•°å€¤
    print(f"\n2. é‹å‹•ç‰¹æ€§ã§ã®å€‹äººå·®ã®å…·ä½“å€¤:")

    def calculate_movement_features(subject_df):
        """è¢«é¨“è€…ã®é‹å‹•ç‰¹æ€§ã‚’æ•°å€¤åŒ–"""
        features = []

        for trial_num, trial_df in subject_df.groupby('trial_num'):
            if len(trial_df) > 10:
                # ç›´ç·šæ€§
                start_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[0].values
                end_pos = trial_df[['HandlePosX', 'HandlePosY']].iloc[-1].values
                actual_path = trial_df[['HandlePosX', 'HandlePosY']].values

                straight_distance = np.linalg.norm(end_pos - start_pos)
                path_length = np.sum([np.linalg.norm(actual_path[i + 1] - actual_path[i])
                                      for i in range(len(actual_path) - 1)])
                linearity = straight_distance / path_length if path_length > 0 else 0

                # é€Ÿåº¦ç‰¹æ€§
                vel_x = trial_df['HandleVelX'].values
                vel_y = trial_df['HandleVelY'].values
                speed = np.sqrt(vel_x ** 2 + vel_y ** 2)

                peak_time_ratio = np.argmax(speed) / len(speed) if len(speed) > 0 else 0.5

                features.append({
                    'linearity': linearity,
                    'peak_time_ratio': peak_time_ratio,
                    'max_speed': np.max(speed),
                    'avg_speed': np.mean(speed)
                })

        return features

    # å„è¢«é¨“è€…ã®é‹å‹•ç‰¹æ€§
    subject_movement_features = {}
    for subject in subjects:
        subject_df = df[df['subject_id'] == subject]
        features = calculate_movement_features(subject_df)
        if features:
            # å¹³å‡å€¤ã‚’è¨ˆç®—
            avg_features = {}
            for key in features[0].keys():
                values = [f[key] for f in features]
                avg_features[key] = np.mean(values)
            subject_movement_features[subject] = avg_features

    # é‹å‹•ç‰¹æ€§ã§ã®è¢«é¨“è€…é–“å·®ã‚’å®šé‡åŒ–
    print(f"è¢«é¨“è€…åˆ¥é‹å‹•ç‰¹æ€§:")
    for subject, features in subject_movement_features.items():
        print(f"  {subject}: {features}")

    # é‹å‹•ç‰¹æ€§ã§ã®åˆ†é›¢ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    if len(subject_movement_features) > 1:
        print(f"\né‹å‹•ç‰¹æ€§ã§ã®åˆ†é›¢åº¦:")

        for feature_name in ['linearity', 'peak_time_ratio', 'max_speed', 'avg_speed']:
            values = [features[feature_name] for features in subject_movement_features.values()]

            if len(values) > 1:
                between_var = np.var(values)

                # è¢«é¨“è€…å†…åˆ†æ•£ã‚’æ¦‚ç®—ï¼ˆå…¨ä½“åˆ†æ•£ã‹ã‚‰è¢«é¨“è€…é–“åˆ†æ•£ã‚’å¼•ãï¼‰
                all_values = []
                for subject in subjects:
                    subject_df = df[df['subject_id'] == subject]
                    feature_values = calculate_movement_features(subject_df)
                    if feature_values:
                        subject_feature_values = [f[feature_name] for f in feature_values]
                        all_values.extend(subject_feature_values)

                if all_values:
                    total_var = np.var(all_values)
                    within_var = max(total_var - between_var, 0.001)  # æœ€å°å€¤ã§åˆ¶é™
                    separation_score = between_var / within_var

                    print(f"  {feature_name}: åˆ†é›¢ã‚¹ã‚³ã‚¢ = {separation_score:.4f}")

                    if separation_score > 1.0:
                        print(f"    ğŸŸ¢ ã“ã®ç‰¹å¾´ã§ã¯å€‹äººå·®ã‚ã‚Šï¼")
                    else:
                        print(f"    ğŸ”´ ã“ã®ç‰¹å¾´ã§ã‚‚å€‹äººå·®å°‘ãªã„")


def reconcile_contradiction(df):
    """çŸ›ç›¾ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®çµ±åˆåˆ†æ"""
    print(f"\nğŸ”„ çŸ›ç›¾ã®è§£æ±ºç­–åˆ†æ")
    print("=" * 50)

    print("ä»®èª¬1: ç‰¹å¾´æŠ½å‡ºãƒ¬ãƒ™ãƒ«ã®é•ã„")
    print("- å·®åˆ†å€¤ã§ã®åˆ†æ â†’ å€‹äººå·®æ¤œå‡ºå›°é›£")
    print("- é«˜æ¬¡ç‰¹å¾´ã§ã®åˆ†æ â†’ å€‹äººå·®æ¤œå‡ºå¯èƒ½")

    print(f"\nä»®èª¬2: ã‚¹ã‚±ãƒ¼ãƒ«ã®å•é¡Œ")
    print("- å¾®ç´°ãªå·®ãŒå¤§ããªãƒã‚¤ã‚ºã«åŸ‹ã‚‚ã‚Œã¦ã„ã‚‹")
    print("- é©åˆ‡ãªæ­£è¦åŒ–ã«ã‚ˆã‚Šå€‹äººå·®ãŒæµ®ä¸Šã™ã‚‹å¯èƒ½æ€§")

    print(f"\nä»®èª¬3: éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­˜åœ¨")
    print("- ç·šå½¢åˆ†æã§ã¯æ‰ãˆã‚‰ã‚Œãªã„å€‹äººå·®")
    print("- VAEã®ã‚ˆã†ãªéç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºå¯èƒ½")

    # å®Ÿéš›ã«éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ç´¢
    subjects = df['subject_id'].unique()

    # å„è¢«é¨“è€…ã®ã€Œé‹å‹•ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã€ã‚’ä½œæˆ
    print(f"\nğŸ¯ çµ±åˆçš„é‹å‹•ã‚·ã‚°ãƒãƒãƒ£ãƒ¼åˆ†æ:")

    subject_signatures = {}

    for subject in subjects:
        subject_df = df[df['subject_id'] == subject]

        # è¤‡æ•°ã®ç‰¹å¾´ã‚’çµ±åˆ
        signature_features = []

        for trial_num, trial_df in subject_df.groupby('trial_num'):
            if len(trial_df) > 20:
                # 1. è»Œé“ç‰¹å¾´
                positions = trial_df[['HandlePosX', 'HandlePosY']].values
                velocities = trial_df[['HandleVelX', 'HandleVelY']].values

                # è¤‡åˆç‰¹å¾´ã‚’è¨ˆç®—
                composite_features = {
                    # æ™‚ç©ºé–“ç‰¹å¾´
                    'path_curvature': calculate_path_curvature(positions),
                    'velocity_smoothness': calculate_velocity_smoothness(velocities),
                    'acceleration_jerk': calculate_jerk_metric(trial_df),

                    # å‹•çš„ç‰¹å¾´
                    'movement_rhythm': calculate_movement_rhythm(velocities),
                    'force_modulation': calculate_force_modulation(trial_df),
                }

                signature_features.append(composite_features)

        if signature_features:
            # å¹³å‡ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã‚’è¨ˆç®—
            avg_signature = {}
            for key in signature_features[0].keys():
                values = [f[key] for f in signature_features if not np.isnan(f[key])]
                if values:
                    avg_signature[key] = np.mean(values)
                else:
                    avg_signature[key] = 0.0

            subject_signatures[subject] = avg_signature

    # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã®å€‹äººå·®ã‚’åˆ†æ
    if len(subject_signatures) > 1:
        print(f"çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã®å€‹äººå·®:")

        for subject, signature in subject_signatures.items():
            print(f"  {subject}: {signature}")

        # ã‚·ã‚°ãƒãƒãƒ£ãƒ¼é–“è·é›¢
        subjects_list = list(subject_signatures.keys())
        signatures_matrix = np.array([list(sig.values()) for sig in subject_signatures.values()])

        if signatures_matrix.shape[0] > 1 and signatures_matrix.shape[1] > 0:
            from scipy.spatial.distance import pdist, squareform

            distances = pdist(signatures_matrix, metric='euclidean')
            distance_matrix = squareform(distances)

            print(f"\nè¢«é¨“è€…é–“ã‚·ã‚°ãƒãƒãƒ£ãƒ¼è·é›¢:")
            for i, subj1 in enumerate(subjects_list):
                for j, subj2 in enumerate(subjects_list):
                    if i < j:
                        print(f"  {subj1} vs {subj2}: {distance_matrix[i, j]:.4f}")

            avg_distance = np.mean(distances)
            print(f"å¹³å‡è·é›¢: {avg_distance:.4f}")

            if avg_distance > 0.5:
                print("ğŸŸ¢ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§æ˜ç¢ºãªå€‹äººå·®ã‚ã‚Šï¼")
            elif avg_distance > 0.1:
                print("ğŸŸ¡ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ä¸­ç¨‹åº¦ã®å€‹äººå·®")
            else:
                print("ğŸ”´ çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã‚‚å€‹äººå·®å°‘ãªã„")


# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
def calculate_path_curvature(positions):
    """è»Œé“ã®æ›²ç‡ã‚’è¨ˆç®—"""
    if len(positions) < 3:
        return 0.0

    # å˜ç´”ãªæ›²ç‡è¨ˆç®—
    curvatures = []
    for i in range(1, len(positions) - 1):
        p1, p2, p3 = positions[i - 1], positions[i], positions[i + 1]

        # 3ç‚¹ã‹ã‚‰æ›²ç‡ã‚’è¨ˆç®—
        a = np.linalg.norm(p2 - p1)
        b = np.linalg.norm(p3 - p2)
        c = np.linalg.norm(p3 - p1)

        if a > 0 and b > 0 and c > 0:
            s = (a + b + c) / 2
            area = np.sqrt(max(0, s * (s - a) * (s - b) * (s - c)))
            curvature = 4 * area / (a * b * c) if (a * b * c) > 0 else 0
            curvatures.append(curvature)

    return np.mean(curvatures) if curvatures else 0.0


def calculate_velocity_smoothness(velocities):
    """é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•ã‚’è¨ˆç®—"""
    if len(velocities) < 2:
        return 0.0

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
    velocity_changes = np.abs(np.diff(vel_magnitude))
    return 1.0 / (1.0 + np.std(velocity_changes))


def calculate_jerk_metric(trial_df):
    """ã‚¸ãƒ£ãƒ¼ã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    if len(trial_df) < 3:
        return 0.0

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values

    jerk_x = np.diff(acc_x)
    jerk_y = np.diff(acc_y)
    jerk_magnitude = np.sqrt(jerk_x ** 2 + jerk_y ** 2)

    return np.mean(jerk_magnitude)


def calculate_movement_rhythm(velocities):
    """é‹å‹•ãƒªã‚ºãƒ ã‚’è¨ˆç®—"""
    if len(velocities) < 10:
        return 0.0

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)

    # FFTã§å‘¨æ³¢æ•°æˆåˆ†ã‚’åˆ†æ
    try:
        fft = np.fft.fft(vel_magnitude)
        freqs = np.fft.fftfreq(len(vel_magnitude))

        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†
        dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft) // 2])) + 1
        dominant_freq = freqs[dominant_freq_idx]

        return abs(dominant_freq)
    except:
        return 0.0


def calculate_force_modulation(trial_df):
    """åŠ›èª¿ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
    if len(trial_df) < 5:
        return 0.0

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values
    force_estimate = np.sqrt(acc_x ** 2 + acc_y ** 2)

    # åŠ›ã®å¤‰èª¿æŒ‡æ¨™
    force_variability = np.std(force_estimate) / (np.mean(force_estimate) + 1e-6)
    return force_variability

# ======== ANOVAã®ãŸã‚ã®é«˜æ¬¡ç‰¹å¾´é‡è¨ˆç®— ========
def calculate_path_curvature(positions):
    """è»Œé“ã®å¹³å‡æ›²ç‡ã‚’è¨ˆç®—"""
    if len(positions) < 3:
        return np.nan

    curvatures = []
    for i in range(1, len(positions) - 1):
        p1, p2, p3 = positions[i - 1], positions[i], positions[i + 1]

        # 3ç‚¹ã‹ã‚‰æ›²ç‡ã‚’è¨ˆç®—
        a = np.linalg.norm(p2 - p1)
        b = np.linalg.norm(p3 - p2)
        c = np.linalg.norm(p3 - p1)

        if a > 0 and b > 0 and c > 0:
            s = (a + b + c) / 2
            area = np.sqrt(max(0, s * (s - a) * (s - b) * (s - c)))
            curvature = 4 * area / (a * b * c) if (a * b * c) > 0 else 0
            curvatures.append(curvature)

    return np.mean(curvatures) if curvatures else 0.0


def calculate_path_linearity(positions):
    """è»Œé“ã®ç›´ç·šæ€§ã‚’è¨ˆç®—"""
    if len(positions) < 2:
        return np.nan

    start_point = positions[0]
    end_point = positions[-1]
    straight_distance = np.linalg.norm(end_point - start_point)

    actual_distance = np.sum([np.linalg.norm(positions[i + 1] - positions[i])
                              for i in range(len(positions) - 1)])

    return straight_distance / actual_distance if actual_distance > 0 else 0


def calculate_path_efficiency_hf(positions):
    """è»Œé“åŠ¹ç‡æ€§ï¼ˆé«˜æ¬¡ç‰ˆï¼‰"""
    return calculate_path_linearity(positions)


def calculate_velocity_smoothness(velocities):
    """é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•ã‚’è¨ˆç®—"""
    if len(velocities) < 2:
        return np.nan

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
    velocity_changes = np.abs(np.diff(vel_magnitude))

    # æ­£è¦åŒ–ã•ã‚ŒãŸæ»‘ã‚‰ã‹ã•æŒ‡æ¨™
    smoothness = 1.0 / (1.0 + np.std(velocity_changes))
    return smoothness


def calculate_velocity_symmetry(velocities):
    """é€Ÿåº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®å¯¾ç§°æ€§"""
    if len(velocities) < 4:
        return np.nan

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)

    # å‰åŠã¨å¾ŒåŠã®ç›¸é–¢ã‚’è¨ˆç®—
    mid_point = len(vel_magnitude) // 2
    first_half = vel_magnitude[:mid_point]
    second_half = vel_magnitude[mid_point:mid_point + len(first_half)][::-1]

    if len(first_half) > 1 and len(second_half) > 1:
        correlation = np.corrcoef(first_half, second_half)[0, 1]
        return correlation if not np.isnan(correlation) else 0
    return 0


def calculate_peak_velocity_timing(velocities):
    """æœ€å¤§é€Ÿåº¦åˆ°é”ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆæ­£è¦åŒ–ï¼‰"""
    if len(velocities) < 2:
        return np.nan

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)
    peak_time_ratio = np.argmax(vel_magnitude) / len(vel_magnitude)

    return peak_time_ratio


def calculate_jerk_metric(trial_df):
    """ã‚¸ãƒ£ãƒ¼ã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    if len(trial_df) < 3:
        return np.nan

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values

    jerk_x = np.diff(acc_x)
    jerk_y = np.diff(acc_y)
    jerk_magnitude = np.sqrt(jerk_x ** 2 + jerk_y ** 2)

    return np.mean(jerk_magnitude)


def calculate_acceleration_smoothness(trial_df):
    """åŠ é€Ÿåº¦ã®æ»‘ã‚‰ã‹ã•"""
    if len(trial_df) < 3:
        return np.nan

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values
    acc_magnitude = np.sqrt(acc_x ** 2 + acc_y ** 2)

    acc_changes = np.abs(np.diff(acc_magnitude))
    return 1.0 / (1.0 + np.std(acc_changes))


def calculate_movement_rhythm(velocities):
    """é‹å‹•ãƒªã‚ºãƒ ï¼ˆä¸»è¦å‘¨æ³¢æ•°æˆåˆ†ï¼‰"""
    if len(velocities) < 10:
        return np.nan

    vel_magnitude = np.sqrt(velocities[:, 0] ** 2 + velocities[:, 1] ** 2)

    try:
        # FFTã§å‘¨æ³¢æ•°è§£æ
        fft = np.fft.fft(vel_magnitude)
        freqs = np.fft.fftfreq(len(vel_magnitude))

        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†
        dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft) // 2])) + 1
        dominant_freq = abs(freqs[dominant_freq_idx])

        return dominant_freq
    except:
        return np.nan


def calculate_force_modulation(trial_df):
    """åŠ›èª¿ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    if len(trial_df) < 5:
        return np.nan

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values
    force_estimate = np.sqrt(acc_x ** 2 + acc_y ** 2)

    # åŠ›ã®å¤‰èª¿æŒ‡æ¨™
    force_variability = np.std(force_estimate) / (np.mean(force_estimate) + 1e-6)
    return force_variability


def calculate_temporal_consistency(trial_df):
    """æ™‚é–“çš„ä¸€è²«æ€§"""
    if len(trial_df) < 10:
        return np.nan

    timestamps = trial_df['Timestamp'].values
    time_intervals = np.diff(timestamps)

    # æ™‚é–“é–“éš”ã®ä¸€è²«æ€§
    consistency = 1.0 / (1.0 + np.std(time_intervals) / np.mean(time_intervals))
    return consistency


def calculate_movement_efficiency_index(trial_df):
    """ç·åˆé‹å‹•åŠ¹ç‡æŒ‡æ¨™"""
    if len(trial_df) < 5:
        return np.nan

    # è¤‡æ•°æŒ‡æ¨™ã®çµ±åˆ
    positions = trial_df[['HandlePosX', 'HandlePosY']].values
    velocities = trial_df[['HandleVelX', 'HandleVelY']].values

    linearity = calculate_path_linearity(positions)
    smoothness = calculate_velocity_smoothness(velocities)
    jerk = calculate_jerk_metric(trial_df)

    # æ­£è¦åŒ–ã—ã¦çµ±åˆï¼ˆã‚¸ãƒ£ãƒ¼ã‚¯ã¯é€†æ•°ï¼‰
    efficiency = (linearity + smoothness) / (1 + jerk)
    return efficiency


def calculate_control_stability(trial_df):
    """åˆ¶å¾¡å®‰å®šæ€§æŒ‡æ¨™"""
    if len(trial_df) < 5:
        return np.nan

    acc_x = trial_df['HandleAccX'].values
    acc_y = trial_df['HandleAccY'].values

    # åŠ é€Ÿåº¦ã®æ¨™æº–åå·®ï¼ˆåˆ¶å¾¡ã®å®‰å®šæ€§ã®é€†æŒ‡æ¨™ï¼‰
    acc_std = np.std(np.sqrt(acc_x ** 2 + acc_y ** 2))
    stability = 1.0 / (1.0 + acc_std)

    return stability

# å®Ÿè¡Œé–¢æ•°
def resolve_contradiction_analysis(df):
    """çŸ›ç›¾ã®è§£æ±ºåˆ†æã‚’å®Ÿè¡Œ"""
    print("ğŸ¯ çŸ›ç›¾è§£æ±ºåˆ†æ")
    print("=" * 60)

    analyze_contradiction(df)
    reconcile_contradiction(df)

    print(f"\nğŸ’¡ çµè«–:")
    print(f"å·®åˆ†ãƒ¬ãƒ™ãƒ«ã§ã¯å€‹äººå·®ãŒæ¤œå‡ºå›°é›£ã ãŒã€")
    print(f"é«˜æ¬¡ç‰¹å¾´ã‚„çµ±åˆã‚·ã‚°ãƒãƒãƒ£ãƒ¼ã§ã¯å€‹äººå·®ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ã€‚")
    print(f"VAEã®ã‚ˆã†ãªéç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ½œåœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ãŒæœ‰æœ›ã€‚")


# ---- ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å€‹äººå·®ANOVAè§£æ ----
def perform_comprehensive_anova_analysis(df, output_dir='./anova_output'):
    """
    å„ç‰©ç†é‡ã«ã¤ã„ã¦è¢«é¨“è€…é–“ã®å€‹äººå·®ã‚’ANOVAã§åˆ†æ

    Parameters:
    df: DataFrame - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

    print("ğŸ”¬ ANOVAåˆ†æã«ã‚ˆã‚‹è¢«é¨“è€…é–“å€‹äººå·®æ¤œå®š")
    print("=" * 70)

    # åˆ†æå¯¾è±¡ã®ç‰©ç†é‡
    physical_quantities = [
        'HandlePosX', 'HandlePosY',
        'HandleVelX', 'HandleVelY',
        'HandleAccX', 'HandleAccY',
        'HandlePosDiffX', 'HandlePosDiffY',
        'HandleVelDiffX', 'HandleVelDiffY',
        'HandleAccDiffX', 'HandleAccDiffY'
    ]

    # åˆ©ç”¨å¯èƒ½ãªç‰©ç†é‡ã®ã¿ã‚’æŠ½å‡º
    available_quantities = [col for col in physical_quantities if col in df.columns]
    print(f"åˆ†æå¯¾è±¡ç‰©ç†é‡: {len(available_quantities)}å€‹")
    print(f"å¯¾è±¡: {available_quantities}")

    # è¢«é¨“è€…æ•°ã¨ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã®ç¢ºèª
    subjects = df['subject_id'].unique()
    print(f"\nè¢«é¨“è€…æ•°: {len(subjects)}")
    print(f"ç·ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(df)}")

    # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°
    subject_counts = df['subject_id'].value_counts()
    print(f"è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°:")
    for subject, count in subject_counts.items():
        print(f"  {subject}: {count}ç‚¹")

    # ANOVAçµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    anova_results = []

    print(f"\n" + "=" * 70)
    print("ANOVAåˆ†æçµæœ")
    print("=" * 70)

    for quantity in available_quantities:
        print(f"\nğŸ“Š {quantity} ã®åˆ†æ")
        print("-" * 50)

        # æ¬ æå€¤ã‚’é™¤å»
        clean_data = df[['subject_id', quantity]].dropna()

        if len(clean_data) == 0:
            print(f"âš ï¸  {quantity}: ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            continue

        # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        subject_data = []
        subject_stats = {}

        for subject in subjects:
            subject_values = clean_data[clean_data['subject_id'] == subject][quantity].values
            if len(subject_values) > 0:
                subject_data.append(subject_values)
                subject_stats[subject] = {
                    'mean': np.mean(subject_values),
                    'std': np.std(subject_values),
                    'n': len(subject_values)
                }

        # è¢«é¨“è€…ãŒ2äººä»¥ä¸Šã„ã‚‹å ´åˆã®ã¿ANOVAã‚’å®Ÿè¡Œ
        if len(subject_data) < 2:
            print(f"âš ï¸  {quantity}: ANOVAå®Ÿè¡Œã«ã¯æœ€ä½2è¢«é¨“è€…å¿…è¦")
            continue

        try:
            # 1. scipy.stats.f_onewayã«ã‚ˆã‚‹ANOVA
            f_statistic, p_value = f_oneway(*subject_data)

            # 2. statsmodelsã«ã‚ˆã‚‹è©³ç´°ãªANOVA
            formula = f'{quantity} ~ C(subject_id)'
            model = ols(formula, data=clean_data).fit()
            anova_table = anova_lm(model, typ=2)

            # 3. åŠ¹æœé‡ï¼ˆeta-squaredï¼‰ã®è¨ˆç®—
            ss_between = anova_table.loc['C(subject_id)', 'sum_sq']
            ss_total = anova_table['sum_sq'].sum()
            eta_squared = ss_between / ss_total

            # 4. çµæœã®è¡¨ç¤º
            print(f"Fçµ±è¨ˆé‡: {f_statistic:.4f}")
            print(f"på€¤: {p_value:.6f}")
            print(f"åŠ¹æœé‡ (Î·Â²): {eta_squared:.4f}")

            # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
            alpha = 0.05
            if p_value < alpha:
                print(f"ğŸŸ¢ çµ±è¨ˆçš„æœ‰æ„ (p < {alpha}) - è¢«é¨“è€…é–“ã«å€‹äººå·®ã‚ã‚Š")
                significance = "æœ‰æ„"
            else:
                print(f"ğŸ”´ çµ±è¨ˆçš„éæœ‰æ„ (p â‰¥ {alpha}) - è¢«é¨“è€…é–“ã«å€‹äººå·®ãªã—")
                significance = "éæœ‰æ„"

            # åŠ¹æœé‡ã®è§£é‡ˆ
            if eta_squared >= 0.14:
                effect_size = "å¤§"
            elif eta_squared >= 0.06:
                effect_size = "ä¸­"
            elif eta_squared >= 0.01:
                effect_size = "å°"
            else:
                effect_size = "ç„¡è¦–ã§ãã‚‹"

            print(f"åŠ¹æœé‡ã®å¤§ãã•: {effect_size}")

            # è¢«é¨“è€…åˆ¥çµ±è¨ˆã®è¡¨ç¤º
            print(f"\nè¢«é¨“è€…åˆ¥çµ±è¨ˆ:")
            for subject, stats in subject_stats.items():
                print(f"  {subject}: å¹³å‡={stats['mean']:.6f}, "
                      f"æ¨™æº–åå·®={stats['std']:.6f}, N={stats['n']}")

            # çµæœã‚’è¨˜éŒ²
            anova_results.append({
                'quantity': quantity,
                'f_statistic': f_statistic,
                'p_value': p_value,
                'eta_squared': eta_squared,
                'significance': significance,
                'effect_size': effect_size,
                'n_subjects': len(subject_data),
                'total_n': len(clean_data)
            })

        except Exception as e:
            print(f"âš ï¸  {quantity}: ANOVAåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ - {str(e)}")

    # ç·åˆçµæœã®è¡¨ç¤º
    print(f"\n" + "=" * 70)
    print("ç·åˆåˆ†æçµæœ")
    print("=" * 70)

    if anova_results:
        results_df = pd.DataFrame(anova_results)

        # æœ‰æ„ãªç‰©ç†é‡
        significant_quantities = results_df[results_df['significance'] == 'æœ‰æ„']['quantity'].tolist()
        non_significant_quantities = results_df[results_df['significance'] == 'éæœ‰æ„']['quantity'].tolist()

        print(f"çµ±è¨ˆçš„æœ‰æ„ãªç‰©ç†é‡ ({len(significant_quantities)}å€‹):")
        if significant_quantities:
            for qty in significant_quantities:
                result = results_df[results_df['quantity'] == qty].iloc[0]
                print(f"  ğŸŸ¢ {qty}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f} ({result['effect_size']})")
        else:
            print("  ãªã—")

        print(f"\nçµ±è¨ˆçš„éæœ‰æ„ãªç‰©ç†é‡ ({len(non_significant_quantities)}å€‹):")
        if non_significant_quantities:
            for qty in non_significant_quantities:
                result = results_df[results_df['quantity'] == qty].iloc[0]
                print(f"  ğŸ”´ {qty}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f}")
        else:
            print("  ãªã—")

        # på€¤ã«ã‚ˆã‚‹é †ä½ä»˜ã‘
        print(f"\nğŸ† å€‹äººå·®ã®å¼·ã„é †ãƒ©ãƒ³ã‚­ãƒ³ã‚° (på€¤ã®æ˜‡é †):")
        sorted_results = results_df.sort_values('p_value')
        for i, (_, result) in enumerate(sorted_results.iterrows(), 1):
            status = "ğŸŸ¢" if result['significance'] == 'æœ‰æ„' else "ğŸ”´"
            print(f"  {i}ä½: {status} {result['quantity']} "
                  f"(p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f})")

        return results_df

    else:
        print("åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None


def create_anova_visualization(df, anova_results_df, output_dir='./anova_output'):
    """
    ANOVAçµæœã®å¯è¦–åŒ–
    """
    if anova_results_df is None or len(anova_results_df) == 0:
        print("å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # å›³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Visualization of ANOVA analysis results', fontsize=16)

    # 1. på€¤ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    ax1 = axes[0, 0]
    quantities = anova_results_df['quantity']
    p_values = anova_results_df['p_value']
    colors = ['green' if p < 0.05 else 'red' for p in p_values]

    bars1 = ax1.bar(range(len(quantities)), p_values, color=colors, alpha=0.7)
    ax1.axhline(y=0.05, color='black', linestyle='--', alpha=0.5, label='Î±=0.05')
    ax1.set_xlabel('Physical Quantity')
    ax1.set_ylabel('p value')
    ax1.set_title('ANOVA p value (Green:Significant, Red:Non Significant)')
    ax1.set_xticks(range(len(quantities)))
    ax1.set_xticklabels(quantities, rotation=45, ha='right')
    ax1.legend()
    ax1.set_yscale('log')

    # 2. åŠ¹æœé‡ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    ax2 = axes[0, 1]
    eta_squared = anova_results_df['eta_squared']

    bars2 = ax2.bar(range(len(quantities)), eta_squared, color='blue', alpha=0.7)
    ax2.set_xlabel('Physical Quantity')
    ax2.set_ylabel('Effectiveness (Î·Â²)')
    ax2.set_title('ANOVA Effectiveness')
    ax2.set_xticks(range(len(quantities)))
    ax2.set_xticklabels(quantities, rotation=45, ha='right')

    # åŠ¹æœé‡ã®è§£é‡ˆç·šã‚’è¿½åŠ 
    ax2.axhline(y=0.01, color='gray', linestyle=':', alpha=0.5, label='Small Effect')
    ax2.axhline(y=0.06, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')
    ax2.axhline(y=0.14, color='red', linestyle=':', alpha=0.5, label='Highly Effect')
    ax2.legend()

    # 3. Fçµ±è¨ˆé‡ã®ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    ax3 = axes[1, 0]
    f_statistics = anova_results_df['f_statistic']

    bars3 = ax3.bar(range(len(quantities)), f_statistics, color='purple', alpha=0.7)
    ax3.set_xlabel('Physical Quantity')
    ax3.set_ylabel('F Statistical Quantity')
    ax3.set_title('ANOVA F Statistical Quantity')
    ax3.set_xticks(range(len(quantities)))
    ax3.set_xticklabels(quantities, rotation=45, ha='right')

    # 4. æ•£å¸ƒå›³ï¼špå€¤ vs åŠ¹æœé‡
    ax4 = axes[1, 1]
    colors_scatter = ['green' if p < 0.05 else 'red' for p in p_values]
    scatter = ax4.scatter(eta_squared, -np.log10(p_values), c=colors_scatter, alpha=0.7, s=100)

    ax4.set_xlabel('Effectiveness (Î·Â²)')
    ax4.set_ylabel('-logâ‚â‚€(på€¤)')
    ax4.set_title('Effectiveness vs Statistical Significance')
    ax4.axhline(y=-np.log10(0.05), color='black', linestyle='--', alpha=0.5, label='Î±=0.05')
    ax4.axvline(x=0.06, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')

    # å„ç‚¹ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    for i, quantity in enumerate(quantities):
        ax4.annotate(quantity, (eta_squared.iloc[i], -np.log10(p_values.iloc[i])),
                     xytext=(5, 5), textcoords='offset points', fontsize=8)

    ax4.legend()

    plt.tight_layout()

    # ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
    plot_path = os.path.join(output_dir, 'anova_analysis_plots.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯è¦–åŒ–çµæœã‚’ä¿å­˜: {plot_path}")
    plt.close()  # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ãƒ—ãƒ­ãƒƒãƒˆã‚’é–‰ã˜ã‚‹


def perform_post_hoc_analysis(df, significant_quantities, output_dir='./anova_output'):
    """
    äº‹å¾Œåˆ†æï¼šæœ‰æ„å·®ãŒè¦‹ã¤ã‹ã£ãŸç‰©ç†é‡ã«ã¤ã„ã¦è©³ç´°åˆ†æ
    """
    if not significant_quantities:
        print("äº‹å¾Œåˆ†æå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"\n" + "=" * 70)
    print("äº‹å¾Œåˆ†æ (Post-hoc Analysis)")
    print("=" * 70)

    from scipy.stats import tukey_hsd

    for quantity in significant_quantities:
        print(f"\nğŸ“ˆ {quantity} ã®äº‹å¾Œåˆ†æ")
        print("-" * 40)

        clean_data = df[['subject_id', quantity]].dropna()
        subjects = clean_data['subject_id'].unique()

        # Tukey's HSDæ¤œå®š
        try:
            # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            subject_groups = []
            subject_names = []

            for subject in subjects:
                subject_values = clean_data[clean_data['subject_id'] == subject][quantity].values
                if len(subject_values) > 0:
                    subject_groups.append(subject_values)
                    subject_names.append(subject)

            if len(subject_groups) >= 2:
                # Tukey HSDæ¤œå®šã‚’å®Ÿè¡Œ
                tukey_result = tukey_hsd(*subject_groups)

                print(f"Tukey HSDæ¤œå®šçµæœ:")
                print(f"çµ±è¨ˆé‡: {tukey_result.statistic}")
                print(f"på€¤è¡Œåˆ—:")

                # på€¤è¡Œåˆ—ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
                n_subjects = len(subject_names)
                for i in range(n_subjects):
                    for j in range(i + 1, n_subjects):
                        p_val = tukey_result.pvalue[i, j] if hasattr(tukey_result, 'pvalue') else 'N/A'
                        print(f"  {subject_names[i]} vs {subject_names[j]}: p={p_val}")

        except Exception as e:
            print(f"Tukey HSDæ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}")

            # ä»£æ›¿ã¨ã—ã¦ã€å…¨ãƒšã‚¢é–“ã®tæ¤œå®šã‚’å®Ÿè¡Œ
            print(f"ä»£æ›¿åˆ†æï¼šå…¨ãƒšã‚¢é–“tæ¤œå®š")
            from scipy.stats import ttest_ind

            subjects_list = list(subjects)
            for i in range(len(subjects_list)):
                for j in range(i + 1, len(subjects_list)):
                    subj1_data = clean_data[clean_data['subject_id'] == subjects_list[i]][quantity].values
                    subj2_data = clean_data[clean_data['subject_id'] == subjects_list[j]][quantity].values

                    if len(subj1_data) > 1 and len(subj2_data) > 1:
                        t_stat, p_val = ttest_ind(subj1_data, subj2_data)
                        significance = "ğŸŸ¢" if p_val < 0.05 else "ğŸ”´"
                        print(f"  {subjects_list[i]} vs {subjects_list[j]}: "
                              f"t={t_stat:.4f}, p={p_val:.6f} {significance}")


def create_individual_quantity_plots(df, anova_results_df, output_dir='./anova_output'):
    """
    å„ç‰©ç†é‡ã«ã¤ã„ã¦å€‹åˆ¥ã®ç®±ã²ã’å›³ã‚’ä½œæˆ
    """
    if anova_results_df is None or len(anova_results_df) == 0:
        return

    print(f"\nğŸ“ˆ å€‹åˆ¥ç‰©ç†é‡ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")

    # æœ‰æ„ãªç‰©ç†é‡ã¨éæœ‰æ„ãªç‰©ç†é‡ã‚’åˆ†ã‘ã¦å‡¦ç†
    significant_quantities = anova_results_df[anova_results_df['significance'] == 'æœ‰æ„']['quantity'].tolist()
    non_significant_quantities = anova_results_df[anova_results_df['significance'] == 'éæœ‰æ„']['quantity'].tolist()

    all_quantities = anova_results_df['quantity'].tolist()

    # å„ç‰©ç†é‡ã«ã¤ã„ã¦ç®±ã²ã’å›³ã‚’ä½œæˆ
    n_quantities = len(all_quantities)
    n_cols = 4
    n_rows = (n_quantities + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)

    fig.suptitle('Inter-subject comparisons for each physical variable (box plots)', fontsize=16)

    for i, quantity in enumerate(all_quantities):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]

        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        clean_data = df[['subject_id', quantity]].dropna()

        if len(clean_data) > 0:
            # ç®±ã²ã’å›³ã®ä½œæˆ
            subjects = clean_data['subject_id'].unique()
            box_data = [clean_data[clean_data['subject_id'] == subject][quantity].values
                        for subject in subjects]

            box_plot = ax.boxplot(box_data, labels=subjects, patch_artist=True)

            # æœ‰æ„æ€§ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
            if quantity in significant_quantities:
                color = 'lightgreen'
                ax.set_title(f'{quantity}\n(Significant: p<0.05)', fontweight='bold', color='green')
            else:
                color = 'lightcoral'
                ax.set_title(f'{quantity}\n(Non Significant: pâ‰¥0.05)', color='red')

            # ç®±ã®è‰²ã‚’è¨­å®š
            for patch in box_plot['boxes']:
                patch.set_facecolor(color)
                patch.set_alpha(0.7)

        ax.set_xlabel('Subject ID')
        ax.set_ylabel(quantity)
        ax.tick_params(axis='x', rotation=45)

    # ä½¿ç”¨ã—ãªã„è»¸ã‚’éè¡¨ç¤º
    for i in range(len(all_quantities), n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        axes[row, col].set_visible(False)

    plt.tight_layout()

    # ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
    boxplot_path = os.path.join(output_dir, 'individual_quantity_boxplots.png')
    plt.savefig(boxplot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å€‹åˆ¥ç®±ã²ã’å›³ã‚’ä¿å­˜: {boxplot_path}")
    plt.close()


def create_summary_report(anova_results_df, output_dir='./anova_output'):
    """
    åˆ†æçµæœã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ä½œæˆ
    """
    if anova_results_df is None:
        return

    report_path = os.path.join(output_dir, 'anova_analysis_report.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("ANOVAåˆ†æã«ã‚ˆã‚‹è¢«é¨“è€…é–“å€‹äººå·®æ¤œå®š - è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("=" * 70 + "\n\n")

        f.write(f"åˆ†ææ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"åˆ†æå¯¾è±¡ç‰©ç†é‡æ•°: {len(anova_results_df)}\n\n")

        # å…¨ä½“æ¦‚è¦
        significant_count = len(anova_results_df[anova_results_df['significance'] == 'æœ‰æ„'])
        f.write("=" * 40 + "\n")
        f.write("å…¨ä½“æ¦‚è¦\n")
        f.write("=" * 40 + "\n")
        f.write(f"çµ±è¨ˆçš„æœ‰æ„ãªç‰©ç†é‡: {significant_count}/{len(anova_results_df)} å€‹\n")
        f.write(f"æœ‰æ„ç‡: {significant_count / len(anova_results_df) * 100:.1f}%\n\n")

        # è©³ç´°çµæœ
        f.write("=" * 40 + "\n")
        f.write("è©³ç´°çµæœï¼ˆpå€¤æ˜‡é †ï¼‰\n")
        f.write("=" * 40 + "\n")

        sorted_results = anova_results_df.sort_values('p_value')
        for i, (_, result) in enumerate(sorted_results.iterrows(), 1):
            f.write(f"\n{i:2d}. {result['quantity']}\n")
            f.write(f"    Fçµ±è¨ˆé‡: {result['f_statistic']:.4f}\n")
            f.write(f"    på€¤:     {result['p_value']:.6f}\n")
            f.write(f"    åŠ¹æœé‡:   {result['eta_squared']:.4f} ({result['effect_size']})\n")
            f.write(f"    åˆ¤å®š:     {result['significance']}\n")
            f.write(f"    è¢«é¨“è€…æ•°: {result['n_subjects']}\n")
            f.write(f"    ç·ãƒ‡ãƒ¼ã‚¿æ•°: {result['total_n']}\n")

        # çµ±è¨ˆçš„æœ‰æ„ãªç‰©ç†é‡ã®è©³ç´°
        significant_df = anova_results_df[anova_results_df['significance'] == 'æœ‰æ„']
        if len(significant_df) > 0:
            f.write("\n" + "=" * 40 + "\n")
            f.write("çµ±è¨ˆçš„æœ‰æ„ãªç‰©ç†é‡ï¼ˆå€‹äººå·®ã‚ã‚Šï¼‰\n")
            f.write("=" * 40 + "\n")
            for _, result in significant_df.iterrows():
                f.write(f"â€¢ {result['quantity']}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f}\n")

        # çµ±è¨ˆçš„éæœ‰æ„ãªç‰©ç†é‡
        non_significant_df = anova_results_df[anova_results_df['significance'] == 'éæœ‰æ„']
        if len(non_significant_df) > 0:
            f.write("\n" + "=" * 40 + "\n")
            f.write("çµ±è¨ˆçš„éæœ‰æ„ãªç‰©ç†é‡ï¼ˆå€‹äººå·®ãªã—ï¼‰\n")
            f.write("=" * 40 + "\n")
            for _, result in non_significant_df.iterrows():
                f.write(f"â€¢ {result['quantity']}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f}\n")

        # çµè«–
        f.write("\n" + "=" * 40 + "\n")
        f.write("çµè«–\n")
        f.write("=" * 40 + "\n")
        if significant_count > 0:
            f.write(f"åˆ†æã®çµæœã€{significant_count}å€‹ã®ç‰©ç†é‡ã§çµ±è¨ˆçš„ã«æœ‰æ„ãªè¢«é¨“è€…é–“å·®ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚\n")
            f.write("ã“ã‚Œã‚‰ã®ç‰©ç†é‡ã§ã¯å€‹äººå·®ï¼ˆé‹å‹•ã‚¹ã‚¿ã‚¤ãƒ«ã®é•ã„ï¼‰ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚\n")
        else:
            f.write("åˆ†æã®çµæœã€ã„ãšã‚Œã®ç‰©ç†é‡ã«ãŠã„ã¦ã‚‚çµ±è¨ˆçš„ã«æœ‰æ„ãªè¢«é¨“è€…é–“å·®ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n")
            f.write("Point-to-Pointã‚¿ã‚¹ã‚¯ã§ã¯å€‹äººå·®ãŒç¾ã‚Œã«ãã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n")

    print(f"ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")


def compare_scaling_effects(original_results, scaled_results, output_dir):
    """
    ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰å¾Œã§ã®ANOVAçµæœã‚’æ¯”è¼ƒåˆ†æ
    """
    if original_results is None or scaled_results is None:
        print("æ¯”è¼ƒã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return

    print("\n" + "=" * 70)
    print("ğŸ”„ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒåˆ†æ")
    print("=" * 70)

    # å…±é€šã®ç‰©ç†é‡ã‚’æŠ½å‡º
    common_quantities = set(original_results['quantity']) & set(scaled_results['quantity'])

    comparison_data = []

    for quantity in common_quantities:
        orig = original_results[original_results['quantity'] == quantity].iloc[0]
        scaled = scaled_results[scaled_results['quantity'] == quantity].iloc[0]

        # på€¤ã®å¤‰åŒ–
        p_change = scaled['p_value'] / orig['p_value'] if orig['p_value'] > 0 else float('inf')

        # åŠ¹æœé‡ã®å¤‰åŒ–
        eta_change = scaled['eta_squared'] / orig['eta_squared'] if orig['eta_squared'] > 0 else float('inf')

        # æœ‰æ„æ€§ã®å¤‰åŒ–
        sig_change = f"{orig['significance']} â†’ {scaled['significance']}"

        comparison_data.append({
            'quantity': quantity,
            'original_p': orig['p_value'],
            'scaled_p': scaled['p_value'],
            'p_ratio': p_change,
            'original_eta2': orig['eta_squared'],
            'scaled_eta2': scaled['eta_squared'],
            'eta2_ratio': eta_change,
            'significance_change': sig_change
        })

        print(f"\nğŸ“Š {quantity}:")
        print(f"  på€¤: {orig['p_value']:.6f} â†’ {scaled['p_value']:.6f} ({p_change:.2f}å€)")
        print(f"  Î·Â²: {orig['eta_squared']:.6f} â†’ {scaled['eta_squared']:.6f} ({eta_change:.2f}å€)")
        print(f"  æœ‰æ„æ€§: {sig_change}")

    # æ¯”è¼ƒçµæœã‚’DataFrameã¨ã—ã¦ä¿å­˜
    comparison_df = pd.DataFrame(comparison_data)
    comparison_path = os.path.join(output_dir, 'scaling_comparison.csv')
    comparison_df.to_csv(comparison_path, index=False)
    print(f"\nğŸ’¾ æ¯”è¼ƒçµæœã‚’ä¿å­˜: {comparison_path}")

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # på€¤ã®æ¯”è¼ƒ
    ax1 = axes[0]
    x_pos = range(len(comparison_df))
    ax1.bar([x - 0.2 for x in x_pos], comparison_df['original_p'], width=0.4, label='Original', alpha=0.7)
    ax1.bar([x + 0.2 for x in x_pos], comparison_df['scaled_p'], width=0.4, label='Scaled', alpha=0.7)
    ax1.set_xlabel('Physical Quantity')
    ax1.set_ylabel('p value')
    ax1.set_title('Comparison of p-values before and after scaling')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(comparison_df['quantity'], rotation=45, ha='right')
    ax1.set_yscale('log')
    ax1.axhline(y=0.05, color='red', linestyle='--', alpha=0.5)
    ax1.legend()

    # åŠ¹æœé‡ã®æ¯”è¼ƒ
    ax2 = axes[1]
    ax2.bar([x - 0.2 for x in x_pos], comparison_df['original_eta2'], width=0.4, label='Original', alpha=0.7)
    ax2.bar([x + 0.2 for x in x_pos], comparison_df['scaled_eta2'], width=0.4, label='Scaled', alpha=0.7)
    ax2.set_xlabel('Physical Quantity')
    ax2.set_ylabel('Effectiveness (Î·Â²)')
    ax2.set_title('Comparison of p-values before and after scaling')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(comparison_df['quantity'], rotation=45, ha='right')
    ax2.legend()

    plt.tight_layout()
    comparison_plot_path = os.path.join(output_dir, 'scaling_comparison_plot.png')
    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {comparison_plot_path}")
    plt.close()


def main_anova_analysis(df, output_dir='./anova_output'):
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šåŒ…æ‹¬çš„ãªANOVAåˆ†æã‚’å®Ÿè¡Œ

    Parameters:
    df: DataFrame - åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
    output_dir: str - å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
    """
    print("ğŸš€ ANOVAåˆ†æé–‹å§‹")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_dir}")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)

    # 1. ãƒ¡ã‚¤ãƒ³ã®ANOVAåˆ†æ
    anova_results_df = perform_comprehensive_anova_analysis(df, output_dir)

    if anova_results_df is not None:
        # 2. å¯è¦–åŒ–
        print(f"\nğŸ“Š çµæœã®å¯è¦–åŒ–...")
        create_anova_visualization(df, anova_results_df, output_dir)

        # 3. å€‹åˆ¥ç‰©ç†é‡ã®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        create_individual_quantity_plots(df, anova_results_df, output_dir)

        # 4. æœ‰æ„ãªç‰©ç†é‡ã«ã¤ã„ã¦äº‹å¾Œåˆ†æ
        significant_quantities = anova_results_df[anova_results_df['significance'] == 'æœ‰æ„']['quantity'].tolist()
        if significant_quantities:
            perform_post_hoc_analysis(df, significant_quantities, output_dir)

        # 5. çµæœã®ä¿å­˜
        csv_path = os.path.join(output_dir, 'anova_results.csv')
        anova_results_df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ åˆ†æçµæœCSVã‚’ä¿å­˜: {csv_path}")

        # 6. è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
        create_summary_report(anova_results_df, output_dir)

        # 7. å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print(f"\nâœ… ANOVAåˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ ã™ã¹ã¦ã®çµæœã¯ '{output_dir}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        print(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - anova_results.csv: åˆ†æçµæœãƒ‡ãƒ¼ã‚¿")
        print(f"  - anova_analysis_plots.png: ç·åˆå¯è¦–åŒ–")
        print(f"  - individual_quantity_boxplots.png: å€‹åˆ¥ç®±ã²ã’å›³")
        print(f"  - anova_analysis_report.txt: è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ")

        return anova_results_df

    else:
        print("ANOVAåˆ†æã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None


def extract_high_level_features(df):
    """
    å„è©¦è¡Œã‹ã‚‰é«˜æ¬¡ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦DataFrameã‚’ä½œæˆ

    Returns:
    pd.DataFrame: è¢«é¨“è€…ãƒ»è©¦è¡Œã”ã¨ã®é«˜æ¬¡ç‰¹å¾´é‡
    """
    print("ğŸ” é«˜æ¬¡ç‰¹å¾´é‡ã®æŠ½å‡ºé–‹å§‹")
    print("=" * 50)

    high_level_features = []

    # è¢«é¨“è€…ãƒ»è©¦è¡Œã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    for (subject_id, trial_num), trial_df in df.groupby(['subject_id', 'trial_num']):
        if len(trial_df) < 20:  # æœ€å°ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã®ç¢ºä¿
            continue

        try:
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            positions = trial_df[['HandlePosX', 'HandlePosY']].values
            velocities = trial_df[['HandleVelX', 'HandleVelY']].values

            # é«˜æ¬¡ç‰¹å¾´é‡ã‚’è¨ˆç®—
            features = {
                'subject_id': subject_id,
                'trial_num': trial_num,

                # 1. è»Œé“ç‰¹å¾´
                'path_curvature': calculate_path_curvature(positions),
                'path_linearity': calculate_path_linearity(positions),
                'path_efficiency': calculate_path_efficiency_hf(positions),

                # 2. é€Ÿåº¦ç‰¹å¾´
                'velocity_smoothness': calculate_velocity_smoothness(velocities),
                'velocity_symmetry': calculate_velocity_symmetry(velocities),
                'peak_velocity_timing': calculate_peak_velocity_timing(velocities),

                # 3. åŠ é€Ÿåº¦ãƒ»ã‚¸ãƒ£ãƒ¼ã‚¯ç‰¹å¾´
                'jerk_metric': calculate_jerk_metric(trial_df),
                'acceleration_smoothness': calculate_acceleration_smoothness(trial_df),

                # 4. å‹•çš„ç‰¹å¾´
                'movement_rhythm': calculate_movement_rhythm(velocities),
                'force_modulation': calculate_force_modulation(trial_df),
                'temporal_consistency': calculate_temporal_consistency(trial_df),

                # 5. çµ±åˆçš„ç‰¹å¾´
                'movement_efficiency_index': calculate_movement_efficiency_index(trial_df),
                'control_stability': calculate_control_stability(trial_df),
            }

            high_level_features.append(features)

        except Exception as e:
            print(f"âš ï¸ {subject_id}-{trial_num}: ç‰¹å¾´æŠ½å‡ºã‚¨ãƒ©ãƒ¼ - {str(e)}")
            continue

    # DataFrameã«å¤‰æ›
    features_df = pd.DataFrame(high_level_features)

    print(f"âœ… æŠ½å‡ºå®Œäº†: {len(features_df)}è©¦è¡Œ, {len(features_df.columns) - 2}ç‰¹å¾´é‡")
    print(f"è¢«é¨“è€…åˆ¥è©¦è¡Œæ•°:")
    for subject in features_df['subject_id'].unique():
        count = len(features_df[features_df['subject_id'] == subject])
        print(f"  {subject}: {count}è©¦è¡Œ")

    return features_df

def perform_high_level_anova(features_df, output_dir='./high_level_anova_results'):
    """
    é«˜æ¬¡ç‰¹å¾´é‡ã§ã®ANOVAåˆ†æã‚’å®Ÿè¡Œ
    """
    os.makedirs(output_dir, exist_ok=True)

    print("\nğŸ”¬ é«˜æ¬¡ç‰¹å¾´é‡ANOVAåˆ†æé–‹å§‹")
    print("=" * 60)

    # åˆ†æå¯¾è±¡ç‰¹å¾´é‡ï¼ˆsubject_idã¨trial_numã‚’é™¤ãï¼‰
    feature_columns = [col for col in features_df.columns
                       if col not in ['subject_id', 'trial_num']]

    print(f"åˆ†æå¯¾è±¡ç‰¹å¾´é‡: {len(feature_columns)}å€‹")
    print(f"ç‰¹å¾´é‡: {feature_columns}")

    # è¢«é¨“è€…æƒ…å ±
    subjects = features_df['subject_id'].unique()
    print(f"\nè¢«é¨“è€…æ•°: {len(subjects)}")
    print(f"ç·è©¦è¡Œæ•°: {len(features_df)}")

    subject_counts = features_df['subject_id'].value_counts()
    print(f"è¢«é¨“è€…åˆ¥è©¦è¡Œæ•°:")
    for subject, count in subject_counts.items():
        print(f"  {subject}: {count}è©¦è¡Œ")

    # ANOVAçµæœæ ¼ç´
    anova_results = []

    print(f"\n" + "=" * 60)
    print("é«˜æ¬¡ç‰¹å¾´é‡ANOVAåˆ†æçµæœ")
    print("=" * 60)

    for feature in feature_columns:
        print(f"\nğŸ“Š {feature} ã®åˆ†æ")
        print("-" * 40)

        # æ¬ æå€¤ã‚’é™¤å»
        clean_data = features_df[['subject_id', feature]].dropna()

        if len(clean_data) == 0:
            print(f"âš ï¸ {feature}: ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
            continue

        # è¢«é¨“è€…ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
        subject_data = []
        subject_stats = {}

        for subject in subjects:
            subject_values = clean_data[clean_data['subject_id'] == subject][feature].values
            if len(subject_values) > 0:
                subject_data.append(subject_values)
                subject_stats[subject] = {
                    'mean': np.mean(subject_values),
                    'std': np.std(subject_values),
                    'n': len(subject_values)
                }

        if len(subject_data) < 2:
            print(f"âš ï¸ {feature}: è¢«é¨“è€…æ•°ä¸è¶³")
            continue

        try:
            # ANOVAå®Ÿè¡Œ
            f_statistic, p_value = f_oneway(*subject_data)

            # è©³ç´°ANOVA
            formula = f'{feature} ~ C(subject_id)'
            model = ols(formula, data=clean_data).fit()
            anova_table = anova_lm(model, typ=2)

            # åŠ¹æœé‡è¨ˆç®—
            ss_between = anova_table.loc['C(subject_id)', 'sum_sq']
            ss_total = anova_table['sum_sq'].sum()
            eta_squared = ss_between / ss_total

            # çµæœè¡¨ç¤º
            print(f"Fçµ±è¨ˆé‡: {f_statistic:.4f}")
            print(f"på€¤: {p_value:.6f}")
            print(f"åŠ¹æœé‡ (Î·Â²): {eta_squared:.4f}")

            # æœ‰æ„æ€§åˆ¤å®š
            alpha = 0.05
            if p_value < alpha:
                print(f"ğŸŸ¢ çµ±è¨ˆçš„æœ‰æ„ (p < {alpha}) - è¢«é¨“è€…é–“ã«å€‹äººå·®ã‚ã‚Š")
                significance = "æœ‰æ„"
            else:
                print(f"ğŸ”´ çµ±è¨ˆçš„éæœ‰æ„ (p â‰¥ {alpha}) - è¢«é¨“è€…é–“ã«å€‹äººå·®ãªã—")
                significance = "éæœ‰æ„"

            # åŠ¹æœé‡è§£é‡ˆ
            if eta_squared >= 0.14:
                effect_size = "å¤§"
            elif eta_squared >= 0.06:
                effect_size = "ä¸­"
            elif eta_squared >= 0.01:
                effect_size = "å°"
            else:
                effect_size = "ç„¡è¦–ã§ãã‚‹"

            print(f"åŠ¹æœé‡ã®å¤§ãã•: {effect_size}")

            # è¢«é¨“è€…åˆ¥çµ±è¨ˆ
            print(f"\nè¢«é¨“è€…åˆ¥çµ±è¨ˆ:")
            for subject, stats in subject_stats.items():
                print(f"  {subject}: å¹³å‡={stats['mean']:.4f}, "
                      f"æ¨™æº–åå·®={stats['std']:.4f}, N={stats['n']}")

            # çµæœè¨˜éŒ²
            anova_results.append({
                'feature': feature,
                'f_statistic': f_statistic,
                'p_value': p_value,
                'eta_squared': eta_squared,
                'significance': significance,
                'effect_size': effect_size,
                'n_subjects': len(subject_data),
                'total_n': len(clean_data)
            })

        except Exception as e:
            print(f"âš ï¸ {feature}: ANOVAåˆ†æã‚¨ãƒ©ãƒ¼ - {str(e)}")

    # çµæœã¾ã¨ã‚
    if anova_results:
        results_df = pd.DataFrame(anova_results)

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print(f"\n" + "=" * 60)
        print("é«˜æ¬¡ç‰¹å¾´é‡åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)

        significant_features = results_df[results_df['significance'] == 'æœ‰æ„']['feature'].tolist()
        non_significant_features = results_df[results_df['significance'] == 'éæœ‰æ„']['feature'].tolist()

        print(f"çµ±è¨ˆçš„æœ‰æ„ãªç‰¹å¾´é‡ ({len(significant_features)}å€‹):")
        if significant_features:
            for feat in significant_features:
                result = results_df[results_df['feature'] == feat].iloc[0]
                print(
                    f"  ğŸŸ¢ {feat}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f} ({result['effect_size']})")
        else:
            print("  ãªã—")

        print(f"\nçµ±è¨ˆçš„éæœ‰æ„ãªç‰¹å¾´é‡ ({len(non_significant_features)}å€‹):")
        if non_significant_features:
            for feat in non_significant_features:
                result = results_df[results_df['feature'] == feat].iloc[0]
                print(f"  ğŸ”´ {feat}: p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f}")
        else:
            print("  ãªã—")

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        print(f"\nğŸ† å€‹äººå·®ã®å¼·ã„é †ãƒ©ãƒ³ã‚­ãƒ³ã‚° (på€¤æ˜‡é †):")
        sorted_results = results_df.sort_values('p_value')
        for i, (_, result) in enumerate(sorted_results.iterrows(), 1):
            status = "ğŸŸ¢" if result['significance'] == 'æœ‰æ„' else "ğŸ”´"
            print(f"  {i}ä½: {status} {result['feature']} "
                  f"(p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f})")

        return results_df, features_df
    else:
        print("åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, None


def create_high_level_visualization(results_df, features_df, output_dir='./high_level_anova_results'):
    """
    é«˜æ¬¡ç‰¹å¾´é‡ANOVAçµæœã®å¯è¦–åŒ–
    """
    if results_df is None or len(results_df) == 0:
        print("å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"\nğŸ“Š é«˜æ¬¡ç‰¹å¾´é‡çµæœã®å¯è¦–åŒ–...")

    # 1. ç·åˆçµæœãƒ—ãƒ­ãƒƒãƒˆ
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('High-Level Features ANOVA Analysis Results', fontsize=16)

    # på€¤ãƒ—ãƒ­ãƒƒãƒˆ
    ax1 = axes[0, 0]
    features = results_df['feature']
    p_values = results_df['p_value']
    colors = ['green' if p < 0.05 else 'red' for p in p_values]

    bars1 = ax1.bar(range(len(features)), p_values, color=colors, alpha=0.7)
    ax1.axhline(y=0.05, color='black', linestyle='--', alpha=0.5, label='Î±=0.05')
    ax1.set_xlabel('High-Level Features')
    ax1.set_ylabel('p value')
    ax1.set_title('ANOVA p-values (Green:Significant, Red:Non-Significant)')
    ax1.set_xticks(range(len(features)))
    ax1.set_xticklabels(features, rotation=45, ha='right')
    ax1.legend()
    ax1.set_yscale('log')

    # åŠ¹æœé‡ãƒ—ãƒ­ãƒƒãƒˆ
    ax2 = axes[0, 1]
    eta_squared = results_df['eta_squared']

    bars2 = ax2.bar(range(len(features)), eta_squared, color='blue', alpha=0.7)
    ax2.set_xlabel('High-Level Features')
    ax2.set_ylabel('Effect Size (Î·Â²)')
    ax2.set_title('ANOVA Effect Sizes')
    ax2.set_xticks(range(len(features)))
    ax2.set_xticklabels(features, rotation=45, ha='right')

    # åŠ¹æœé‡åŸºæº–ç·š
    ax2.axhline(y=0.01, color='gray', linestyle=':', alpha=0.5, label='Small Effect')
    ax2.axhline(y=0.06, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')
    ax2.axhline(y=0.14, color='red', linestyle=':', alpha=0.5, label='Large Effect')
    ax2.legend()

    # Fçµ±è¨ˆé‡ãƒ—ãƒ­ãƒƒãƒˆ
    ax3 = axes[1, 0]
    f_statistics = results_df['f_statistic']

    bars3 = ax3.bar(range(len(features)), f_statistics, color='purple', alpha=0.7)
    ax3.set_xlabel('High-Level Features')
    ax3.set_ylabel('F-Statistic')
    ax3.set_title('ANOVA F-Statistics')
    ax3.set_xticks(range(len(features)))
    ax3.set_xticklabels(features, rotation=45, ha='right')

    # æ•£å¸ƒå›³
    ax4 = axes[1, 1]
    colors_scatter = ['green' if p < 0.05 else 'red' for p in p_values]
    ax4.scatter(eta_squared, -np.log10(p_values), c=colors_scatter, alpha=0.7, s=100)

    ax4.set_xlabel('Effect Size (Î·Â²)')
    ax4.set_ylabel('-logâ‚â‚€(p-value)')
    ax4.set_title('Effect Size vs Statistical Significance')
    ax4.axhline(y=-np.log10(0.05), color='black', linestyle='--', alpha=0.5, label='Î±=0.05')
    ax4.axvline(x=0.06, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')

    # ãƒ©ãƒ™ãƒ«è¿½åŠ 
    for i, feature in enumerate(features):
        ax4.annotate(feature, (eta_squared.iloc[i], -np.log10(p_values.iloc[i])),
                     xytext=(5, 5), textcoords='offset points', fontsize=8)

    ax4.legend()

    plt.tight_layout()

    # ä¿å­˜
    plot_path = os.path.join(output_dir, 'high_level_features_anova_plots.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç·åˆå¯è¦–åŒ–ã‚’ä¿å­˜: {plot_path}")
    plt.close()

    # 2. æœ‰æ„ãªç‰¹å¾´é‡ã®ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
    significant_features = results_df[results_df['significance'] == 'æœ‰æ„']['feature'].tolist()

    if significant_features:
        n_features = len(significant_features)
        n_cols = 3
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
        if n_rows == 1:
            axes = axes.reshape(1, -1) if n_features > 1 else [axes]

        fig.suptitle('Significant High-Level Features - Individual Differences', fontsize=16)

        for i, feature in enumerate(significant_features):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]

            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            clean_data = features_df[['subject_id', feature]].dropna()

            if len(clean_data) > 0:
                subjects = clean_data['subject_id'].unique()
                box_data = [clean_data[clean_data['subject_id'] == subject][feature].values
                            for subject in subjects]

                box_plot = ax.boxplot(box_data, labels=subjects, patch_artist=True)

                # æœ‰æ„ãªç‰¹å¾´é‡ãªã®ã§ç·‘è‰²
                for patch in box_plot['boxes']:
                    patch.set_facecolor('lightgreen')
                    patch.set_alpha(0.7)

                result = results_df[results_df['feature'] == feature].iloc[0]
                ax.set_title(f'{feature}\n(p={result["p_value"]:.4f}, Î·Â²={result["eta_squared"]:.4f})',
                             fontweight='bold', color='green')

            ax.set_xlabel('Subject ID')
            ax.set_ylabel(feature)
            ax.tick_params(axis='x', rotation=45)

        # æœªä½¿ç”¨è»¸ã‚’éè¡¨ç¤º
        for i in range(len(significant_features), n_rows * n_cols):
            row = i // n_cols
            col = i % n_cols
            if n_rows > 1:
                axes[row, col].set_visible(False)
            else:
                if col < len(axes):
                    axes[col].set_visible(False)

        plt.tight_layout()

        boxplot_path = os.path.join(output_dir, 'significant_features_boxplots.png')
        plt.savefig(boxplot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æœ‰æ„ç‰¹å¾´é‡ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {boxplot_path}")
        plt.close()


def save_high_level_results(results_df, features_df, output_dir='./high_level_anova_results'):
    """
    é«˜æ¬¡ç‰¹å¾´é‡åˆ†æçµæœã‚’ä¿å­˜
    """
    if results_df is not None:
        # ANOVAçµæœ
        results_path = os.path.join(output_dir, 'high_level_features_anova_results.csv')
        results_df.to_csv(results_path, index=False)
        print(f"ğŸ’¾ ANOVAçµæœã‚’ä¿å­˜: {results_path}")

        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        features_path = os.path.join(output_dir, 'high_level_features_data.csv')
        features_df.to_csv(features_path, index=False)
        print(f"ğŸ’¾ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {features_path}")

        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report_path = os.path.join(output_dir, 'high_level_features_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("é«˜æ¬¡ç‰¹å¾´é‡ã«ã‚ˆã‚‹è¢«é¨“è€…é–“å€‹äººå·®åˆ†æ - è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"åˆ†ææ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åˆ†æå¯¾è±¡ç‰¹å¾´é‡æ•°: {len(results_df)}\n\n")

            significant_count = len(results_df[results_df['significance'] == 'æœ‰æ„'])
            f.write("=" * 40 + "\n")
            f.write("åˆ†æçµæœæ¦‚è¦\n")
            f.write("=" * 40 + "\n")
            f.write(f"çµ±è¨ˆçš„æœ‰æ„ãªç‰¹å¾´é‡: {significant_count}/{len(results_df)} å€‹\n")
            f.write(f"æœ‰æ„ç‡: {significant_count / len(results_df) * 100:.1f}%\n\n")

            # è©³ç´°çµæœ
            sorted_results = results_df.sort_values('p_value')
            f.write("è©³ç´°çµæœï¼ˆpå€¤æ˜‡é †ï¼‰:\n")
            f.write("-" * 40 + "\n")
            for i, (_, result) in enumerate(sorted_results.iterrows(), 1):
                f.write(f"{i:2d}. {result['feature']}\n")
                f.write(f"    Fçµ±è¨ˆé‡: {result['f_statistic']:.4f}\n")
                f.write(f"    på€¤: {result['p_value']:.6f}\n")
                f.write(f"    åŠ¹æœé‡: {result['eta_squared']:.4f} ({result['effect_size']})\n")
                f.write(f"    åˆ¤å®š: {result['significance']}\n\n")

        print(f"ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")

def main_high_level_analysis(df, output_dir='./high_level_anova_results'):
    """
    é«˜æ¬¡ç‰¹å¾´é‡åˆ†æã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸš€ é«˜æ¬¡ç‰¹å¾´é‡ã«ã‚ˆã‚‹å€‹äººå·®åˆ†æé–‹å§‹")
    print("=" * 70)

    # 1. é«˜æ¬¡ç‰¹å¾´é‡æŠ½å‡º
    features_df = extract_high_level_features(df)

    if features_df.empty:
        print("âŒ é«˜æ¬¡ç‰¹å¾´é‡ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None

    # 2. ANOVAåˆ†æå®Ÿè¡Œ
    results_df, features_df = perform_high_level_anova(features_df, output_dir)

    if results_df is not None:
        # 3. å¯è¦–åŒ–
        create_high_level_visualization(results_df, features_df, output_dir)

        # 4. çµæœä¿å­˜
        save_high_level_results(results_df, features_df, output_dir)

        # 5. ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã¨ã®æ¯”è¼ƒåˆ†æ
        compare_with_low_level_features(results_df, output_dir)

        # 6. å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print(f"\nâœ… é«˜æ¬¡ç‰¹å¾´é‡åˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ çµæœã¯ '{output_dir}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        print(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - high_level_features_anova_results.csv: ANOVAåˆ†æçµæœ")
        print(f"  - high_level_features_data.csv: æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿")
        print(f"  - high_level_features_anova_plots.png: ç·åˆå¯è¦–åŒ–")
        print(f"  - significant_features_boxplots.png: æœ‰æ„ç‰¹å¾´é‡ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ")
        print(f"  - high_level_features_report.txt: è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ")
        print(f"  - comparison_with_low_level.png: ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã¨ã®æ¯”è¼ƒ")

        return results_df, features_df

    else:
        print("âŒ é«˜æ¬¡ç‰¹å¾´é‡åˆ†æã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, None


def compare_with_low_level_features(high_level_results, output_dir):
    """
    é«˜æ¬¡ç‰¹å¾´é‡ã¨ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã®åˆ†æçµæœã‚’æ¯”è¼ƒ
    """
    print(f"\nğŸ“Š ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã¨ã®æ¯”è¼ƒåˆ†æ...")

    # ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã®å…¸å‹çš„ãªçµæœï¼ˆã‚ãªãŸã®æ—¢å­˜åˆ†æã‹ã‚‰ï¼‰
    low_level_results = {
        'HandlePosX': {'p_value': 0.000000, 'eta_squared': 0.0019, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
        'HandlePosY': {'p_value': 0.000000, 'eta_squared': 0.0008, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
        'HandleVelX': {'p_value': 0.000000, 'eta_squared': 0.0012, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
        'HandleVelY': {'p_value': 0.000000, 'eta_squared': 0.0036, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
        'HandleAccX': {'p_value': 0.999917, 'eta_squared': 0.0000, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
        'HandleAccY': {'p_value': 0.999173, 'eta_squared': 0.0000, 'effect_size': 'ç„¡è¦–ã§ãã‚‹'},
    }

    # æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle('Low-Level vs High-Level Features: Individual Differences Analysis', fontsize=16)

    # åŠ¹æœé‡æ¯”è¼ƒ
    ax1 = axes[0]

    # ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡
    low_level_names = list(low_level_results.keys())
    low_level_eta2 = [low_level_results[name]['eta_squared'] for name in low_level_names]

    # é«˜æ¬¡ç‰¹å¾´é‡
    high_level_names = high_level_results['feature'].tolist()
    high_level_eta2 = high_level_results['eta_squared'].tolist()

    # ãƒ—ãƒ­ãƒƒãƒˆ
    x_low = range(len(low_level_names))
    x_high = range(len(low_level_names), len(low_level_names) + len(high_level_names))

    bars1 = ax1.bar(x_low, low_level_eta2, color='lightcoral', alpha=0.7,
                    label='Low-Level Features')
    bars2 = ax1.bar(x_high, high_level_eta2, color='lightgreen', alpha=0.7,
                    label='High-Level Features')

    ax1.set_xlabel('Features')
    ax1.set_ylabel('Effect Size (Î·Â²)')
    ax1.set_title('Effect Size Comparison: Low-Level vs High-Level Features')
    ax1.set_xticks(list(x_low) + list(x_high))
    ax1.set_xticklabels(low_level_names + high_level_names, rotation=45, ha='right')
    ax1.legend()

    # åŸºæº–ç·š
    ax1.axhline(y=0.01, color='gray', linestyle=':', alpha=0.5, label='Small Effect')
    ax1.axhline(y=0.06, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')
    ax1.axhline(y=0.14, color='red', linestyle=':', alpha=0.5, label='Large Effect')

    # æœ‰æ„ç‰¹å¾´é‡æ•°ã®æ¯”è¼ƒ
    ax2 = axes[1]

    # ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã®æœ‰æ„æ•°
    low_level_significant = sum(1 for result in low_level_results.values()
                                if result['p_value'] < 0.05)
    low_level_total = len(low_level_results)

    # é«˜æ¬¡ç‰¹å¾´é‡ã®æœ‰æ„æ•°
    high_level_significant = len(high_level_results[high_level_results['significance'] == 'æœ‰æ„'])
    high_level_total = len(high_level_results)

    categories = ['Low-Level\nFeatures', 'High-Level\nFeatures']
    significant_counts = [low_level_significant, high_level_significant]
    total_counts = [low_level_total, high_level_total]
    non_significant_counts = [total_counts[i] - significant_counts[i] for i in range(2)]

    # ç©ã¿ä¸Šã’ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    bars1 = ax2.bar(categories, significant_counts, color='green', alpha=0.7,
                    label='Significant (p<0.05)')
    bars2 = ax2.bar(categories, non_significant_counts, bottom=significant_counts,
                    color='red', alpha=0.7, label='Non-Significant (pâ‰¥0.05)')

    ax2.set_ylabel('Number of Features')
    ax2.set_title('Statistical Significance: Feature Type Comparison')
    ax2.legend()

    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¡¨ç¤º
    for i, (cat, sig, total) in enumerate(zip(categories, significant_counts, total_counts)):
        percentage = sig / total * 100 if total > 0 else 0
        ax2.text(i, sig / 2, f'{sig}/{total}\n({percentage:.1f}%)',
                 ha='center', va='center', fontweight='bold')

    plt.tight_layout()

    # ä¿å­˜
    comparison_path = os.path.join(output_dir, 'comparison_with_low_level.png')
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æ¯”è¼ƒåˆ†æãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {comparison_path}")
    plt.close()

    # æ¯”è¼ƒã‚µãƒãƒªãƒ¼å‡ºåŠ›
    print(f"\nğŸ“ˆ æ¯”è¼ƒåˆ†æçµæœ:")
    print(f"ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡: {low_level_significant}/{low_level_total} "
          f"({low_level_significant / low_level_total * 100:.1f}%) ãŒæœ‰æ„")
    print(f"é«˜æ¬¡ç‰¹å¾´é‡: {high_level_significant}/{high_level_total} "
          f"({high_level_significant / high_level_total * 100:.1f}%) ãŒæœ‰æ„")

    # åŠ¹æœé‡ã®æ”¹å–„åº¦
    low_level_max_eta2 = max(low_level_eta2) if low_level_eta2 else 0
    high_level_max_eta2 = max(high_level_eta2) if high_level_eta2 else 0

    print(f"\nåŠ¹æœé‡ã®æ”¹å–„:")
    print(f"ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡æœ€å¤§Î·Â²: {low_level_max_eta2:.4f}")
    print(f"é«˜æ¬¡ç‰¹å¾´é‡æœ€å¤§Î·Â²: {high_level_max_eta2:.4f}")
    if low_level_max_eta2 > 0:
        improvement_ratio = high_level_max_eta2 / low_level_max_eta2
        print(f"æ”¹å–„å€ç‡: {improvement_ratio:.2f}å€")


def run_complete_analysis(train_df):
    """
    æ—¢å­˜ã®ä½ãƒ¬ãƒ™ãƒ«åˆ†æã¨é«˜æ¬¡ç‰¹å¾´é‡åˆ†æã‚’ä¸¡æ–¹å®Ÿè¡Œ
    """
    print("ğŸ¯ å®Œå…¨å€‹äººå·®åˆ†æå®Ÿè¡Œ")
    print("=" * 70)

    # 1. æ—¢å­˜ã®ä½ãƒ¬ãƒ™ãƒ«ANOVAåˆ†æï¼ˆã‚ãªãŸã®æ—¢å­˜ã‚³ãƒ¼ãƒ‰ï¼‰
    print("\n1ï¸âƒ£ ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ANOVAåˆ†æ...")
    # ã“ã“ã§æ—¢å­˜ã®main_anova_analysis()ã‚’å®Ÿè¡Œ

    # 2. é«˜æ¬¡ç‰¹å¾´é‡ANOVAåˆ†æ
    print("\n2ï¸âƒ£ é«˜æ¬¡ç‰¹å¾´é‡ANOVAåˆ†æ...")
    high_level_results, high_level_features = main_high_level_analysis(
        train_df,
        output_dir='./anova_results/high_level_features'
    )

    # 3. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    create_integrated_report(high_level_results, high_level_features)

    return high_level_results, high_level_features


def create_integrated_report(high_level_results, high_level_features):
    """
    ä½ãƒ¬ãƒ™ãƒ«ï¼‹é«˜æ¬¡ç‰¹å¾´é‡ã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    """
    if high_level_results is None:
        return

    print(f"\nğŸ“ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ...")

    report_path = './anova_results/integrated_analysis_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Point-to-Pointã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹å€‹äººå·®åˆ†æ - çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("=" * 80 + "\n\n")

        f.write("ã€ç ”ç©¶ã®ç›®çš„ã€‘\n")
        f.write("ä½ãƒ¬ãƒ™ãƒ«é‹å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆä½ç½®ãƒ»é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ï¼‰ã¨é«˜æ¬¡ç‰¹å¾´é‡ï¼ˆè»Œé“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»\n")
        f.write("åˆ¶å¾¡ç‰¹æ€§ãªã©ï¼‰ã«ãŠã‘ã‚‹è¢«é¨“è€…é–“å€‹äººå·®ã®æ¯”è¼ƒåˆ†æ\n\n")

        f.write("ã€ä¸»è¦ãªç™ºè¦‹ã€‘\n")
        f.write("1. ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡: çµ±è¨ˆçš„æœ‰æ„ã ãŒåŠ¹æœé‡ã¯ç„¡è¦–ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ï¼ˆÎ·Â² < 0.004ï¼‰\n")
        f.write("2. é«˜æ¬¡ç‰¹å¾´é‡: ã‚ˆã‚Šå¤§ããªå€‹äººå·®ã‚’æ¤œå‡ºï¼ˆè©³ç´°ã¯ä»¥ä¸‹å‚ç…§ï¼‰\n")
        f.write("3. Point-to-Pointã‚¿ã‚¹ã‚¯ã§ã¯åˆ¶å¾¡ãƒ¬ãƒ™ãƒ«ãŒä½ã„ã»ã©å€‹äººå·®ãŒæ¸›å°‘\n\n")

        # é«˜æ¬¡ç‰¹å¾´é‡ã®çµæœè©³ç´°
        significant_hl = high_level_results[high_level_results['significance'] == 'æœ‰æ„']
        f.write("ã€é«˜æ¬¡ç‰¹å¾´é‡ã§ã®æœ‰æ„ãªå€‹äººå·®ã€‘\n")
        if len(significant_hl) > 0:
            for _, result in significant_hl.iterrows():
                f.write(f"â€¢ {result['feature']}: ")
                f.write(f"p={result['p_value']:.6f}, Î·Â²={result['eta_squared']:.4f}\n")
        else:
            f.write("çµ±è¨ˆçš„æœ‰æ„ãªå€‹äººå·®ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n")

        f.write(f"\nã€çµè«–ã€‘\n")
        f.write("Point-to-Pointã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ä½ãƒ¬ãƒ™ãƒ«ã®é‹å‹•ãƒ‡ãƒ¼ã‚¿ã§ã¯å®Ÿç”¨çš„ãª\n")
        f.write("å€‹äººå·®ã¯å­˜åœ¨ã—ãªã„ãŒã€é«˜æ¬¡ã®é‹å‹•åˆ¶å¾¡ç‰¹å¾´ã§ã¯å€‹äººå·®ãŒå­˜åœ¨ã™ã‚‹\n")
        f.write("å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ã“ã®çµæœã¯é‹å‹•åˆ¶å¾¡ã®éšå±¤çš„æ€§è³ªã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚\n")

    print(f"ğŸ“„ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ----------------------------------------
def load_rawdata(data_dir: str) -> pd.DataFrame | None:
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ã¦ã®CSVã‚’èª­ã¿è¾¼ã¿ã€ä¸€ã¤ã®DataFrameã«çµåˆã™ã‚‹ã€‚"""
    all_files = glob.glob(os.path.join(data_dir, "*.csv"))
    if not all_files:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{data_dir}' ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    df_list = []
    for filename in all_files:
        try:
            basename = os.path.basename(filename)
            parts = basename.replace('.csv', '').split('_')
            subject_id = parts[0]
            block_num = int(parts[1].replace('Block', ''))

            df = pd.read_csv(filename)
            # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ã—ã€æƒ…å ±ã‚’ä»˜ä¸
            df = df.rename(columns={'SubjectId': 'subject_id', 'CurrentTrial': 'trial_num', 'Block': 'block'})
            df['subject_id'] = subject_id
            df['block'] = block_num
            df_list.append(df)
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    if not df_list:
        return None
    return pd.concat(df_list, ignore_index=True)


# ----------------------------------------
# 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ----------------------------------------
def calculate_jerk(acceleration: np.ndarray, dt: float) -> float:
    if len(acceleration) < 2: return np.nan
    jerk = np.gradient(acceleration, dt, axis=0)
    return np.mean(np.linalg.norm(jerk, axis=1))


def calculate_path_efficiency(trajectory: np.ndarray) -> float:
    start_point, end_point = trajectory[0], trajectory[-1]
    straight_distance = np.linalg.norm(end_point - start_point)
    total_distance = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
    if total_distance < 1e-6: return 1.0
    return straight_distance / total_distance


def calculate_approach_angle(trajectory: np.ndarray, n_points: int = 5) -> float:
    if len(trajectory) < n_points: return np.nan
    vec = trajectory[-1] - trajectory[-n_points]
    return np.arctan2(vec[1], vec[0]) * 180 / np.pi


def calculate_sparc(velocity: np.ndarray, dt: float) -> float:
    if len(velocity) < 2: return np.nan
    velocity_magnitude = np.linalg.norm(velocity, axis=1)
    return 1.0 / (np.std(velocity_magnitude) + 1e-6)


# ----------------------------------------
# 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DataFrameã®ä½œæˆ
# ----------------------------------------
def create_performance_dataframe(raw_df: pd.DataFrame, target_sequence_length: int = 100) -> pd.DataFrame:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€è©¦è¡Œã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DataFrameã‚’ä½œæˆã™ã‚‹ã€‚"""
    # 'TRIAL_RUNNING'çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
    df = raw_df[raw_df['TrialState'] == 'TRIAL_RUNNING'].copy()
    if df.empty:
        print("ã‚¨ãƒ©ãƒ¼: 'TRIAL_RUNNING'çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # --- è©¦è¡Œé–“ã®ã°ã‚‰ã¤ãã‚’å…ˆã«è¨ˆç®— ---
    all_trial_variability = {}
    for subject_id, subject_df in df.groupby('subject_id'):
        resampled_trajs = []
        for _, trial_df in subject_df.groupby('trial_num'):
            traj_points = trial_df[['HandlePosX', 'HandlePosY']].values
            original_indices = np.linspace(0, 1, num=len(traj_points))
            target_indices = np.linspace(0, 1, num=target_sequence_length)
            resampled_x = np.interp(target_indices, original_indices, traj_points[:, 0])
            resampled_y = np.interp(target_indices, original_indices, traj_points[:, 1])
            resampled_trajs.append(np.column_stack([resampled_x, resampled_y]))
        if resampled_trajs:
            variability = np.mean(np.std(np.array(resampled_trajs), axis=0))
            all_trial_variability[subject_id] = variability

    # --- å„è©¦è¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¨ˆç®— ---
    def calculate_metrics_for_trial(group: pd.DataFrame) -> pd.Series:
        trajectory = group[['HandlePosX', 'HandlePosY']].values
        velocity = group[['HandleVelX', 'HandleVelY']].values
        acceleration = group[['HandleAccX', 'HandleAccY']].values
        target_end_pos = group[['TargetEndPosX', 'TargetEndPosY']].iloc[-1].values
        time_stamps = group['Timestamp'].values

        dt = np.mean(np.diff(time_stamps))
        if np.isnan(dt) or dt == 0: dt = 0.01

        return pd.Series({
            'block': group['block'].iloc[0],
            'trial_time': time_stamps[-1] - time_stamps[0],
            'trial_error': np.linalg.norm(trajectory[-1] - target_end_pos),
            'jerk': calculate_jerk(acceleration, dt),
            'path_efficiency': calculate_path_efficiency(trajectory),
            'approach_angle': calculate_approach_angle(trajectory),
            'sparc': calculate_sparc(velocity, dt),
            'trial_variability': all_trial_variability.get(group.name[0], np.nan)
        })

    performance_df = df.groupby(['subject_id', 'trial_num']).apply(calculate_metrics_for_trial).reset_index()
    return performance_df


# ----------------------------------------
# 4. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã¨ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
# ----------------------------------------
def calculate_skill_scores(perf_df: pd.DataFrame) -> pd.Series:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™DFã‹ã‚‰ã€è¢«é¨“è€…ã”ã¨ã®ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    # è¢«é¨“è€…ã”ã¨ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®å¹³å‡ã‚’è¨ˆç®—
    subject_metrics = perf_df.groupby('subject_id').mean()

    # Z-scoreåŒ–
    metrics_zscored = subject_metrics.drop(columns=['block', 'trial_num']).apply(lambda x: (x - x.mean()) / x.std())

    # æ–¹å‘æ€§ã®çµ±ä¸€ (å€¤ãŒä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã®ç¬¦å·ã‚’åè»¢)
    metrics_zscored['trial_time'] *= -1
    metrics_zscored['trial_error'] *= -1
    metrics_zscored['jerk'] *= -1
    metrics_zscored['trial_variability'] *= -1
    # path_efficiency, sparc, approach_angleã¯é«˜ã„æ–¹ãŒè‰¯ã„ã®ã§ãã®ã¾ã¾

    # ç·åˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
    skill_scores = metrics_zscored.sum(axis=1)
    return skill_scores


def classify_by_median_split(skill_scores: pd.Series) -> pd.Series:
    """ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’ä¸­å¤®å€¤ã§ã€Œç†Ÿé”è€…(1)ã€ã¨ã€Œåˆå¿ƒè€…(0)ã€ã«åˆ†é¡ã™ã‚‹ã€‚"""
    median_score = skill_scores.median()
    return (skill_scores > median_score).astype(int).rename('is_expert')

def add_diffs(group):
    """å„ç¨®ç‰©ç†é‡ã‚’å·®åˆ†è¡¨ç¤ºå¤‰æ›"""
    pos_data = group[['HandlePosX', 'HandlePosY']].values
    pos_diffs = np.diff(pos_data, axis=0, prepend=pos_data[0:1])

    vel_data = group[['HandleVelX', 'HandleVelY']].values
    vel_diffs = np.diff(vel_data, axis=0, prepend=vel_data[0:1])

    acc_data = group[['HandleAccX', 'HandleAccY']].values
    acc_diffs = np.diff(acc_data, axis=0, prepend=acc_data[0:1])

    group = group.copy()
    group['HandlePosDiffX'] = pos_diffs[:, 0]
    group['HandlePosDiffY'] = pos_diffs[:, 1]

    group['HandleVelDiffX'] = vel_diffs[:, 0]
    group['HandleVelDiffY'] = vel_diffs[:, 1]

    group['HandleAccDiffX'] = acc_diffs[:, 0]
    group['HandleAccDiffY'] = acc_diffs[:, 1]

    return group


def prepare_scaler(feature_df:pd.DataFrame):
    """ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã”ã¨ã«å€‹åˆ¥ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""

    # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ç¢ºèª
    available_cols = feature_df.columns.tolist()
    print(f"Available columns: {available_cols}")

    scalers = {}

    # ä½ç½®
    position_cols = ['HandlePosX', 'HandlePosY']
    if all(col in available_cols for col in position_cols):
        scalers['position'] = StandardScaler().fit(feature_df[position_cols])
        print(f"Position scaler created:{position_cols}")

    # ä½ç½®å·®åˆ†
    position_diff_cols = ['HandlePosDiffX', 'HandlePosDiffY']
    if all(col in available_cols for col in position_diff_cols):
        scalers['position_diff'] = StandardScaler().fit(feature_df[position_diff_cols])
        print(f"Position diff scaler created:{position_diff_cols}")

    # é€Ÿåº¦
    velocity_cols = ['HandleVelX', 'HandleVelY']
    if all(col in available_cols for col in velocity_cols):
        scalers['velocity'] = StandardScaler().fit(feature_df[velocity_cols])
        print(f"Velocity scaler created:{velocity_cols}")

    # é€Ÿåº¦å·®åˆ†
    velocity_diff_cols = ['HandleVelDiffX', 'HandleVelDiffY']
    if all(col in available_cols for col in velocity_diff_cols):
        scalers['velocity_diff'] = StandardScaler().fit(feature_df[velocity_diff_cols])
        print(f"Velocity diff scaler created:{velocity_diff_cols}")

    # åŠ é€Ÿåº¦
    acceleration_cols = ['HandleAccX', 'HandleAccY']
    if all(col in available_cols for col in acceleration_cols):
        scalers['acceleration'] = StandardScaler().fit(feature_df[acceleration_cols])
        print(f"acceleration scaler created:{acceleration_cols}")

    # åŠ é€Ÿåº¦å·®åˆ†
    acceleration_diff_cols = ['HandleAccDiffX', 'HandleAccDiffY']
    if all(col in available_cols for col in acceleration_diff_cols):
        scalers['acceleration_diff'] = StandardScaler().fit(feature_df[acceleration_diff_cols])
        print(f"acceleration diff scaler created:{acceleration_diff_cols}")

     # å°†æ¥çš„ãªã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    jerk_cols = ['JerkX', 'JerkY']
    if all(col in available_cols for col in jerk_cols):
        scalers['jerk'] = StandardScaler().fit(feature_df[jerk_cols])
        print(f"Jerk scaler created: {jerk_cols}")

    return scalers


def analyze_style_hierarchy(df):
    """ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤ã®å®šé‡çš„åˆ†æ"""

    # è¢«é¨“è€…ã”ã¨ã®å„ç‰©ç†é‡ã®ç‰¹å¾´ã‚’æŠ½å‡º
    subjects = df['subject_id'].unique()

    # å„ç‰©ç†é‡ã§ã®è¢«é¨“è€…é–“åˆ†é›¢åº¦ã‚’è¨ˆç®—
    separation_scores = {}

    feature_groups = {
        'Position': ['HandlePosDiffX', 'HandlePosDiffY'],
        'Velocity': ['HandleVelDiffX', 'HandleVelDiffY'],
        'Acceleration': ['HandleAccDiffX', 'HandleAccDiffY']
    }

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            # è¢«é¨“è€…ã”ã¨ã®å¹³å‡ã‚’è¨ˆç®—
            subject_means = []
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].mean().values
                subject_means.append(subject_data)

            subject_means = np.array(subject_means)

            # è¢«é¨“è€…é–“åˆ†æ•£ / è¢«é¨“è€…å†…åˆ†æ•£
            between_var = np.var(subject_means.mean(axis=1))

            within_vars = []
            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 1:
                    within_var = np.var(subject_data.mean(axis=1))
                    within_vars.append(within_var)

            avg_within_var = np.mean(within_vars) if within_vars else 0.01
            separation_score = between_var / (avg_within_var + 1e-8)

            separation_scores[group_name] = separation_score

            print(f"{group_name}:")
            print(f"  è¢«é¨“è€…é–“åˆ†æ•£: {between_var:.6f}")
            print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.6f}")
            print(f"  åˆ†é›¢ã‚¹ã‚³ã‚¢: {separation_score:.4f}")

    return separation_scores


def analyze_style_hierarchy(df):
    """ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤ã®å®šé‡çš„åˆ†æ"""
    print("\n" + "=" * 50)
    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æé–‹å§‹")
    print("=" * 50)

    # è¢«é¨“è€…ã”ã¨ã®å„ç‰©ç†é‡ã®ç‰¹å¾´ã‚’æŠ½å‡º
    subjects = df['subject_id'].unique()
    print(f"åˆ†æå¯¾è±¡è¢«é¨“è€…æ•°: {len(subjects)}")
    print(f"è¢«é¨“è€…ID: {list(subjects)}")

    # å„ç‰©ç†é‡ã§ã®è¢«é¨“è€…é–“åˆ†é›¢åº¦ã‚’è¨ˆç®—
    separation_scores = {}

    feature_groups = {
        'Position': ['HandlePosX', 'HandlePosY'],
        'Velocity': ['HandleVelX', 'HandleVelY'],
        'Acceleration': ['HandleAccX', 'HandleAccY']
    }

    print(f"\nåˆ†æå¯¾è±¡ç‰©ç†é‡ã‚°ãƒ«ãƒ¼ãƒ—:")
    for group_name, features in feature_groups.items():
        available = all(f in df.columns for f in features)
        print(f"  {group_name}: {features} -> {'âœ…' if available else 'âŒ'}")

    for group_name, features in feature_groups.items():
        if all(f in df.columns for f in features):
            # è¢«é¨“è€…ã”ã¨ã®å¹³å‡ã‚’è¨ˆç®—
            subject_means = []
            subject_within_vars = []

            for subject in subjects:
                subject_data = df[df['subject_id'] == subject][features].values
                if len(subject_data) > 0:
                    subject_mean = np.mean(subject_data, axis=0)
                    subject_means.append(subject_mean)

                    # è¢«é¨“è€…å†…åˆ†æ•£ã‚‚è¨ˆç®—
                    if len(subject_data) > 1:
                        within_var = np.var(subject_data, axis=0).mean()
                        subject_within_vars.append(within_var)

            if len(subject_means) > 1:
                subject_means = np.array(subject_means)

                # è¢«é¨“è€…é–“åˆ†æ•£ï¼šå„è¢«é¨“è€…ã®å¹³å‡å€¤ã®åˆ†æ•£
                between_var = np.var(subject_means, axis=0).mean()

                # å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£
                avg_within_var = np.mean(subject_within_vars) if subject_within_vars else 0.01

                # åˆ†é›¢ã‚¹ã‚³ã‚¢ï¼šè¢«é¨“è€…é–“åˆ†æ•£ / è¢«é¨“è€…å†…åˆ†æ•£
                separation_score = between_var / (avg_within_var + 1e-8)

                separation_scores[group_name] = separation_score

                print(f"\n{group_name}:")
                print(f"  è¢«é¨“è€…é–“åˆ†æ•£: {between_var:.6f}")
                print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.6f}")
                print(f"  åˆ†é›¢ã‚¹ã‚³ã‚¢: {separation_score:.4f}")

                # åˆ†é›¢åº¦ã®è©•ä¾¡
                if separation_score > 2.0:
                    print(f"  ğŸŸ¢ é«˜ã„åˆ†é›¢åº¦ - ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ãŒè±Šå¯Œ")
                elif separation_score > 1.0:
                    print(f"  ğŸŸ¡ ä¸­ç¨‹åº¦ã®åˆ†é›¢åº¦ - ä¸€éƒ¨ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ã‚ã‚Š")
                else:
                    print(f"  ğŸ”´ ä½ã„åˆ†é›¢åº¦ - ã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ãŒå°‘ãªã„")

    print(f"\n" + "=" * 50)
    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æçµæœ")
    print("=" * 50)

    # åˆ†é›¢ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    sorted_scores = sorted(separation_scores.items(), key=lambda x: x[1], reverse=True)
    for i, (group, score) in enumerate(sorted_scores, 1):
        print(f"{i}. {group}: {score:.4f}")

    if sorted_scores:
        best_group, best_score = sorted_scores[0]
        print(f"\nğŸ¯ æœ€ã‚‚æœ‰åŠ¹ãªã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±: {best_group} (åˆ†é›¢ã‚¹ã‚³ã‚¢: {best_score:.4f})")

        if best_score > 2.0:
            print("âœ… è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™")
        elif best_score > 1.0:
            print("âš ï¸  è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯å›°é›£ã§ã™ãŒå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        else:
            print("âŒ è¢«é¨“è€…ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯éå¸¸ã«å›°é›£ã§ã™")

    return separation_scores

def main():
    TARGET_SEQ_LEN = 100
    RAWDATA_DIR = '../../../data/RawDatas/'
    PROCESSED_DATA_DIR = 'PredictiveLatentSpaceNavigationModel/DataPreprocess/ForGeneralizedCoordinate'
    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

    # --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Ÿè¡Œ ---
    # 1. ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    raw_df = load_rawdata(RAWDATA_DIR)
    if raw_df is None: return

    # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
    performance_df = create_performance_dataframe(raw_df, target_sequence_length=TARGET_SEQ_LEN)
    if performance_df.empty: return

    # 3. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    skill_scores = calculate_skill_scores(performance_df)

    # 4. ç†Ÿé”è€…/åˆå¿ƒè€…ã®åˆ†é¡
    expertise_labels = classify_by_median_split(skill_scores)

    # 5. ç†Ÿç·´åº¦ã®æƒ…å ±ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    filtered_df = raw_df[raw_df['TrialState'] == 'TRIAL_RUNNING'].copy()
    filtered_df['is_expert'] = filtered_df['subject_id'].map(expertise_labels)

    # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è»Œé“ãƒ‡ãƒ¼ã‚¿ã«çµåˆ
    # performance_dfã®åˆ—åã«prefixã‚’è¿½åŠ ã—ã¦ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆã‚’é¿ã‘ã‚‹
    performance_df_renamed = performance_df.add_prefix('perf_').rename(columns={'perf_subject_id': 'subject_id', 'perf_trial_num': 'trial_num'})
    master_df = filtered_df.merge(
        performance_df_renamed,
        on=['subject_id', 'trial_num'],
        how='left'
    )

    # 7. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    subject_ids = master_df['subject_id'].unique()
    train_subjects, test_subjects = train_test_split(subject_ids, test_size=0.2, random_state=42)

    train_df = master_df[master_df['subject_id'].isin(train_subjects)]
    test_df = master_df[master_df['subject_id'].isin(test_subjects)]

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿è¢«é¨“è€…æ•°: {len(train_subjects)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¢«é¨“è€…æ•°: {len(test_subjects)}")

    # 8. å·®åˆ†è¡¨ç¤ºã®è¨ˆç®—
    train_df = train_df.groupby(['subject_id', 'trial_num'], group_keys=False).apply(add_diffs)
    test_df = test_df.groupby(['subject_id', 'trial_num'], group_keys=False).apply(add_diffs)
    print("å·®åˆ†è¡¨ç¤ºã®è¨ˆç®—å®Œäº† âœ… ")

    print("ã‚¹ã‚¿ã‚¤ãƒ«éšå±¤åˆ†æå®Ÿè¡Œä¸­...")
    # separation_score = analyze_style_hierarchy(master_df)
    # comprehensive_ptp_analysis(master_df)
    resolve_contradiction_analysis(train_df)
    # print(train_df.columns)

    # 8. ã‚¹ã‚±ãƒ¼ãƒ©ã®ç”¨æ„
    feature_cols = [
        'HandlePosX', 'HandlePosY',
        'HandleVelX', 'HandleVelY',
        'HandleAccX', 'HandleAccY',
    ]

    scalers = prepare_scaler(train_df[feature_cols])
    print("StandardScalerã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¾ã—ãŸã€‚âœ…")

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    scaled_train_df = train_df.copy()

    # å„ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    if 'position' in scalers:
        pos_cols = ['HandlePosX', 'HandlePosY']
        scaled_train_df[pos_cols] = scalers['position'].transform(train_df[pos_cols])

    if 'velocity' in scalers:
        vel_cols = ['HandleVelX', 'HandleVelY']
        scaled_train_df[vel_cols] = scalers['velocity'].transform(train_df[vel_cols])

    if 'acceleration' in scalers:
        acc_cols = ['HandleAccX', 'HandleAccY']
        scaled_train_df[acc_cols] = scalers['acceleration'].transform(train_df[acc_cols])

    # # å·®åˆ†ã‚‚ANOVAè§£æå¯¾è±¡ã«è¿½åŠ 
    # diff_feature_cols = [
    #     'HandlePosDiffX', 'HandlePosDiffY',
    #     'HandleVelDiffX', 'HandleVelDiffY',
    #     'HandleAccDiffX', 'HandleAccDiffY'
    # ]

    # ANOVAã§å€‹äººå·®ã®åˆ†æ
    ANOVA_RESULT_PATH = "PredictiveLatentSpaceNavigationModel/DataPreprocess/AnovaResults/"
    # 1. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®åˆ†æ
    print("=" * 60)
    print("ğŸ” ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ãƒ‡ãƒ¼ã‚¿ã§ã®ANOVAåˆ†æ")
    print("=" * 60)
    anova_cols_original = ['subject_id'] + feature_cols
    anova_data_original = train_df[anova_cols_original].copy()
    anova_result_original = main_anova_analysis(
        anova_data_original,
        os.path.join(ANOVA_RESULT_PATH, 'original_scale')
    )

    # 2. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®åˆ†æ
    print("=" * 60)
    print("ğŸ” ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œãƒ‡ãƒ¼ã‚¿ã§ã®ANOVAåˆ†æ")
    print("=" * 60)
    anova_cols_scaled = ['subject_id'] + feature_cols
    anova_data_scaled = scaled_train_df[anova_cols_scaled].copy()
    anova_result_scaled = main_anova_analysis(
        anova_data_scaled,
        os.path.join(ANOVA_RESULT_PATH, 'scaled')
    )

    # 3. æ¯”è¼ƒåˆ†æ
    compare_scaling_effects(anova_result_original, anova_result_scaled, ANOVA_RESULT_PATH)

    # 4. é«˜æ¬¡ç‰¹å¾´é‡ã®ANOVAè§£æ
    high_level_results, high_level_features = main_high_level_analysis(
        train_df,
        os.path.join(ANOVA_RESULT_PATH, 'HighLevelFeatures')
    )

if __name__ == "__main__":
    main()
