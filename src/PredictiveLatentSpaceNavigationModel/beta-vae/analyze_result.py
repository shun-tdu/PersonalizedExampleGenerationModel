import marimo

__generated_with = "0.14.16"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    import sqlite3 
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import json
    from pathlib import Path
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    from scipy import stats
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    import warnings
    warnings.filterwarnings('ignore')

    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
    sns.set_palette("husl")
    # plt.style.use('seaborn-v0_8')

    mo.md("# ğŸ§¬ Î²-VAEå®Ÿé¨“çµæœ åŒ…æ‹¬çš„åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    return go, json, make_subplots, mo, np, pd, plt, px, sqlite3, stats


@app.cell
def _(mo):
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹è¨­å®š
    db_path_input = mo.ui.text(
        value="PredictiveLatentSpaceNavigationModel/experiments.db",
        label="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹:",
        placeholder="experiments.dbã®ãƒ‘ã‚¹ã‚’å…¥åŠ›"
    )

    analysis_mode = mo.ui.dropdown(
        options={
            "ğŸ” åŸºæœ¬åˆ†æ": "basic",
            "ğŸ“Š è©³ç´°çµ±è¨ˆ": "detailed", 
            "ğŸ§  ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–": "hyperopt",
            "ğŸ¯ æ½œåœ¨ç©ºé–“åˆ†æ": "latent",
            "ğŸ“ˆ å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ": "training"
        },
        value="ğŸ” åŸºæœ¬åˆ†æ",
        label="åˆ†æãƒ¢ãƒ¼ãƒ‰"
    )

    mo.vstack([
        mo.md("## âš™ï¸ åˆ†æè¨­å®š"),
        db_path_input,
        analysis_mode
    ])
    return analysis_mode, db_path_input


@app.cell
def _(db_path_input, json, mo, pd, sqlite3):
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    def load_experiment_data(db_path):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ¦‚è¦ã‚’è¿”ã™é–¢æ•°"""
        try:
            # é–¢æ•°å†…ã§connã‚’å®šç¾©ã€‚ã“ã‚Œã¯ã“ã®é–¢æ•°ã ã‘ã®ã‚‚ã®ã«ãªã‚‹ã€‚
            conn = sqlite3.connect(db_path)

            # å…¨å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å–å¾—
            df_experiments = pd.read_sql_query("""
                SELECT * FROM experiments 
                WHERE status = 'completed' AND reconstruction_mse IS NOT NULL
                ORDER BY created_at DESC
            """, conn)

            # JSONç›¸é–¢ãƒ‡ãƒ¼ã‚¿ã®å±•é–‹
            def parse_correlations(corr_str):
                if pd.isna(corr_str) or not corr_str:
                    return {}
                try:
                    return json.loads(corr_str)
                except:
                    return {}

            df_experiments['correlations_parsed'] = df_experiments['skill_correlations'].apply(parse_correlations)

            # å®Ÿé¨“çµ±è¨ˆ
            n_experiments = len(df_experiments)
            date_range = pd.to_datetime(df_experiments['start_time']).dt.date
            experiment_period = f"{date_range.min()} ~ {date_range.max()}"

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
            best_mse = df_experiments['reconstruction_mse'].min()
            worst_mse = df_experiments['reconstruction_mse'].max()
            avg_mse = df_experiments['reconstruction_mse'].mean()

            conn.close()

            # çµæœã‚’Markdownã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ä½œæˆ
            status_content = mo.md(f"""
            ## ğŸ“Š å®Ÿé¨“æ¦‚è¦
            - **ç·å®Ÿé¨“æ•°**: {n_experiments}
            - **å®Ÿé¨“æœŸé–“**: {experiment_period}
            - **æœ€è‰¯MSE**: {best_mse:.2e}
            - **æœ€æ‚ªMSE**: {worst_mse:.2e} 
            - **å¹³å‡MSE**: {avg_mse:.2e}
            - **æ€§èƒ½æ”¹å–„**: {((worst_mse - best_mse) / worst_mse * 100):.1f}%
            """)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨è¡¨ç¤ºå†…å®¹ã‚’ä¸¡æ–¹è¿”ã™
            return df_experiments, status_content

        except Exception as e:
            df_experiments = pd.DataFrame()
            status_content = mo.md(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return df_experiments, status_content

    df_experiments, status_content = load_experiment_data(db_path_input.value)
    status_content
    return (df_experiments,)


@app.cell
def _(analysis_mode, df_experiments, mo, pd, plt):
    # åŸºæœ¬åˆ†æ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    if analysis_mode.value == "basic" and not df_experiments.empty:

        # Matplotlibã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šè»½é‡ãªå›³ã‚’ä½œæˆ
        import io
        import base64

        # 1. MSEåˆ†å¸ƒï¼ˆmatplotlibç‰ˆï¼‰
        fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax1.hist(df_experiments['reconstruction_mse'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('Reconstruction MSE')
        ax1.set_ylabel('Num Experiments')
        ax1.set_title('Reconstruction MSE Distribution')
        ax1.grid(True, alpha=0.3)

        # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        ax1.boxplot(df_experiments['reconstruction_mse'], vert=False, patch_artist=True)

        # 2. å®Ÿé¨“ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆmatplotlibç‰ˆï¼‰
        df_time = df_experiments.copy()
        df_time['start_time'] = pd.to_datetime(df_time['start_time'])
        df_time = df_time.sort_values('start_time')
        df_time['experiment_order'] = range(1, len(df_time) + 1)

        ax2.plot(df_time['experiment_order'], df_time['reconstruction_mse'], 
                'bo-', linewidth=2, markersize=6, label='MSE Trend')

        # ç§»å‹•å¹³å‡ã‚’è¿½åŠ 
        if len(df_time) >= 3:
            df_time['mse_ma'] = df_time['reconstruction_mse'].rolling(window=3).mean()
            ax2.plot(df_time['experiment_order'], df_time['mse_ma'], 
                    'r--', linewidth=2, label='Moving Average')

        ax2.set_xlabel('Experiment Index')
        ax2.set_ylabel('Reconstruction MSE')
        ax2.set_title('MSE Progression Over Experiments')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è»½é‡åŒ–
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', dpi=100, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.read()).decode()
        plt.close()

        # 3. è¨­å®šåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        param_performance = []
        for _, row in df_experiments.iterrows():
            param_performance.append({
                'experiment_id': row['id'],
                'config_file': row['config_file'],
                'lr': row['lr'],
                'beta': row['beta'],
                'batch_size': row['batch_size'],
                'latent_total': row['style_latent_dim'] + row['skill_latent_dim'],
                'reconstruction_mse': row['reconstruction_mse'],
                'training_time': (pd.to_datetime(row['end_time']) - pd.to_datetime(row['start_time'])).total_seconds() / 3600
            })

        df_perf = pd.DataFrame(param_performance)

        # ãƒˆãƒƒãƒ—3ã¨ãƒ¯ãƒ¼ã‚¹ãƒˆ3
        top3 = df_perf.nsmallest(3, 'reconstruction_mse')[['config_file', 'reconstruction_mse', 'lr', 'beta']]
        worst3 = df_perf.nlargest(3, 'reconstruction_mse')[['config_file', 'reconstruction_mse', 'lr', 'beta']]

        # è»½é‡åŒ–ã•ã‚ŒãŸå‡ºåŠ›
        basic_content = mo.vstack([
            mo.md("## ğŸ” åŸºæœ¬åˆ†æçµæœ"),
            mo.image(src=f"data:image/png;base64,{img_base64}"),
            mo.md("### ğŸ† ãƒˆãƒƒãƒ—3å®Ÿé¨“"),
            mo.ui.table(top3.round(6)),
            mo.md("### ğŸ“‰ æ”¹å–„ãŒå¿…è¦ãªå®Ÿé¨“"),
            mo.ui.table(worst3.round(6)),
            mo.md(f"""
            ### ğŸ“Š ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
            - **å®Ÿé¨“æ•°**: {len(df_experiments)}
            - **æœ€è‰¯MSE**: {df_experiments['reconstruction_mse'].min():.2e}
            - **æœ€æ‚ªMSE**: {df_experiments['reconstruction_mse'].max():.2e}
            - **å¹³å‡MSE**: {df_experiments['reconstruction_mse'].mean():.2e}
            - **æ¨™æº–åå·®**: {df_experiments['reconstruction_mse'].std():.2e}
            """)
        ])
    else:
        basic_content = mo.md("åŸºæœ¬åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")

    basic_content
    return


@app.cell
def _(analysis_mode, df_experiments, mo, np, pd, plt, stats):
    def _():
        # è©³ç´°çµ±è¨ˆåˆ†æ
        if analysis_mode.value == "detailed" and not df_experiments.empty:

            # 1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ±è¨ˆçš„å½±éŸ¿åˆ†æ
            params = ['lr', 'beta', 'batch_size', 'hidden_dim', 'style_latent_dim', 'skill_latent_dim']
            param_correlations = {}

            for param in params:
                if param in df_experiments.columns:
                    corr, p_val = stats.pearsonr(df_experiments[param], df_experiments['reconstruction_mse'])
                    param_correlations[param] = {'correlation': corr, 'p_value': p_val, 'significant': p_val < 0.05}

            # matplotlibã§è»½é‡ãªç›¸é–¢å›³ã‚’ä½œæˆ
            import io
            import base64

            if param_correlations:
                corr_df = pd.DataFrame(param_correlations).T
                corr_df = corr_df.sort_values('correlation', key=abs, ascending=False)

                fig, ax = plt.subplots(figsize=(10, 4))
                colors = ['red' if sig else 'lightblue' for sig in corr_df['significant']]
                bars = ax.bar(corr_df.index, corr_df['correlation'], color=colors, alpha=0.7)
                ax.set_title('ğŸ”— ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨MSEã®ç›¸é–¢')
                ax.set_ylabel('ç›¸é–¢ä¿‚æ•°')
                ax.grid(True, alpha=0.3)
                ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)

                # å‡¡ä¾‹
                import matplotlib.patches as mpatches
                sig_patch = mpatches.Patch(color='red', label='æœ‰æ„ (p<0.05)')
                non_sig_patch = mpatches.Patch(color='lightblue', label='éæœ‰æ„')
                ax.legend(handles=[sig_patch, non_sig_patch])

                plt.xticks(rotation=45)
                plt.tight_layout()

                # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=100, bbox_inches='tight')
                img_buffer.seek(0)
                img_base64 = base64.b64encode(img_buffer.read()).decode()
                plt.close()

                correlation_image = mo.image(src=f"data:image/png;base64,{img_base64}")
            else:
                correlation_image = mo.md("ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # çµ±è¨ˆã‚µãƒãƒªãƒ¼
            numeric_cols = ['reconstruction_mse', 'final_total_loss', 'best_val_loss'] + params
            existing_cols = [col for col in numeric_cols if col in df_experiments.columns]
            stats_summary = df_experiments[existing_cols].describe()

            # ç•°å¸¸å€¤æ¤œå‡º
            from scipy import stats as scipy_stats
            z_scores = np.abs(scipy_stats.zscore(df_experiments['reconstruction_mse']))
            outliers = df_experiments[z_scores > 2]['config_file'].tolist()

            detailed_content = mo.vstack([
                mo.md("## ğŸ“Š è©³ç´°çµ±è¨ˆåˆ†æ"),
                correlation_image,
                mo.md("### ğŸ“ˆ ç›¸é–¢åˆ†æçµæœ"),
                mo.ui.table(corr_df.round(4)) if param_correlations else mo.md("ãƒ‡ãƒ¼ã‚¿ãªã—"),
                mo.md("### ğŸ“‹ çµ±è¨ˆã‚µãƒãƒªãƒ¼"),
                mo.ui.table(stats_summary.round(6)),
                mo.md(f"### âš ï¸ ç•°å¸¸å€¤æ¤œå‡º (Z-score > 2): {len(outliers)}ä»¶"),
                mo.md(f"ç•°å¸¸å€¤: {', '.join(outliers) if outliers else 'ãªã—'}")
            ])
        else:
            detailed_content = mo.md("è©³ç´°çµ±è¨ˆãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return detailed_content


    _()
    return


@app.cell
def _(analysis_mode, df_experiments, go, make_subplots, mo, np, pd):
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–åˆ†æ
    if analysis_mode.value == "hyperopt" and not df_experiments.empty:

        # 1. ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®ç‰¹å®š
        # è¤‡æ•°ç›®çš„ã§ã®æœ€é©åŒ–: MSEæœ€å°åŒ– & å­¦ç¿’æ™‚é–“æœ€å°åŒ–
        df_opt = df_experiments.copy()
        df_opt['training_time'] = (pd.to_datetime(df_opt['end_time']) - pd.to_datetime(df_opt['start_time'])).dt.total_seconds() / 3600

        # ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆè¨ˆç®—
        def is_pareto_optimal(costs):
            is_efficient = np.ones(costs.shape[0], dtype=bool)
            for i, c in enumerate(costs):
                if is_efficient[i]:
                    is_efficient[is_efficient] = np.any(costs[is_efficient] <= c, axis=1)
                    is_efficient[i] = True
            return is_efficient

        objectives = df_opt[['reconstruction_mse', 'training_time']].values
        pareto_mask = is_pareto_optimal(objectives)

        # ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–
        fig_pareto = go.Figure()

        # éåŠ¹ç‡è§£
        fig_pareto.add_trace(go.Scatter(
            x=df_opt[~pareto_mask]['reconstruction_mse'],
            y=df_opt[~pareto_mask]['training_time'],
            mode='markers',
            name='ä¸€èˆ¬è§£',
            marker=dict(color='lightblue', size=8)
        ))

        # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£
        fig_pareto.add_trace(go.Scatter(
            x=df_opt[pareto_mask]['reconstruction_mse'],
            y=df_opt[pareto_mask]['training_time'],
            mode='markers',
            name='ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£',
            marker=dict(color='red', size=12, symbol='star')
        ))

        fig_pareto.update_layout(
            title="ğŸ¯ ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£åˆ†æ (MSE vs å­¦ç¿’æ™‚é–“)",
            xaxis_title="Reconstruction MSE",
            yaxis_title="å­¦ç¿’æ™‚é–“ (æ™‚é–“)"
        )

        # 2. æ¨å¥¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        pareto_experiments = df_opt[pareto_mask]
        best_balance = pareto_experiments.loc[pareto_experiments['reconstruction_mse'].idxmin()]

        # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦åˆ†æ
        param_ranges = {}
        for param in ['lr', 'beta', 'batch_size']:
            if param in df_experiments.columns:
                param_values = df_experiments[param].values
                mse_values = df_experiments['reconstruction_mse'].values

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’åŒºé–“ã«åˆ†å‰²
                n_bins = min(5, len(df_experiments))
                bins = pd.qcut(param_values, n_bins, duplicates='drop')
                bin_stats = df_experiments.groupby(bins)['reconstruction_mse'].agg(['mean', 'std', 'count'])

                param_ranges[param] = bin_stats

        # æ„Ÿåº¦åˆ†æã®å¯è¦–åŒ–
        fig_sensitivity = make_subplots(
            rows=1, cols=len(param_ranges),
            subplot_titles=list(param_ranges.keys())
        )

        for i, (param, stats_df) in enumerate(param_ranges.items()):
            fig_sensitivity.add_trace(
                go.Bar(
                    x=[str(interval) for interval in stats_df.index],
                    y=stats_df['mean'],
                    error_y=dict(type='data', array=stats_df['std']),
                    name=param,
                    showlegend=False
                ),
                row=1, col=i+1
            )

        fig_sensitivity.update_layout(height=400, title_text="ğŸ“ˆ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦åˆ†æ")

        hyperopt_content = mo.vstack([
            mo.md("## ğŸ§  ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–åˆ†æ"),
            mo.Html(fig_pareto.to_html()),
            mo.md("### ğŸ† æ¨å¥¨è¨­å®š (ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‹ã‚‰)"),
            mo.md(f"""
            - **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: {best_balance['config_file']}
            - **å­¦ç¿’ç‡**: {best_balance['lr']:.2e}
            - **Î²å€¤**: {best_balance['beta']:.3f}
            - **ãƒãƒƒãƒã‚µã‚¤ã‚º**: {int(best_balance['batch_size'])}
            - **MSE**: {best_balance['reconstruction_mse']:.2e}
            - **å­¦ç¿’æ™‚é–“**: {best_balance['training_time']:.2f}æ™‚é–“
            """),
            mo.Html(fig_sensitivity.to_html()),
            mo.md("### ğŸ“‹ ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ä¸€è¦§"),
            mo.ui.table(pareto_experiments[['config_file', 'lr', 'beta', 'reconstruction_mse', 'training_time']].round(6))
        ])
    else:
        hyperopt_content = mo.md("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")

    hyperopt_content
    return


@app.cell
def _(analysis_mode, df_experiments, mo, pd, px):
    def _():
        # æ½œåœ¨ç©ºé–“åˆ†æ
        if analysis_mode.value == "latent" and not df_experiments.empty:

            # ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ
            all_correlations = []

            for _, row in df_experiments.iterrows():
                if row['correlations_parsed']:
                    exp_id = row['id']
                    config = row['config_file']
                    mse = row['reconstruction_mse']

                    for metric, corr_list in row['correlations_parsed'].items():
                        for dim, (corr, p_val) in enumerate(corr_list):
                            all_correlations.append({
                                'experiment_id': exp_id,
                                'config_file': config,
                                'reconstruction_mse': mse,
                                'metric': metric,
                                'z_skill_dim': dim,
                                'correlation': corr,
                                'p_value': p_val,
                                'significant': p_val < 0.05,
                                'abs_correlation': abs(corr)
                            })

            if all_correlations:
                corr_df = pd.DataFrame(all_correlations)

                # 1. æœ€å¼·ç›¸é–¢ã®ç‰¹å®š
                significant_corr = corr_df[corr_df['significant']]
                if not significant_corr.empty:
                    strongest_corr = significant_corr.loc[significant_corr['abs_correlation'].idxmax()]

                    # 2. å®Ÿé¨“åˆ¥ç›¸é–¢å¼·åº¦
                    exp_corr_strength = corr_df.groupby('experiment_id').agg({
                        'abs_correlation': ['mean', 'max'],
                        'significant': 'sum'
                    }).round(4)
                    exp_corr_strength.columns = ['å¹³å‡ç›¸é–¢å¼·åº¦', 'æœ€å¤§ç›¸é–¢å¼·åº¦', 'æœ‰æ„ç›¸é–¢æ•°']

                    # 3. ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³
                    metric_analysis = corr_df.groupby('metric').agg({
                        'abs_correlation': ['mean', 'std', 'max'],
                        'significant': ['sum', 'count']
                    }).round(4)

                    # 4. ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆå®Ÿé¨“Ã—ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰
                    pivot_corr = corr_df.groupby(['experiment_id', 'metric'])['abs_correlation'].max().unstack(fill_value=0)

                    fig_heatmap = px.imshow(
                        pivot_corr,
                        title="ğŸ”¥ å®Ÿé¨“åˆ¥ãƒ¡ãƒˆãƒªãƒƒã‚¯ç›¸é–¢å¼·åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
                        labels=dict(x="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™", y="å®Ÿé¨“ID", color="ç›¸é–¢å¼·åº¦"),
                        color_continuous_scale='Viridis'
                    )

                    # 5. æ¬¡å…ƒåˆ¥ç›¸é–¢åˆ†æ
                    dim_analysis = corr_df.groupby('z_skill_dim').agg({
                        'abs_correlation': ['mean', 'std', 'count'],
                        'significant': 'sum'
                    }).round(4)

                    fig_dim_analysis = px.bar(
                        x=dim_analysis.index,
                        y=dim_analysis[('abs_correlation', 'mean')],
                        error_y=dim_analysis[('abs_correlation', 'std')],
                        title="ğŸ“Š z_skillæ¬¡å…ƒåˆ¥å¹³å‡ç›¸é–¢å¼·åº¦"
                    )

                    latent_content = mo.vstack([
                        mo.md("## ğŸ¯ æ½œåœ¨ç©ºé–“åˆ†æ"),
                        mo.md(f"""
                        ### ğŸ† æœ€å¼·ç›¸é–¢
                        - **å®Ÿé¨“**: {strongest_corr['config_file']}
                        - **æŒ‡æ¨™**: {strongest_corr['metric']}
                        - **æ¬¡å…ƒ**: z_skill_{strongest_corr['z_skill_dim']}
                        - **ç›¸é–¢**: {strongest_corr['correlation']:.4f}
                        - **på€¤**: {strongest_corr['p_value']:.4f}
                        """),
                        mo.Html(fig_heatmap.to_html()),
                        mo.Html(fig_dim_analysis.to_html()),
                        mo.md("### ğŸ“ˆ å®Ÿé¨“åˆ¥ç›¸é–¢å¼·åº¦"),
                        mo.ui.table(exp_corr_strength),
                        mo.md("### ğŸ“Š ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥åˆ†æ"),
                        mo.ui.table(metric_analysis)
                    ])
                else:
                    latent_content = mo.md("### âš ï¸ æœ‰æ„ãªç›¸é–¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                latent_content = mo.md("### â„¹ï¸ ç›¸é–¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            latent_content = mo.md("æ½œåœ¨ç©ºé–“åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return latent_content


    _()
    return


@app.cell
def _(analysis_mode, df_experiments, go, mo, pd, px):
    # å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ
    if analysis_mode.value == "training" and not df_experiments.empty:

        # 1. å­¦ç¿’åŠ¹ç‡åˆ†æ
        df_training = df_experiments.copy()
        df_training['training_time'] = (pd.to_datetime(df_training['end_time']) - pd.to_datetime(df_training['start_time'])).dt.total_seconds() / 3600
        df_training['efficiency'] = 1 / (df_training['reconstruction_mse'] * df_training['training_time'])
        df_training['convergence_ratio'] = df_training['best_val_loss'] / df_training['final_total_loss']

        # 2. å­¦ç¿’æ™‚é–“ vs ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        fig_efficiency = px.scatter(
            df_training,
            x='training_time',
            y='reconstruction_mse',
            size='num_epochs',
            color='lr',
            hover_data=['config_file', 'batch_size'],
            title="âš¡ å­¦ç¿’åŠ¹ç‡åˆ†æ (æ™‚é–“ vs æ€§èƒ½)",
            labels={'training_time': 'å­¦ç¿’æ™‚é–“ (æ™‚é–“)', 'reconstruction_mse': 'MSE'}
        )

        # 3. åæŸæ€§åˆ†æ
        fig_convergence = px.scatter(
            df_training,
            x='final_total_loss',
            y='best_val_loss',
            color='convergence_ratio',
            size='num_epochs',
            title="ğŸ¯ å­¦ç¿’åæŸæ€§åˆ†æ",
            labels={'final_total_loss': 'æœ€çµ‚å­¦ç¿’æå¤±', 'best_val_loss': 'æœ€è‰¯æ¤œè¨¼æå¤±'}
        )

        # ç†æƒ³çš„ãªåæŸç·šã‚’è¿½åŠ 
        min_loss = min(df_training['final_total_loss'].min(), df_training['best_val_loss'].min())
        max_loss = max(df_training['final_total_loss'].max(), df_training['best_val_loss'].max())
        fig_convergence.add_trace(go.Scatter(
            x=[min_loss, max_loss],
            y=[min_loss, max_loss],
            mode='lines',
            name='ç†æƒ³åæŸç·š',
            line=dict(dash='dash', color='red')
        ))

        # 4. å­¦ç¿’åŠ¹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        efficiency_ranking = df_training.nlargest(5, 'efficiency')[['config_file', 'reconstruction_mse', 'training_time', 'efficiency']]

        # 5. æ—©æœŸåœæ­¢åŠ¹æœåˆ†æ
        if 'num_epochs' in df_training.columns:
            epoch_analysis = df_training.groupby('num_epochs').agg({
                'reconstruction_mse': ['mean', 'std', 'count'],
                'training_time': 'mean'
            }).round(4)

            fig_epochs = px.scatter(
                df_training,
                x='num_epochs',
                y='reconstruction_mse',
                size='training_time',
                title="ğŸ“š ã‚¨ãƒãƒƒã‚¯æ•°ã¨æ€§èƒ½ã®é–¢ä¿‚"
            )

        training_content = mo.vstack([
            mo.md("## ğŸ“ˆ å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ"),
            mo.Html(fig_efficiency.to_html()),
            mo.Html(fig_convergence.to_html()),
            mo.md("### âš¡ å­¦ç¿’åŠ¹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° (åŠ¹ç‡ = 1/(MSE Ã— æ™‚é–“))"),
            mo.ui.table(efficiency_ranking.round(6)),
            mo.Html(fig_epochs.to_html()) if 'num_epochs' in df_training.columns else mo.md(""),
            mo.md("### ğŸ“Š åæŸæ€§ã‚µãƒãƒªãƒ¼"),
            mo.md(f"""
            - **å¹³å‡å­¦ç¿’æ™‚é–“**: {df_training['training_time'].mean():.2f}æ™‚é–“
            - **æœ€é«˜åŠ¹ç‡å®Ÿé¨“**: {efficiency_ranking.iloc[0]['config_file']}
            - **å¹³å‡åæŸæ¯”**: {df_training['convergence_ratio'].mean():.4f}
            - **éå­¦ç¿’ç‡**: {(df_training['convergence_ratio'] > 1.1).sum() / len(df_training) * 100:.1f}%
            """)
        ])
    else:
        training_content = mo.md("å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")

    training_content
    return


if __name__ == "__main__":
    app.run()
