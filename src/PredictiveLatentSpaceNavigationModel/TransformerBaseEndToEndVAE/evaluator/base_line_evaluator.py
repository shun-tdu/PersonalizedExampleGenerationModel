from typing import List, Dict, Any, Union, Tuple
import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
import pandas as pd

# CLAUDE_ADDED: æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’å›é¿ã™ã‚‹ãŸã‚ã®matplotlibè¨­å®š
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']

from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity

from .base_evaluator import BaseEvaluator
from .result_manager import EnhancedEvaluationResult
try:
    from PredictiveLatentSpaceNavigationModel.DataPreprocess.analyze_skill_metrics import SkillMetricCalculator
except ImportError:
    try:
        from DataPreprocess.analyze_skill_metrics import SkillMetricCalculator
    except ImportError:
        import sys
        import os
        # è¤‡æ•°ã®å¯èƒ½ãªãƒ‘ã‚¹ã‚’è©¦ã™
        possible_paths = ['/app', '/app/PredictiveLatentSpaceNavigationModel', 
                         os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')),
                         os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../..'))]
        for path in possible_paths:
            if path not in sys.path:
                sys.path.append(path)
        
        # æœ€å¾Œã®è©¦è¡Œ
        try:
            from DataPreprocess.analyze_skill_metrics import SkillMetricCalculator
        except ImportError:
            from PredictiveLatentSpaceNavigationModel.DataPreprocess.analyze_skill_metrics import SkillMetricCalculator


class TrajectoryGenerationEvaluator(BaseEvaluator):
    """å†æ§‹ç¯‰è»Œé“ã®çµ±è¨ˆçš„ãªãƒ‡ãƒ¼ã‚¿ã®å†ç¾åº¦ã‚’è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        originals = test_data.get('originals')
        reconstructed = test_data.get('reconstructed')
        scalers = test_data.get('scalers')  # CLAUDE_ADDED: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—

        # CLAUDE_ADDED: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯è»Œé“ã‚’ç”Ÿæˆã™ã‚‹
        if reconstructed is None:
            print("å†æ§‹æˆãƒ‡ãƒ¼ã‚¿ãªã—: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç”¨ã«è»Œé“ã‚’ç”Ÿæˆã—ã¾ã™")
            is_diffusion_model = hasattr(model, 'sample') and hasattr(model, 'num_timesteps')

            if is_diffusion_model:
                # æ½œåœ¨å¤‰æ•°ã‹ã‚‰è»Œé“ã‚’ç”Ÿæˆ
                z_style = test_data.get('z_style')
                z_skill = test_data.get('z_skill')

                if z_style is not None and z_skill is not None:
                    model.eval()
                    with torch.no_grad():
                        # numpy -> torch tensor
                        z_style_tensor = torch.tensor(z_style, dtype=torch.float32).to(device)
                        z_skill_tensor = torch.tensor(z_skill, dtype=torch.float32).to(device)

                        # æ‹¡æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        print(f"æ‹¡æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ: {len(z_style)} ã‚µãƒ³ãƒ—ãƒ«")
                        reconstructed_tensor = model.sample(z_style_tensor, z_skill_tensor)
                        reconstructed = reconstructed_tensor.cpu().numpy()
                        print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: shape={reconstructed.shape}")
                else:
                    print("è­¦å‘Š: z_style ã¾ãŸã¯ z_skill ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    return
            else:
                print("è­¦å‘Š: reconstructed ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return

        print("=" * 60)
        print("å†æ§‹ç¯‰è»Œé“èª¤å·®è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        # CLAUDE_ADDED: é€†æ­£è¦åŒ–ã—ã¦ç‰©ç†å˜ä½ã§è©•ä¾¡
        if scalers is not None:
            print("ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã‚’é€†æ­£è¦åŒ–ã—ã¦ç‰©ç†å˜ä½ã§è©•ä¾¡ã—ã¾ã™")
            originals_denorm = self._denormalize_trajectory(originals, scalers)
            reconstructed_denorm = self._denormalize_trajectory(reconstructed, scalers)

            # æ­£è¦åŒ–ç©ºé–“ã§ã®RMSEï¼ˆå­¦ç¿’æ™‚ã®æå¤±ã¨å¯¾å¿œï¼‰
            reconstructed_trajectory_rmse_normalized = self._evaluate_reconstruction_rmse(originals, reconstructed)

            # ç‰©ç†ç©ºé–“ã§ã®RMSEï¼ˆå®Ÿéš›ã®èª¤å·®ï¼‰
            reconstructed_trajectory_rmse_physical = self._evaluate_reconstruction_rmse(originals_denorm, reconstructed_denorm)

            # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®RMSEï¼ˆç‰©ç†ç©ºé–“ã§è¨ˆç®—ï¼‰
            reconstructed_trajectory_skill_metrics_rmse = self._evaluate_skill_metric_rmse(originals_denorm, reconstructed_denorm)

            # ä¸¡æ–¹ã®æŒ‡æ¨™ã‚’ä¿å­˜
            result.add_metric(name='reconstructed_trajectory_rmse_normalized',
                            value=reconstructed_trajectory_rmse_normalized,
                            description='å…ƒè»Œé“ã¨å†æ§‹ç¯‰è»Œé“ã®RMSEï¼ˆæ­£è¦åŒ–ç©ºé–“ï¼‰',
                            category='baseline')

            result.add_metric(name='reconstructed_trajectory_rmse_physical',
                            value=reconstructed_trajectory_rmse_physical,
                            description='å…ƒè»Œé“ã¨å†æ§‹ç¯‰è»Œé“ã®RMSEï¼ˆç‰©ç†ç©ºé–“ï¼‰',
                            category='baseline')
        else:
            print("âš ï¸ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±ãŒãªã„ãŸã‚ã€æ­£è¦åŒ–ç©ºé–“ã§è©•ä¾¡ã—ã¾ã™ï¼ˆç‰©ç†å˜ä½ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")
            reconstructed_trajectory_rmser = self._evaluate_reconstruction_rmse(originals, reconstructed)
            reconstructed_trajectory_skill_metrics_rmse = self._evaluate_skill_metric_rmse(originals, reconstructed)

            result.add_metric(name='reconstructed_trajectory_rmse',
                            value=reconstructed_trajectory_rmser,
                            description='å…ƒè»Œé“ã¨å†æ§‹ç¯‰è»Œé“ã®RMSE',
                            category='baseline')

            result.add_metric(name='reconstructed_trajectory_skill_metrics_rmse',
                            value=reconstructed_trajectory_skill_metrics_rmse,
                            description='å…ƒè»Œé“ã¨å†æ§‹ç¯‰è»Œé“ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®RMSE',
                            category='baseline')

        # ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®RMSEã‚’ä¿å­˜
        result.add_metric(name='reconstructed_trajectory_skill_metrics_rmse',
                          value=reconstructed_trajectory_skill_metrics_rmse,
                          description='å…ƒè»Œé“ã¨å†æ§‹ç¯‰è»Œé“ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®RMSE',
                          category='baseline')

        print("âœ… å†æ§‹ç¯‰è»Œé“èª¤å·®è©•ä¾¡å®Œäº†")

    def _evaluate_reconstruction_rmse(self, originals: np.ndarray, reconstructions: np.ndarray) -> float:
        """è»Œé“ã®å†æ§‹æˆèª¤å·®ã®å¹³å‡"""
        rmse_per_batch = np.sqrt(np.mean((originals - reconstructions) ** 2, axis=(1, 2)))
        mean_rmse = np.mean(rmse_per_batch)

        return mean_rmse

    def _evaluate_skill_metric_rmse(self, originals: np.ndarray, reconstructions: np.ndarray) -> float:
        """
        è»Œé“ã‹ã‚‰è¨ˆç®—ã—ãŸã‚¹ã‚­ãƒ«æŒ‡æ¨™ã®RMSEã‚’è¨ˆç®—ã™ã‚‹

        :param originals: ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹è»Œé“ãƒ‡ãƒ¼ã‚¿ [batch, seq_len, dim]
        :param reconstructions: ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å‡ºåŠ›ã•ã‚ŒãŸè»Œé“ãƒ‡ãƒ¼ã‚¿ [batch, seq_len, dim]
        :return:
        """
        # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’ç”Ÿãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
        original_skill_metrics_raw = self._calculate_skill_metrics_raw(originals)
        reconstructions_skill_metrics_raw = self._calculate_skill_metrics_raw(reconstructions)
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§çµ±ä¸€ã‚¹ã‚±ãƒ¼ãƒ©ã‚’ä½œæˆ
        combined_metrics = np.vstack([original_skill_metrics_raw, reconstructions_skill_metrics_raw])
        scaler = StandardScaler()
        scaler.fit(combined_metrics)
        
        # åŒã˜ã‚¹ã‚±ãƒ¼ãƒ©ã§ä¸¡æ–¹ã‚’æ¨™æº–åŒ–
        original_skill_metrics = scaler.transform(original_skill_metrics_raw)
        reconstructions_skill_metrics = scaler.transform(reconstructions_skill_metrics_raw)

        rmse_per_batch = np.mean((original_skill_metrics - reconstructions_skill_metrics) ** 2, axis=1)
        mean_rmse = np.mean(rmse_per_batch)

        return mean_rmse

    def _denormalize_trajectory(self, trajectory: np.ndarray, scalers: Dict) -> np.ndarray:
        """
        CLAUDE_ADDED: æ­£è¦åŒ–ã•ã‚ŒãŸè»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
        :param trajectory: æ­£è¦åŒ–ã•ã‚ŒãŸè»Œé“ãƒ‡ãƒ¼ã‚¿ [batch, seq_len, dim]
        :param scalers: ç‰¹å¾´é‡ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼è¾æ›¸
        :return: é€†æ­£è¦åŒ–ã•ã‚ŒãŸè»Œé“ãƒ‡ãƒ¼ã‚¿ [batch, seq_len, dim]
        """
        trajectory_features = ['HandlePosX', 'HandlePosY', 'HandleVelX',
                              'HandleVelY', 'HandleAccX', 'HandleAccY']

        batch_size, seq_len, n_features = trajectory.shape
        denormalized = trajectory.copy()

        # å„ç‰¹å¾´é‡ã‚’é€†å¤‰æ›
        for feat_idx, feat_name in enumerate(trajectory_features):
            if feat_name in scalers:
                scaler = scalers[feat_name]

                # [batch, seq_len] -> [batch*seq_len, 1] ã« reshape
                feature_data = trajectory[:, :, feat_idx].reshape(-1, 1)

                # é€†å¤‰æ›
                denorm_feature = scaler.inverse_transform(feature_data)

                # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
                denormalized[:, :, feat_idx] = denorm_feature.reshape(batch_size, seq_len)
            else:
                print(f"Warning: Scaler for '{feat_name}' not found. Skipping denormalization.")

        return denormalized

    def _calculate_skill_metrics_raw(self, trajectory: np.ndarray) -> np.ndarray:
        """
        è»Œé“ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ¨™æº–åŒ–ãªã—ï¼‰
        :param trajectory: è»Œé“ãƒ‡ãƒ¼ã‚¿[batch, seq_len, dim]
        :return: å„batchã®ã‚¹ã‚­ãƒ«æŒ‡æ¨™ [batch, num_skill_metrics]ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
        """
        columns = ['HandlePosX','HandlePosY','HandleVelX','HandleVelY','HandleAccX','HandleAccY']

        assert trajectory.shape[2] == len(columns), \
            f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒæ•°ãŒåˆã„ã¾ã›ã‚“ã€‚æœŸå¾…å€¤: {len(columns)}, å®Ÿéš›å€¤: {trajectory.shape[2]}"

        batch_size = trajectory.shape[0]
        seq_len = trajectory.shape[1]

        # DataFrameã«å¤‰æ›
        list_of_trajectories = [pd.DataFrame(trajectory[i], columns=columns).assign(Timestamp=np.arange(seq_len)*0.01) for i in range(batch_size)]

        # å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¹ã‚­ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—
        all_skill_metrics = []
        for trajectory_df in list_of_trajectories:
            skill_metrics_per_batch = [SkillMetricCalculator.calculate_curvature(trajectory_df),
                                       SkillMetricCalculator.calculate_velocity_smoothness(trajectory_df),
                                       SkillMetricCalculator.calculate_acceleration_smoothness(trajectory_df),
                                       SkillMetricCalculator.calculate_jerk(trajectory_df),
                                       SkillMetricCalculator.calculate_control_stability(trajectory_df),
                                       SkillMetricCalculator.calculate_temporal_consistency(trajectory_df),
                                       SkillMetricCalculator.calculate_trial_time(trajectory_df)]
            
            # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ã¯ç›®æ¨™ä½ç½®ãŒå¿…è¦ãªãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¨ˆç®—
            if 'TargetEndPosX' in trajectory_df.columns and 'TargetEndPosY' in trajectory_df.columns:
                skill_metrics_per_batch.append(SkillMetricCalculator.calculate_endpoint_error(trajectory_df))
            else:
                # ç›®æ¨™ä½ç½®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯0ã§ä»£æ›¿ï¼ˆã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                skill_metrics_per_batch.append(0.0)
            
            all_skill_metrics.append(skill_metrics_per_batch)

        all_skill_metrics_array = np.array(all_skill_metrics)  # shape: [batch_size, num_metrics]

        # NaN/infå€¤ã®å‡¦ç†
        if np.any(np.isnan(all_skill_metrics_array)) or np.any(np.isinf(all_skill_metrics_array)):
            print("Warning: NaN or inf values found in skill metrics. Replacing with zeros.")
            all_skill_metrics_array = np.nan_to_num(all_skill_metrics_array, nan=0.0, posinf=0.0, neginf=0.0)

        return all_skill_metrics_array  # æ¨™æº–åŒ–ãªã—ã§è¿”ã™

    def get_required_data(self) -> List[str]:
        # CLAUDE_ADDED: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ - reconstructionsã¾ãŸã¯z_style/z_skillãŒå¿…è¦
        # CLAUDE_ADDED: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±ã‚‚å¿…è¦ï¼ˆé€†æ­£è¦åŒ–ã®ãŸã‚ï¼‰
        return ['originals', 'z_style', 'z_skill', 'scalers']

class OrthogonalityEvaluator(BaseEvaluator):
    """æ½œåœ¨ç©ºé–“ã®ç›´äº¤æ€§è©•ä¾¡"""
    def __init__(self, config:Dict[str, Any]):
        super().__init__(config)

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        """æ½œåœ¨ç©ºé–“ã®ç›´äº¤æ€§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_style = test_data.get('z_style')
        z_skill = test_data.get('z_skill')

        print("=" * 60)
        print("æ½œåœ¨ç©ºé–“ã®ç›´äº¤æ€§è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        cosine_sim = self._calc_cosine_similarity(z_style, z_skill)

        # ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
        latent_space_crr_heatmap = self._create_latent_space_correlation_heatmap(z_style, z_skill)

        result.add_metric(name='latent_space_cosine_similarity',
                          value=cosine_sim,
                          description='æ½œåœ¨å¤‰æ•°ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦',
                          category='baseline')

        result.add_visualization(name='latent_space_correlation',
                                 fig_or_path=latent_space_crr_heatmap,
                                 description="æ½œåœ¨å¤‰æ•°ã®ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
                                 category='baseline')

        print("âœ… æ½œåœ¨ç©ºé–“ã®ç›´äº¤æ€§è©•ä¾¡å®Œäº†")

    def _calc_cosine_similarity(self, z_style: np.ndarray, z_skill: np.ndarray) -> float:
        """å„æ½œåœ¨å¤‰æ•°ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        similarities = []
        for i in range(len(z_style)):
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’2æ¬¡å…ƒé…åˆ—ã«reshape
            sim = cosine_similarity(z_style[i].reshape(-1, 1), z_skill[i].reshape(-1, 1))
            similarities.append(sim[0, 0])

        # å…¨ã‚µãƒ³ãƒ—ãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¹³å‡å€¤ã®çµ¶å¯¾å€¤ã‚’è¨ˆç®—
        return np.mean(np.abs(similarities))

    def _create_latent_space_correlation_heatmap(self, z_style: np.ndarray, z_skill: np.ndarray) -> plt.Figure:
        """å„æ½œåœ¨å¤‰æ•°ã®ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ"""
        # å„æ½œåœ¨å¤‰æ•°ã‚’DataFrameã«å¤‰æ›
        style_cols = [f'style_{i}' for i in range(z_style.shape[1])]
        skill_cols = [f'skill_{i}' for i in range(z_skill.shape[1])]
        df_style = pd.DataFrame(z_style, columns=style_cols)
        df_skill = pd.DataFrame(z_skill, columns=skill_cols)

        # 2ã¤ã®DataFrameã‚’ã‚«ãƒ©ãƒ æ–¹å‘ã«çµåˆ
        combined_df = pd.concat([df_style, df_skill], axis=1)

        # å…¨ä½“ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
        correlation_matrix = combined_df.corr()

        # ã‚¹ã‚¿ã‚¤ãƒ«æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«æ¬¡å…ƒã®ç›¸é–¢ã ã‘ã‚’æŠ½å‡º
        cross_correlation = correlation_matrix.loc[style_cols, skill_cols]

        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ
        fig_latent_corr, ax_latent_corr = plt.subplots(figsize=(6, 8))
        sns.heatmap(cross_correlation, annot=True, cmap='RdBu_r', center=0, fmt='.2f', ax=ax_latent_corr,annot_kws={"size": 4})
        ax_latent_corr.set_title('Cross-Correlation between Style (8D) and Skill (2D)')

        return fig_latent_corr

    def get_required_data(self) -> List[str]:
        return ['z_style', 'z_skill']


