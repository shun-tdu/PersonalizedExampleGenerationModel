# ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®è©•ä¾¡å™¨
from typing import List, Dict, Any, Union, Tuple
import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib
import plotly
import plotly.express as px
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# CLAUDE_ADDED: æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’å›é¿ã™ã‚‹ãŸã‚ã®matplotlibè¨­å®š
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']

from .base_evaluator import BaseEvaluator
from .result_manager import EnhancedEvaluationResult


class VisualizeSkillSpaceEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples_for_tsne = 30
        skill_components = config.get('evaluation', {}).get('skill_component', 2)
        self.n_components = skill_components if skill_components in [2, 3] else 2

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('all_z_skill')
        skill_scores = test_data.get('all_skill_scores')  # CLAUDE_ADDED: å…¨ãƒ‡ãƒ¼ã‚¿ç”¨ã«ä¿®æ­£
        subject_ids = test_data.get('all_subject_ids')   # CLAUDE_ADDED: å…¨ãƒ‡ãƒ¼ã‚¿ç”¨ã«ä¿®æ­£

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        pca_fig, tsne_fig = self._create_skill_latent_space_visualizations(
            z_skill=z_skill, skill_scores=skill_scores, subject_ids=subject_ids, n_components=self.n_components
        )

        result.add_visualization("skill_pca", pca_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®PCAå¯è¦–åŒ–ï¼ˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼‰",
                                category="skill_analysis")
        result.add_visualization("skill_tsne", tsne_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®t-SNEå¯è¦–åŒ–ï¼ˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼‰",
                                category="skill_analysis")

        print("âœ… ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“å¯è¦–åŒ–è©•ä¾¡å®Œäº†")

    def get_required_data(self) -> List[str]:
        return ['all_z_skill', 'all_skill_scores', 'all_subject_ids', 'experiment_id']

    def _create_skill_latent_space_visualizations(self, z_skill: np.ndarray, skill_scores: np.ndarray, subject_ids: List[str], n_components: int = 2) -> Union[Tuple[plt.Figure, plt.Figure], Tuple[plotly.graph_objs.Figure, plotly.graph_objs.Figure]]:
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ç”Ÿæˆ - ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ã€‚2Dã®å ´åˆã¯Matplotlibã€3Dã®å ´åˆã¯Plotly Figureã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        print(f"\nğŸ¯ ã‚¹ã‚­ãƒ«ç©ºé–“å¯è¦–åŒ–ç”Ÿæˆä¸­...")

        # CLAUDE_ADDED: ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’numpyé…åˆ—ã«å¤‰æ›ã—ã€NaNã‚„Infã‚’ãƒã‚§ãƒƒã‚¯
        skill_scores = np.array(skill_scores, dtype=np.float64)

        # NaNã‚„InfãŒã‚ã‚‹å ´åˆã¯ä¸­å¤®å€¤ã§ç½®æ›
        if np.any(np.isnan(skill_scores)) or np.any(np.isinf(skill_scores)):
            print(f"  âš ï¸ ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«NaN/InfãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ä¸­å¤®å€¤ã§ç½®æ›ã—ã¾ã™ã€‚")
            valid_mask = ~(np.isnan(skill_scores) | np.isinf(skill_scores))
            if np.any(valid_mask):
                median_score = np.median(skill_scores[valid_mask])
            else:
                median_score = 0.0
            skill_scores = np.where(np.isnan(skill_scores) | np.isinf(skill_scores), median_score, skill_scores)

        # 2æ¬¡å…ƒã®å ´åˆï¼ˆmatplotlibï¼‰
        if n_components == 2:
            # 1. ã‚¹ã‚­ãƒ«ç©ºé–“PCA
            fig_pca, ax_pca = plt.subplots(figsize=(12, 10))
            if z_skill.shape[1] >= 2:
                pca_skill = PCA(n_components=2)
                z_skill_pca = pca_skill.fit_transform(z_skill)

                scatter = ax_pca.scatter(z_skill_pca[:, 0], z_skill_pca[:, 1],
                                        c=skill_scores, cmap='RdYlBu_r', alpha=0.7, s=50,
                                        edgecolors='black', linewidth=0.5)
                ax_pca.set_title(f'Skill Space PCA\n(Color: Skill Score, Explained: {pca_skill.explained_variance_ratio_.sum():.3f})')
                ax_pca.set_xlabel(f'PC1 ({pca_skill.explained_variance_ratio_[0]:.3f})')
                ax_pca.set_ylabel(f'PC2 ({pca_skill.explained_variance_ratio_[1]:.3f})')

                # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ 
                cbar = plt.colorbar(scatter, ax=ax_pca)
                cbar.set_label('Skill Score', rotation=270, labelpad=20)

                # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
                ax_pca.text(0.02, 0.98, f'Samples: {len(z_skill)}\nSubjects: {len(set(subject_ids))}\nSkill Range: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]', 
                           transform=ax_pca.transAxes, verticalalignment='top', 
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            plt.tight_layout()

            # 2. ã‚¹ã‚­ãƒ«ç©ºé–“t-SNE
            fig_tsne, ax_tsne = plt.subplots(figsize=(12, 10))
            if len(z_skill) >= self.min_samples_for_tsne:
                try:
                    tsne = TSNE(n_components=2, perplexity=min(30, len(z_skill) // 4), random_state=42)
                    z_skill_tsne = tsne.fit_transform(z_skill)

                    scatter = ax_tsne.scatter(z_skill_tsne[:, 0], z_skill_tsne[:, 1],
                                            c=skill_scores, cmap='RdYlBu_r', alpha=0.7, s=50,
                                            edgecolors='black', linewidth=0.5)
                    ax_tsne.set_title('Skill Space t-SNE\n(Color: Skill Score)')
                    ax_tsne.set_xlabel('t-SNE 1')
                    ax_tsne.set_ylabel('t-SNE 2')

                    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ 
                    cbar = plt.colorbar(scatter, ax=ax_tsne)
                    cbar.set_label('Skill Score', rotation=270, labelpad=20)

                    # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
                    ax_tsne.text(0.02, 0.98, f'Samples: {len(z_skill)}\nSkill Range: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]', 
                               transform=ax_tsne.transAxes, verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

                except Exception as e:
                    ax_tsne.text(0.5, 0.5, f't-SNE Failed: {str(e)}', transform=ax_tsne.transAxes, ha='center')
            else:
                ax_tsne.text(0.5, 0.5, 'Insufficient samples for t-SNE', transform=ax_tsne.transAxes, ha='center')

            plt.tight_layout()
            return fig_pca, fig_tsne

        # 3æ¬¡å…ƒã®å ´åˆï¼ˆplotlyï¼‰
        elif n_components == 3:
            fig_pca = None
            # 1. ã‚¹ã‚­ãƒ«ç©ºé–“PCA
            if z_skill.shape[1] >= 3:
                pca_skill = PCA(n_components=3)
                z_skill_pca = pca_skill.fit_transform(z_skill)

                df_pca = pd.DataFrame({
                    "PC1": z_skill_pca[:, 0],
                    "PC2": z_skill_pca[:, 1],
                    "PC3": z_skill_pca[:, 2],
                    "skill_scores": skill_scores,
                    "subject_ids": subject_ids
                })

                fig_pca = px.scatter_3d(
                    df_pca,
                    x="PC1",
                    y="PC2",
                    z="PC3",
                    color='skill_scores',
                    color_continuous_scale='RdYlBu_r',
                    title=f'Skill Space PCA (3D)\n(Color: Skill Score, Explained: {pca_skill.explained_variance_ratio_.sum():.3f})',
                    hover_data={
                        'subject_ids': True,
                        'skill_scores': ':.3f',
                        'PC1': ':.3f',
                        'PC2': ':.3f', 
                        'PC3': ':.3f'
                    },
                    labels={
                        'PC1': f'PC1 ({pca_skill.explained_variance_ratio_[0]:.3f})',
                        'PC2': f'PC2 ({pca_skill.explained_variance_ratio_[1]:.3f})',
                        'PC3': f'PC3 ({pca_skill.explained_variance_ratio_[2]:.3f})',
                        'skill_scores': 'Skill Score'
                    }
                )
                fig_pca.update_layout(title_x=0.5)
                fig_pca.update_traces(marker=dict(size=4, opacity=0.8, line=dict(width=0.5, color='black')))

            # 2. ã‚¹ã‚­ãƒ«ç©ºé–“t-SNE
            fig_tsne = None
            if len(z_skill) >= self.min_samples_for_tsne:
                try:
                    tsne = TSNE(n_components=3, perplexity=min(30, len(z_skill) // 4), random_state=42)
                    z_skill_tsne = tsne.fit_transform(z_skill)

                    df_tsne = pd.DataFrame({
                        "t-SNE1": z_skill_tsne[:, 0],
                        "t-SNE2": z_skill_tsne[:, 1],
                        "t-SNE3": z_skill_tsne[:, 2],
                        "skill_scores": skill_scores,
                        "subject_ids": subject_ids
                    })

                    fig_tsne = px.scatter_3d(
                        df_tsne,
                        x="t-SNE1",
                        y="t-SNE2",
                        z="t-SNE3",
                        color='skill_scores',
                        color_continuous_scale='RdYlBu_r',
                        title='Skill Space t-SNE (3D)\n(Color: Skill Score)',
                        hover_data={
                            'subject_ids': True,
                            'skill_scores': ':.3f',
                            't-SNE1': ':.3f',
                            't-SNE2': ':.3f',
                            't-SNE3': ':.3f'
                        },
                        labels={'skill_scores': 'Skill Score'}
                    )
                    fig_tsne.update_layout(title_x=0.5)
                    fig_tsne.update_traces(marker=dict(size=4, opacity=0.8, line=dict(width=0.5, color='black')))

                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ç©ºã®Figureã‚’ä½œæˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã§é€šçŸ¥
                    fig_tsne = plotly.graph_objs.Figure()
                    fig_tsne.add_annotation(text=f"t-SNE Failed: {e}", showarrow=False, font=dict(size=16))
                    fig_tsne.update_layout(title_text="Skill Space t-SNE (3D) - Failed")
            else:
                # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã‚‚åŒæ§˜
                fig_tsne = plotly.graph_objs.Figure()
                fig_tsne.add_annotation(text="Insufficient samples for t-SNE", showarrow=False, font=dict(size=16))
                fig_tsne.update_layout(title_text="Skill Space t-SNE (3D) - Skipped")

            return fig_pca, fig_tsne
        else:
            raise ValueError(f"ä¸»æˆåˆ†åˆ†æã®ä¸»æˆåˆ†æ¬¡å…ƒæ•°ãŒä¸é©åˆ‡ã§ã™ï¼2ã‹3ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼")


class SkillScoreRegressionEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‹ã‚‰ç°¡å˜ãªSVM,MLPã§ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’å›å¸°å¯èƒ½ã‹ã‚’è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.test_size = 0.2
        self.cv_folds = 5
        self.random_state = 42
        self.min_samples = 20

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‹ã‚‰ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡å®Ÿè¡Œ (MLP & SVM)")
        print("=" * 60)

        if len(skill_scores) < self.min_samples:
            print(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {len(skill_scores)} < {self.min_samples}")
            result.add_metric("regression_status", 0, "ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³", "skill_regression")
            return

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X_processed, y_processed = self._preprocess_data(z_skill, skill_scores)
        
        # 1. MLPå›å¸°è©•ä¾¡
        mlp_results = self._evaluate_mlp_regression(X_processed, y_processed)
        
        # 2. SVMå›å¸°è©•ä¾¡
        svm_results = self._evaluate_svm_regression(X_processed, y_processed)
        
        # 3. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆç·šå½¢å›å¸°ï¼‰
        baseline_results = self._evaluate_baseline_regression(X_processed, y_processed)
        
        # 4. äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹é ‘å¥æ€§è©•ä¾¡
        cv_results = self._perform_cross_validation(X_processed, y_processed)
        
        # 5. å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_regression_visualization(
            X_processed, y_processed, mlp_results, svm_results, baseline_results
        )
        
        # çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
        self._add_regression_metrics(result, mlp_results, svm_results, baseline_results, cv_results)
        
        # å¯è¦–åŒ–ã‚’è¿½åŠ 
        result.add_visualization("skill_regression_analysis", visualization_fig,
                                description="MLPã¨SVMã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°æ€§èƒ½åˆ†æ",
                                category="skill_analysis")
        
        print("âœ… ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡å®Œäº†")

    def _preprocess_data(self, z_skill: np.ndarray, skill_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        from sklearn.preprocessing import StandardScaler
        
        print(f"\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†...")
        print(f"  å…¥åŠ›æ¬¡å…ƒ: {z_skill.shape[1]}")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(skill_scores)}")
        print(f"  ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç¯„å›²: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]")
        
        # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(z_skill)
        
        # NaNé™¤å»
        valid_indices = ~(np.isnan(X_scaled).any(axis=1) | np.isnan(skill_scores))
        X_clean = X_scaled[valid_indices]
        y_clean = np.array(skill_scores)[valid_indices]
        
        print(f"  å‰å‡¦ç†å¾Œã‚µãƒ³ãƒ—ãƒ«æ•°: {len(y_clean)}")
        
        return X_clean, y_clean

    def _evaluate_mlp_regression(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """MLPå›å¸°è©•ä¾¡"""
        from sklearn.neural_network import MLPRegressor
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ§  MLPå›å¸°è©•ä¾¡...")
        
        try:
            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            # è¤‡æ•°ã®MLPã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è©¦ã™
            mlp_configs = [
                {'hidden_layer_sizes': (50,), 'name': 'MLP-50'},
                {'hidden_layer_sizes': (100,), 'name': 'MLP-100'},
                {'hidden_layer_sizes': (50, 25), 'name': 'MLP-50-25'},
                {'hidden_layer_sizes': (100, 50), 'name': 'MLP-100-50'},
            ]
            
            best_mlp_result = None
            best_r2 = -np.inf
            
            for config in mlp_configs:
                try:
                    mlp = MLPRegressor(
                        hidden_layer_sizes=config['hidden_layer_sizes'],
                        activation='relu',
                        solver='adam',
                        alpha=0.001,
                        max_iter=1000,
                        random_state=self.random_state,
                        early_stopping=True,
                        validation_fraction=0.1
                    )
                    
                    mlp.fit(X_train, y_train)
                    y_pred = mlp.predict(X_test)
                    
                    # è©•ä¾¡æŒ‡æ¨™
                    mse = mean_squared_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    
                    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½
                    y_train_pred = mlp.predict(X_train)
                    train_r2 = r2_score(y_train, y_train_pred)
                    
                    result_config = {
                        'name': config['name'],
                        'architecture': config['hidden_layer_sizes'],
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'train_r2': train_r2,
                        'overfitting': train_r2 - r2,
                        'y_pred': y_pred,
                        'y_test': y_test,
                        'model': mlp,
                        'success': True
                    }
                    
                    print(f"  {config['name']}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, éå­¦ç¿’={train_r2-r2:.4f}")
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_mlp_result = result_config
                        
                except Exception as e:
                    print(f"  {config['name']}: ã‚¨ãƒ©ãƒ¼ {e}")
                    continue
            
            if best_mlp_result:
                print(f"  æœ€å„ªç§€MLP: {best_mlp_result['name']} (RÂ²={best_mlp_result['r2']:.4f})")
                return best_mlp_result
            else:
                return {'success': False, 'error': 'ã™ã¹ã¦ã®MLPæ§‹æˆãŒå¤±æ•—'}
                
        except Exception as e:
            print(f"  âŒ MLPè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _evaluate_svm_regression(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """SVMå›å¸°è©•ä¾¡"""
        from sklearn.svm import SVR
        from sklearn.model_selection import train_test_split, GridSearchCV
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ“ SVMå›å¸°è©•ä¾¡...")
        
        try:
            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            # SVM ã‚«ãƒ¼ãƒãƒ«ã¨ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            svm_configs = [
                {'kernel': 'linear', 'name': 'SVM-Linear'},
                {'kernel': 'rbf', 'name': 'SVM-RBF'},
                {'kernel': 'poly', 'degree': 2, 'name': 'SVM-Poly2'},
            ]
            
            best_svm_result = None
            best_r2 = -np.inf
            
            for config in svm_configs:
                try:
                    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
                    if config['kernel'] == 'linear':
                        param_grid = {'C': [0.1, 1, 10]}
                    else:
                        param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1]}
                    
                    base_svm = SVR(kernel=config['kernel'], **{k: v for k, v in config.items() if k not in ['kernel', 'name']})
                    
                    # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã¯ç°¡å˜ãªè©•ä¾¡
                    if len(X_train) < 50:
                        svm = base_svm
                        if config['kernel'] != 'linear':
                            svm.set_params(C=1, gamma='scale')
                        else:
                            svm.set_params(C=1)
                    else:
                        grid_search = GridSearchCV(base_svm, param_grid, cv=min(3, self.cv_folds), 
                                                 scoring='r2', n_jobs=-1)
                        grid_search.fit(X_train, y_train)
                        svm = grid_search.best_estimator_
                    
                    svm.fit(X_train, y_train)
                    y_pred = svm.predict(X_test)
                    
                    # è©•ä¾¡æŒ‡æ¨™
                    mse = mean_squared_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    
                    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½
                    y_train_pred = svm.predict(X_train)
                    train_r2 = r2_score(y_train, y_train_pred)
                    
                    result_config = {
                        'name': config['name'],
                        'kernel': config['kernel'],
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'train_r2': train_r2,
                        'overfitting': train_r2 - r2,
                        'y_pred': y_pred,
                        'y_test': y_test,
                        'model': svm,
                        'success': True
                    }
                    
                    print(f"  {config['name']}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, éå­¦ç¿’={train_r2-r2:.4f}")
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_svm_result = result_config
                        
                except Exception as e:
                    print(f"  {config['name']}: ã‚¨ãƒ©ãƒ¼ {e}")
                    continue
            
            if best_svm_result:
                print(f"  æœ€å„ªç§€SVM: {best_svm_result['name']} (RÂ²={best_svm_result['r2']:.4f})")
                return best_svm_result
            else:
                return {'success': False, 'error': 'ã™ã¹ã¦ã®SVMæ§‹æˆãŒå¤±æ•—'}
                
        except Exception as e:
            print(f"  âŒ SVMè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _evaluate_baseline_regression(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å›å¸°è©•ä¾¡ï¼ˆç·šå½¢å›å¸°ï¼‰"""
        from sklearn.linear_model import LinearRegression
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å›å¸°è©•ä¾¡...")
        
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            lr = LinearRegression()
            lr.fit(X_train, y_train)
            y_pred = lr.predict(X_test)
            
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            print(f"  ç·šå½¢å›å¸°: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
            
            return {
                'name': 'LinearRegression',
                'mse': mse,
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'y_pred': y_pred,
                'y_test': y_test,
                'model': lr,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _perform_cross_validation(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Dict[str, Any]]:
        """äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹é ‘å¥æ€§è©•ä¾¡"""
        from sklearn.model_selection import cross_val_score
        from sklearn.neural_network import MLPRegressor
        from sklearn.svm import SVR
        from sklearn.linear_model import LinearRegression
        
        print(f"\nğŸ”„ äº¤å·®æ¤œè¨¼è©•ä¾¡...")
        
        cv_results = {}
        models = {
            'MLP': MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, random_state=self.random_state),
            'SVM': SVR(kernel='rbf', C=1, gamma='scale'),
            'Linear': LinearRegression()
        }
        
        for name, model in models.items():
            try:
                scores = cross_val_score(model, X, y, cv=min(self.cv_folds, len(y)//4), 
                                       scoring='r2', n_jobs=-1)
                cv_results[name] = {
                    'mean_r2': np.mean(scores),
                    'std_r2': np.std(scores),
                    'scores': scores,
                    'success': True
                }
                print(f"  {name}: RÂ² = {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
                
            except Exception as e:
                print(f"  {name}: CV ã‚¨ãƒ©ãƒ¼ {e}")
                cv_results[name] = {'success': False, 'error': str(e)}
        
        return cv_results

    def _create_regression_visualization(self, X: np.ndarray, y: np.ndarray, mlp_results: Dict[str, Any], svm_results: Dict[str, Any], baseline_results: Dict[str, Any]) -> plt.Figure:
        """å›å¸°æ€§èƒ½ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # çµæœãƒªã‚¹ãƒˆ
        results = [
            (mlp_results, 'MLP'),
            (svm_results, 'SVM'), 
            (baseline_results, 'Linear')
        ]
        
        # æˆåŠŸã—ãŸçµæœã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        successful_results = [(r, name) for r, name in results if r.get('success', False)]
        
        if not successful_results:
            axes[0, 0].text(0.5, 0.5, 'ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—', ha='center', va='center', transform=axes[0, 0].transAxes)
            return fig
        
        # 1. äºˆæ¸¬ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
        for i, (result, name) in enumerate(successful_results[:3]):
            if 'y_test' in result and 'y_pred' in result:
                y_test = result['y_test']
                y_pred = result['y_pred']
                
                axes[0, i].scatter(y_test, y_pred, alpha=0.6, s=30)
                
                # ç†æƒ³ç·š
                min_val, max_val = min(np.min(y_test), np.min(y_pred)), max(np.max(y_test), np.max(y_pred))
                axes[0, i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
                
                axes[0, i].set_xlabel('True Skill Score')
                axes[0, i].set_ylabel('Predicted Skill Score')
                axes[0, i].set_title(f'{name}\n(RÂ² = {result.get("r2", 0):.3f})')
                
                # RÂ²ã¨RMSEã‚’è¡¨ç¤º
                axes[0, i].text(0.05, 0.95, f'RÂ² = {result.get("r2", 0):.3f}\nRMSE = {result.get("rmse", 0):.3f}',
                               transform=axes[0, i].transAxes, verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 4. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼‰
        best_result = max(successful_results, key=lambda x: x[0].get('r2', -np.inf))
        if best_result and 'y_test' in best_result[0] and 'y_pred' in best_result[0]:
            result, name = best_result
            residuals = result['y_test'] - result['y_pred']
            axes[1, 0].scatter(result['y_pred'], residuals, alpha=0.6, s=30)
            axes[1, 0].axhline(y=0, color='r', linestyle='--')
            axes[1, 0].set_xlabel('Predicted Skill Score')
            axes[1, 0].set_ylabel('Residuals')
            axes[1, 0].set_title(f'Residual Plot - {name}')
        
        # 5. æ€§èƒ½æ¯”è¼ƒãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
        if len(successful_results) > 1:
            names = [name for _, name in successful_results]
            r2_scores = [result.get('r2', 0) for result, _ in successful_results]
            rmse_scores = [result.get('rmse', 0) for result, _ in successful_results]
            
            x_pos = np.arange(len(names))
            
            axes[1, 1].bar(x_pos - 0.2, r2_scores, 0.4, label='RÂ²', alpha=0.7)
            axes[1, 1].set_xlabel('Models')
            axes[1, 1].set_ylabel('RÂ² Score')
            axes[1, 1].set_title('Model Comparison (RÂ²)')
            axes[1, 1].set_xticks(x_pos)
            axes[1, 1].set_xticklabels(names)
            
            # æ•°å€¤è¡¨ç¤º
            for i, score in enumerate(r2_scores):
                axes[1, 1].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')
        
        # 6. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        axes[1, 2].axis('off')
        summary_text = "Regression Performance Summary\n" + "="*20 + "\n"
        
        for result, name in successful_results:
            summary_text += f"{name}:\n"
            summary_text += f"  RÂ² = {result.get('r2', 0):.3f}\n"
            summary_text += f"  RMSE = {result.get('rmse', 0):.3f}\n"
            summary_text += f"  MAE = {result.get('mae', 0):.3f}\n"
            if 'overfitting' in result:
                summary_text += f"  Overfitting = {result.get('overfitting', 0):.3f}\n"
            summary_text += "\n"
        
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,
                       verticalalignment='top', fontsize=9,
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        return fig

    def _add_regression_metrics(self, result: EnhancedEvaluationResult, mlp_results: Dict[str, Any], svm_results: Dict[str, Any], baseline_results: Dict[str, Any], cv_results: Dict[str, Dict[str, Any]]) -> None:
        """å›å¸°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµæœã«è¿½åŠ """
        # MLPçµæœ
        if mlp_results.get('success', False):
            result.add_metric('skill_mlp_r2', mlp_results.get('r2', 0),
                            'MLPå›å¸°ã®RÂ²å€¤', 'skill_regression')
            result.add_metric('skill_mlp_rmse', mlp_results.get('rmse', 0),
                            'MLPå›å¸°ã®RMSE', 'skill_regression')
            result.add_metric('skill_mlp_overfitting', mlp_results.get('overfitting', 0),
                            'MLPã®éå­¦ç¿’åº¦', 'skill_regression')
        
        # SVMçµæœ  
        if svm_results.get('success', False):
            result.add_metric('skill_svm_r2', svm_results.get('r2', 0),
                            'SVMå›å¸°ã®RÂ²å€¤', 'skill_regression')
            result.add_metric('skill_svm_rmse', svm_results.get('rmse', 0),
                            'SVMå›å¸°ã®RMSE', 'skill_regression')
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœ
        if baseline_results.get('success', False):
            result.add_metric('skill_linear_r2', baseline_results.get('r2', 0),
                            'ç·šå½¢å›å¸°ã®RÂ²å€¤', 'skill_regression')
        
        # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«
        all_results = [mlp_results, svm_results, baseline_results]
        successful = [r for r in all_results if r.get('success', False)]
        if successful:
            best_model = max(successful, key=lambda x: x.get('r2', -np.inf))
            result.add_metric('skill_best_regression_r2', best_model.get('r2', 0),
                            'æœ€å„ªç§€å›å¸°ãƒ¢ãƒ‡ãƒ«ã®RÂ²å€¤', 'skill_regression')
        
        # äº¤å·®æ¤œè¨¼çµæœ
        for name, cv_result in cv_results.items():
            if cv_result.get('success', False):
                result.add_metric(f'skill_{name.lower()}_cv_r2_mean', cv_result.get('mean_r2', 0),
                                f'{name}ã®CVå¹³å‡RÂ²å€¤', 'skill_regression')
                result.add_metric(f'skill_{name.lower()}_cv_r2_std', cv_result.get('std_r2', 0),
                                f'{name}ã®CV RÂ²æ¨™æº–åå·®', 'skill_regression')

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'experiment_id']


class SkillLatentDimensionVSScoreEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã®ä¸»æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãŒç·šå½¢ãªé–¢ä¿‚ã«ã‚ã‚‹ã‹ã‚’è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples = 10
        self.significance_level = 0.05

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        """ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç·šå½¢é–¢ä¿‚è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒ vs ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç·šå½¢é–¢ä¿‚è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        if len(skill_scores) < self.min_samples:
            print(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {len(skill_scores)} < {self.min_samples}")
            result.add_metric("linear_relationship_status", 0, "ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³", "skill_linearity")
            return

        # 1. PCAåˆ†æã«ã‚ˆã‚‹ä¸»æ¬¡å…ƒç‰¹å®š
        pca_results = self._perform_pca_analysis(z_skill, skill_scores)
        
        # 2. å„æ¬¡å…ƒã¨ã®ç›¸é–¢åˆ†æ
        correlation_results = self._analyze_dimension_correlations(z_skill, skill_scores)
        
        # 3. ç·šå½¢å›å¸°åˆ†æ
        regression_results = self._perform_regression_analysis(z_skill, skill_scores)
        
        # 4. å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_linearity_visualization(
            z_skill, skill_scores, pca_results, correlation_results, regression_results
        )
        
        # çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
        self._add_linearity_metrics(result, pca_results, correlation_results, regression_results)
        
        # å¯è¦–åŒ–ã‚’è¿½åŠ 
        result.add_visualization("skill_linearity_analysis", visualization_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç·šå½¢é–¢ä¿‚åˆ†æ",
                                category="skill_analysis")
        
        print("âœ… ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒ vs ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç·šå½¢é–¢ä¿‚è©•ä¾¡å®Œäº†")

    def _perform_pca_analysis(self, z_skill: np.ndarray, skill_scores: np.ndarray) -> Dict[str, Any]:
        """PCAåˆ†æã§ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®ä¸»æ¬¡å…ƒã‚’ç‰¹å®š"""
        from sklearn.decomposition import PCA
        from scipy.stats import pearsonr
        
        print(f"\nğŸ” PCAåˆ†æå®Ÿè¡Œ...")
        
        # PCAå®Ÿè¡Œ
        pca = PCA()
        z_skill_pca = pca.fit_transform(z_skill)
        
        # å„ä¸»æˆåˆ†ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç›¸é–¢
        pc_correlations = []
        pc_p_values = []
        
        n_components = min(z_skill.shape[1], len(skill_scores))
        
        for i in range(n_components):
            try:
                corr, p_value = pearsonr(z_skill_pca[:, i], skill_scores)
                pc_correlations.append(corr)
                pc_p_values.append(p_value)
                
                significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
                print(f"  PC{i+1}: r={corr:.4f}, p={p_value:.4f} {significance}")
                
            except Exception as e:
                print(f"  PC{i+1}: è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {e}")
                pc_correlations.append(0.0)
                pc_p_values.append(1.0)
        
        # æœ€ã‚‚ç›¸é–¢ã®é«˜ã„ä¸»æˆåˆ†
        abs_correlations = [abs(c) for c in pc_correlations]
        best_pc_idx = np.argmax(abs_correlations) if abs_correlations else 0
        best_correlation = pc_correlations[best_pc_idx] if pc_correlations else 0.0
        
        print(f"  æœ€é«˜ç›¸é–¢: PC{best_pc_idx+1} (r={best_correlation:.4f})")
        
        return {
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'pc_correlations': pc_correlations,
            'pc_p_values': pc_p_values,
            'best_pc_idx': best_pc_idx,
            'best_correlation': best_correlation,
            'pca_components': pca.components_,
            'z_skill_pca': z_skill_pca
        }

    def _analyze_dimension_correlations(self, z_skill: np.ndarray, skill_scores: np.ndarray) -> Dict[str, Any]:
        """å„æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç›¸é–¢åˆ†æ"""
        from scipy.stats import pearsonr, spearmanr
        
        print(f"\nğŸ“Š æ¬¡å…ƒåˆ¥ç›¸é–¢åˆ†æ...")
        
        correlations = []
        p_values = []
        spearman_correlations = []
        spearman_p_values = []
        
        for dim in range(z_skill.shape[1]):
            try:
                # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ï¼ˆç·šå½¢é–¢ä¿‚ï¼‰
                pearson_corr, pearson_p = pearsonr(z_skill[:, dim], skill_scores)
                correlations.append(pearson_corr)
                p_values.append(pearson_p)
                
                # ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ï¼ˆå˜èª¿é–¢ä¿‚ï¼‰
                spearman_corr, spearman_p = spearmanr(z_skill[:, dim], skill_scores)
                spearman_correlations.append(spearman_corr)
                spearman_p_values.append(spearman_p)
                
                significance = "***" if pearson_p < 0.001 else "**" if pearson_p < 0.01 else "*" if pearson_p < 0.05 else ""
                print(f"  Dim{dim+1}: Pearson r={pearson_corr:.4f}(p={pearson_p:.4f}){significance}, Spearman Ï={spearman_corr:.4f}")
                
            except Exception as e:
                print(f"  Dim{dim+1}: è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {e}")
                correlations.append(0.0)
                p_values.append(1.0)
                spearman_correlations.append(0.0)
                spearman_p_values.append(1.0)
        
        # æœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ¬¡å…ƒ
        abs_correlations = [abs(c) for c in correlations]
        best_dim_idx = np.argmax(abs_correlations) if abs_correlations else 0
        best_dim_correlation = correlations[best_dim_idx] if correlations else 0.0
        
        print(f"  æœ€é«˜ç·šå½¢ç›¸é–¢: Dim{best_dim_idx+1} (r={best_dim_correlation:.4f})")
        
        return {
            'pearson_correlations': correlations,
            'pearson_p_values': p_values,
            'spearman_correlations': spearman_correlations,
            'spearman_p_values': spearman_p_values,
            'best_dim_idx': best_dim_idx,
            'best_dim_correlation': best_dim_correlation
        }

    def _perform_regression_analysis(self, z_skill: np.ndarray, skill_scores: np.ndarray) -> Dict[str, Any]:
        """ç·šå½¢å›å¸°åˆ†æ"""
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score, mean_squared_error
        from scipy import stats
        import warnings
        
        print(f"\nğŸ“ˆ ç·šå½¢å›å¸°åˆ†æ...")
        
        results = {}
        
        try:
            # å˜å¤‰é‡å›å¸°ï¼ˆæœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ¬¡å…ƒï¼‰
            correlations = []
            for dim in range(z_skill.shape[1]):
                try:
                    corr, _ = stats.pearsonr(z_skill[:, dim], skill_scores)
                    correlations.append(abs(corr))
                except:
                    correlations.append(0.0)
            
            best_dim = np.argmax(correlations)
            X_univariate = z_skill[:, best_dim].reshape(-1, 1)
            
            # å˜å¤‰é‡ç·šå½¢å›å¸°
            lr_uni = LinearRegression()
            lr_uni.fit(X_univariate, skill_scores)
            y_pred_uni = lr_uni.predict(X_univariate)
            
            r2_uni = r2_score(skill_scores, y_pred_uni)
            mse_uni = mean_squared_error(skill_scores, y_pred_uni)
            
            # å¤šå¤‰é‡å›å¸°ï¼ˆå…¨æ¬¡å…ƒï¼‰
            lr_multi = LinearRegression()
            lr_multi.fit(z_skill, skill_scores)
            y_pred_multi = lr_multi.predict(z_skill)
            
            r2_multi = r2_score(skill_scores, y_pred_multi)
            mse_multi = mean_squared_error(skill_scores, y_pred_multi)
            
            # çµ±è¨ˆçš„æ¤œå®š
            n = len(skill_scores)
            p_uni = z_skill.shape[1]  # å˜å¤‰é‡ãªã®ã§å®Ÿè³ª1
            p_multi = z_skill.shape[1]
            
            # Fçµ±è¨ˆé‡ï¼ˆå¤šå¤‰é‡å›å¸°ã®æœ‰æ„æ€§ï¼‰
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                f_stat = (r2_multi / p_multi) / ((1 - r2_multi) / max(1, n - p_multi - 1))
                f_p_value = 1 - stats.f.cdf(f_stat, p_multi, max(1, n - p_multi - 1))
            
            print(f"  å˜å¤‰é‡å›å¸° (Dim{best_dim+1}): RÂ²={r2_uni:.4f}, MSE={mse_uni:.4f}")
            print(f"  å¤šå¤‰é‡å›å¸° (å…¨æ¬¡å…ƒ): RÂ²={r2_multi:.4f}, MSE={mse_multi:.4f}")
            print(f"  Fæ¤œå®š: F={f_stat:.4f}, p={f_p_value:.4f}")
            
            # ç·šå½¢æ€§åˆ¤å®š
            if r2_uni > 0.7:
                linearity_status = "Strong linear relationship"
            elif r2_uni > 0.4:
                linearity_status = "Moderate linear relationship"
            elif r2_uni > 0.1:
                linearity_status = "Weak linear relationship"
            else:
                linearity_status = "No linear relationship"
                
            print(f"  ç·šå½¢æ€§åˆ¤å®š: {linearity_status}")
            
            results = {
                'best_dim': best_dim,
                'univariate_r2': r2_uni,
                'univariate_mse': mse_uni,
                'multivariate_r2': r2_multi,
                'multivariate_mse': mse_multi,
                'f_statistic': f_stat,
                'f_p_value': f_p_value,
                'linearity_status': linearity_status,
                'univariate_coef': lr_uni.coef_[0],
                'univariate_intercept': lr_uni.intercept_,
                'y_pred_uni': y_pred_uni,
                'y_pred_multi': y_pred_multi,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ å›å¸°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            results = {'success': False, 'error': str(e)}
        
        return results

    def _create_linearity_visualization(self, z_skill: np.ndarray, skill_scores: np.ndarray, pca_results: Dict[str, Any], correlation_results: Dict[str, Any], regression_results: Dict[str, Any]) -> plt.Figure:
        """ç·šå½¢é–¢ä¿‚ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. ä¸»æˆåˆ†ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ•£å¸ƒå›³
        if pca_results and 'z_skill_pca' in pca_results:
            best_pc = pca_results['best_pc_idx']
            pc_data = pca_results['z_skill_pca'][:, best_pc]

            # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®ç¢ºèªã¨èª¿æ•´
            min_len = min(len(pc_data), len(skill_scores))
            pc_data = pc_data[:min_len]
            skill_scores_plot = skill_scores[:min_len]

            axes[0, 0].scatter(pc_data, skill_scores_plot, alpha=0.6, s=30)
            axes[0, 0].set_xlabel(f'PC{best_pc+1}')
            axes[0, 0].set_ylabel('Skill Score')
            axes[0, 0].set_title(f'PC{best_pc+1} vs Skill Score\n(r={pca_results["best_correlation"]:.3f})')
            
            # å›å¸°ç›´ç·š
            z = np.polyfit(pc_data, skill_scores, 1)
            p = np.poly1d(z)
            axes[0, 0].plot(sorted(pc_data), p(sorted(pc_data)), "r--", alpha=0.8)
        
        # 2. æœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ•£å¸ƒå›³
        if correlation_results and regression_results.get('success', False):
            best_dim = correlation_results['best_dim_idx']
            dim_data = z_skill[:, best_dim]
            
            axes[0, 1].scatter(dim_data, skill_scores, alpha=0.6, s=30)
            axes[0, 1].set_xlabel(f'Latent Dim {best_dim+1}')
            axes[0, 1].set_ylabel('Skill Score')
            axes[0, 1].set_title(f'Best Latent Dim vs Skill Score\n(r={correlation_results["best_dim_correlation"]:.3f})')
            
            # å›å¸°ç›´ç·š
            if 'y_pred_uni' in regression_results:
                sorted_indices = np.argsort(dim_data)
                axes[0, 1].plot(dim_data[sorted_indices], regression_results['y_pred_uni'][sorted_indices], "r--", alpha=0.8)
        
        # 3. ç›¸é–¢ä¿‚æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if correlation_results:
            correlations = correlation_results['pearson_correlations']
            n_dims = len(correlations)
            corr_matrix = np.array(correlations).reshape(1, -1)
            
            im = axes[0, 2].imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
            axes[0, 2].set_xticks(range(n_dims))
            axes[0, 2].set_xticklabels([f'D{i+1}' for i in range(n_dims)])
            axes[0, 2].set_yticks([0])
            axes[0, 2].set_yticklabels(['Skill Score'])
            axes[0, 2].set_title('Dimension-Skill Correlations')
            plt.colorbar(im, ax=axes[0, 2])
        
        # 4. å›å¸°è¨ºæ–­: æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        if regression_results.get('success', False) and 'y_pred_uni' in regression_results:
            residuals = skill_scores - regression_results['y_pred_uni']
            axes[1, 0].scatter(regression_results['y_pred_uni'], residuals, alpha=0.6, s=30)
            axes[1, 0].axhline(y=0, color='r', linestyle='--')
            axes[1, 0].set_xlabel('Predicted Skill Score')
            axes[1, 0].set_ylabel('Residuals')
            axes[1, 0].set_title('Residual Plot (Univariate)')
        
        # 5. äºˆæ¸¬ vs å®Ÿæ¸¬å€¤
        if regression_results.get('success', False):
            axes[1, 1].scatter(skill_scores, regression_results['y_pred_uni'], alpha=0.6, s=30, label='Univariate')
            if 'y_pred_multi' in regression_results:
                axes[1, 1].scatter(skill_scores, regression_results['y_pred_multi'], alpha=0.6, s=30, label='Multivariate')
            
            # ç†æƒ³ç·š
            min_score, max_score = np.min(skill_scores), np.max(skill_scores)
            axes[1, 1].plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.8)
            axes[1, 1].set_xlabel('True Skill Score')
            axes[1, 1].set_ylabel('Predicted Skill Score')
            axes[1, 1].set_title('Prediction vs True Values')
            axes[1, 1].legend()
        
        # 6. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        axes[1, 2].axis('off')
        summary_text = "Linear Relationship Analysis\n" + "="*20 + "\n"
        
        if regression_results.get('success', False):
            summary_text += f"Univariate RÂ²: {regression_results.get('univariate_r2', 0):.3f}\n"
            summary_text += f"Multivariate RÂ²: {regression_results.get('multivariate_r2', 0):.3f}\n"
            summary_text += f"Verdict: {regression_results.get('linearity_status', 'N/A')}\n\n"
        
        if pca_results:
            summary_text += f"Highest PC Correlation: {pca_results.get('best_correlation', 0):.3f}\n"
        
        if correlation_results:
            summary_text += f"Highest Dim Correlation: {correlation_results.get('best_dim_correlation', 0):.3f}\n"
            
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes, 
                       verticalalignment='top', fontsize=10,
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        return fig

    def _add_linearity_metrics(self, result: EnhancedEvaluationResult, pca_results: Dict[str, Any], correlation_results: Dict[str, Any], regression_results: Dict[str, Any]) -> None:
        """ç·šå½¢é–¢ä¿‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµæœã«è¿½åŠ """
        # PCAé–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if pca_results:
            result.add_metric('skill_pc_best_correlation', 
                            abs(pca_results.get('best_correlation', 0)),
                            'ä¸»æˆåˆ†ã¨ã®æœ€é«˜ç›¸é–¢ä¿‚æ•°', 'skill_linearity')
        
        # æ¬¡å…ƒåˆ¥ç›¸é–¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if correlation_results:
            result.add_metric('skill_dim_best_correlation',
                            abs(correlation_results.get('best_dim_correlation', 0)),
                            'æ½œåœ¨æ¬¡å…ƒã¨ã®æœ€é«˜ç›¸é–¢ä¿‚æ•°', 'skill_linearity')
            
            # æœ‰æ„ãªç›¸é–¢ã®æ•°
            significant_correlations = sum(1 for p in correlation_results.get('pearson_p_values', []) 
                                        if p < self.significance_level)
            result.add_metric('skill_significant_correlations',
                            significant_correlations,
                            'æœ‰æ„ãªç›¸é–¢ã‚’æŒã¤æ¬¡å…ƒæ•°', 'skill_linearity')
        
        # å›å¸°åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if regression_results.get('success', False):
            result.add_metric('skill_univariate_r2',
                            regression_results.get('univariate_r2', 0),
                            'å˜å¤‰é‡å›å¸°ã®RÂ²å€¤', 'skill_linearity')
            result.add_metric('skill_multivariate_r2',
                            regression_results.get('multivariate_r2', 0),
                            'å¤šå¤‰é‡å›å¸°ã®RÂ²å€¤', 'skill_linearity')
            result.add_metric('skill_regression_f_statistic',
                            regression_results.get('f_statistic', 0),
                            'å›å¸°ã®æœ‰æ„æ€§Fçµ±è¨ˆé‡', 'skill_linearity')

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'experiment_id']


class SkillManifoldAnalysisEvaluator(BaseEvaluator):
    """CLAUDE_ADDED: ã‚¹ã‚­ãƒ«ç©ºé–“ã®ç†Ÿé”å¤šæ§˜ä½“å½¢æˆã‚’ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã§è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples = 20
        self.skilled_threshold_percentile = 75  # ä¸Šä½25%ã‚’ç†Ÿé”è€…ã¨ã¿ãªã™
        self.density_grid_size = 50
        self.manifold_threshold = 0.8  # å¯†åº¦ã—ãã„å€¤ï¼ˆæ­£è¦åŒ–å¾Œï¼‰

    def evaluate(self, model: torch.nn.Module, test_data: Dict[str, Any], device: torch.device, result: EnhancedEvaluationResult):
        """ç†Ÿé”å¤šæ§˜ä½“åˆ†æè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«ç©ºé–“ç†Ÿé”å¤šæ§˜ä½“åˆ†æè©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        # CLAUDE_ADDED: ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’ç¢ºèªã—ã¦numpyé…åˆ—ã«å¤‰æ›
        z_skill = np.array(z_skill)
        skill_scores = np.array(skill_scores)

        # CLAUDE_ADDED: z_skillã®å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        print(f"  z_skill shape: {z_skill.shape}")
        print(f"  skill_scores shape: {skill_scores.shape}")

        if len(skill_scores) < self.min_samples:
            print(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {len(skill_scores)} < {self.min_samples}")
            result.add_metric("manifold_analysis_status", 0, "ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³", "skill_manifold")
            return

        # CLAUDE_ADDED: Handle multi-dimensional skill factors
        # If skill_scores is multi-dimensional [B, num_factors], convert to scalar per sample
        if skill_scores.ndim != 1:
            print(f"âš ï¸ skill_scoresãŒå¤šæ¬¡å…ƒã§ã™: {skill_scores.shape}")
            if skill_scores.shape[0] == z_skill.shape[0]:
                # Multi-dimensional skill factors: use mean or first factor as representative
                print(f"  â†’ å„ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡å€¤ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚«ãƒ©ãƒ¼åŒ–ã—ã¾ã™")
                skill_scores_scalar = np.mean(skill_scores, axis=1)  # [B, num_factors] â†’ [B]
            else:
                print(f"  â†’ ã‚¨ãƒ©ãƒ¼: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
                result.add_metric("manifold_analysis_status", 0, "ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ä¸ä¸€è‡´", "skill_manifold")
                return
        else:
            skill_scores_scalar = skill_scores

        # 1. ç†Ÿé”è€…ãƒ»éç†Ÿé”è€…ã®åˆ†é¡
        skilled_threshold = np.percentile(skill_scores_scalar, self.skilled_threshold_percentile)
        skilled_mask = skill_scores_scalar >= skilled_threshold

        # CLAUDE_ADDED: z_skillã®æœ€åˆã®æ¬¡å…ƒãŒskill_scoresã®é•·ã•ã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if z_skill.shape[0] != len(skill_scores):
            print(f"âš ï¸ z_skill[{z_skill.shape}]ã¨skill_scores[{len(skill_scores)}]ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
            result.add_metric("manifold_analysis_status", 0, "ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ä¸ä¸€è‡´", "skill_manifold")
            return

        z_skilled = z_skill[skilled_mask]
        z_unskilled = z_skill[~skilled_mask]
        scores_skilled = skill_scores[skilled_mask]
        scores_unskilled = skill_scores[~skilled_mask]

        print(f"ç†Ÿé”è€…: {len(z_skilled)}ã‚µãƒ³ãƒ—ãƒ« (ã‚¹ã‚³ã‚¢ >= {skilled_threshold:.3f})")
        print(f"éç†Ÿé”è€…: {len(z_unskilled)}ã‚µãƒ³ãƒ—ãƒ« (ã‚¹ã‚³ã‚¢ < {skilled_threshold:.3f})")

        # 2. ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã«ã‚ˆã‚‹å¯†åº¦åˆ†æ
        density_analysis = self._perform_density_analysis(z_skill, z_skilled, z_unskilled, skill_scores)

        # 3. å¤šæ§˜ä½“ç‰¹æ€§ã®è©•ä¾¡
        manifold_metrics = self._evaluate_manifold_properties(z_skilled, z_unskilled, density_analysis)

        # 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ
        clustering_analysis = self._perform_clustering_analysis(z_skilled, z_unskilled)

        # 5. å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_manifold_visualization(
            z_skill, skill_scores, z_skilled, z_unskilled,
            density_analysis, manifold_metrics, clustering_analysis
        )

        # çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
        self._add_manifold_metrics(result, manifold_metrics, clustering_analysis, density_analysis)

        # å¯è¦–åŒ–ã‚’è¿½åŠ 
        result.add_visualization("skill_manifold_analysis", visualization_fig,
                                description="ã‚¹ã‚­ãƒ«ç©ºé–“ã«ãŠã‘ã‚‹ç†Ÿé”å¤šæ§˜ä½“ã®å½¢æˆåˆ†æ",
                                category="skill_analysis")

        print("âœ… ã‚¹ã‚­ãƒ«ç©ºé–“ç†Ÿé”å¤šæ§˜ä½“åˆ†æè©•ä¾¡å®Œäº†")

    def _perform_density_analysis(self, z_skill: np.ndarray, z_skilled: np.ndarray, z_unskilled: np.ndarray, skill_scores: np.ndarray) -> Dict[str, Any]:
        """ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã«ã‚ˆã‚‹å¯†åº¦åˆ†æ"""
        from sklearn.neighbors import KernelDensity
        from sklearn.decomposition import PCA

        print(f"\nğŸŒ ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šå®Ÿè¡Œ...")

        try:
            # 2æ¬¡å…ƒã«æ¬¡å…ƒå‰Šæ¸›ã—ã¦ã‹ã‚‰å¯†åº¦æ¨å®š
            pca = PCA(n_components=2)
            z_skill_2d = pca.fit_transform(z_skill)
            z_skilled_2d = pca.transform(z_skilled)
            z_unskilled_2d = pca.transform(z_unskilled)

            # ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼ˆç†Ÿé”è€…ãƒ‡ãƒ¼ã‚¿ï¼‰
            kde_skilled = KernelDensity(kernel='gaussian', bandwidth=0.5)
            kde_skilled.fit(z_skilled_2d)

            # ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼ˆéç†Ÿé”è€…ãƒ‡ãƒ¼ã‚¿ï¼‰
            kde_unskilled = KernelDensity(kernel='gaussian', bandwidth=0.5)
            kde_unskilled.fit(z_unskilled_2d)

            # å…¨ä½“ã®ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®š
            kde_all = KernelDensity(kernel='gaussian', bandwidth=0.5)
            kde_all.fit(z_skill_2d)

            # ã‚°ãƒªãƒƒãƒ‰ä½œæˆ
            x_min, x_max = z_skill_2d[:, 0].min() - 1, z_skill_2d[:, 0].max() + 1
            y_min, y_max = z_skill_2d[:, 1].min() - 1, z_skill_2d[:, 1].max() + 1

            xx, yy = np.meshgrid(np.linspace(x_min, x_max, self.density_grid_size),
                               np.linspace(y_min, y_max, self.density_grid_size))
            grid_points = np.c_[xx.ravel(), yy.ravel()]

            # å¯†åº¦è¨ˆç®—
            density_skilled = np.exp(kde_skilled.score_samples(grid_points)).reshape(xx.shape)
            density_unskilled = np.exp(kde_unskilled.score_samples(grid_points)).reshape(xx.shape)
            density_all = np.exp(kde_all.score_samples(grid_points)).reshape(xx.shape)

            # ç†Ÿé”è€…ã®å¯†åº¦é›†ä¸­åº¦ã‚’è¨ˆç®—
            skilled_density_ratio = density_skilled / (density_all + 1e-8)

            # é«˜å¯†åº¦é ˜åŸŸã®ç‰¹å®š
            density_threshold = np.percentile(density_skilled.ravel(), 90)
            high_density_mask = density_skilled > density_threshold

            print(f"  ç†Ÿé”è€…å¯†åº¦ãƒ”ãƒ¼ã‚¯: {np.max(density_skilled):.4f}")
            print(f"  éç†Ÿé”è€…å¯†åº¦ãƒ”ãƒ¼ã‚¯: {np.max(density_unskilled):.4f}")
            print(f"  ç†Ÿé”è€…é«˜å¯†åº¦é ˜åŸŸ: {np.sum(high_density_mask) / high_density_mask.size * 100:.1f}%")

            return {
                'pca': pca,
                'z_skill_2d': z_skill_2d,
                'z_skilled_2d': z_skilled_2d,
                'z_unskilled_2d': z_unskilled_2d,
                'kde_skilled': kde_skilled,
                'kde_unskilled': kde_unskilled,
                'kde_all': kde_all,
                'xx': xx,
                'yy': yy,
                'density_skilled': density_skilled,
                'density_unskilled': density_unskilled,
                'density_all': density_all,
                'skilled_density_ratio': skilled_density_ratio,
                'high_density_mask': high_density_mask,
                'density_peak_skilled': np.max(density_skilled),
                'density_peak_unskilled': np.max(density_unskilled),
                'high_density_area_ratio': np.sum(high_density_mask) / high_density_mask.size,
                'success': True
            }

        except Exception as e:
            print(f"  âŒ å¯†åº¦æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _evaluate_manifold_properties(self, z_skilled: np.ndarray, z_unskilled: np.ndarray, density_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å¤šæ§˜ä½“ç‰¹æ€§ã®è©•ä¾¡"""
        print(f"\nğŸ“ å¤šæ§˜ä½“ç‰¹æ€§è©•ä¾¡...")

        try:
            metrics = {}

            # 1. ç†Ÿé”è€…ã®é›†ä¸­åº¦ï¼ˆåˆ†æ•£æ¯”è¼ƒï¼‰
            skilled_var = np.var(z_skilled, axis=0).mean()
            unskilled_var = np.var(z_unskilled, axis=0).mean()
            concentration_ratio = unskilled_var / (skilled_var + 1e-8)

            print(f"  ç†Ÿé”è€…åˆ†æ•£: {skilled_var:.4f}")
            print(f"  éç†Ÿé”è€…åˆ†æ•£: {unskilled_var:.4f}")
            print(f"  é›†ä¸­åº¦æ¯”: {concentration_ratio:.4f}")

            # 2. å¯†åº¦é›†ä¸­åº¦
            if density_analysis.get('success', False):
                density_concentration = density_analysis['density_peak_skilled'] / (density_analysis['density_peak_unskilled'] + 1e-8)
                high_density_compactness = density_analysis['high_density_area_ratio']

                print(f"  å¯†åº¦é›†ä¸­åº¦: {density_concentration:.4f}")
                print(f"  é«˜å¯†åº¦é ˜åŸŸåœ§ç¸®æ€§: {high_density_compactness:.4f}")

                metrics.update({
                    'density_concentration': density_concentration,
                    'high_density_compactness': high_density_compactness
                })

            # 3. ç†Ÿé”è€…é–“ã®å¹³å‡è·é›¢ vs éç†Ÿé”è€…é–“ã®å¹³å‡è·é›¢
            from scipy.spatial.distance import pdist

            if len(z_skilled) > 1:
                skilled_distances = pdist(z_skilled)
                avg_skilled_distance = np.mean(skilled_distances)
            else:
                avg_skilled_distance = 0

            if len(z_unskilled) > 1:
                unskilled_distances = pdist(z_unskilled)
                avg_unskilled_distance = np.mean(unskilled_distances)
            else:
                avg_unskilled_distance = 0

            distance_ratio = avg_unskilled_distance / (avg_skilled_distance + 1e-8)

            print(f"  ç†Ÿé”è€…å¹³å‡è·é›¢: {avg_skilled_distance:.4f}")
            print(f"  éç†Ÿé”è€…å¹³å‡è·é›¢: {avg_unskilled_distance:.4f}")
            print(f"  è·é›¢æ¯”: {distance_ratio:.4f}")

            # 4. å¤šæ§˜ä½“å½¢æˆåˆ¤å®š
            manifold_score = (concentration_ratio + distance_ratio) / 2
            if density_analysis.get('success', False):
                manifold_score = (manifold_score + density_analysis['density_concentration']) / 2

            if manifold_score > 2.0:
                manifold_status = "Strong manifold formation"
            elif manifold_score > 1.5:
                manifold_status = "Moderate manifold formation"
            elif manifold_score > 1.0:
                manifold_status = "Weak manifold formation"
            else:
                manifold_status = "No clear manifold formation"

            print(f"  å¤šæ§˜ä½“ã‚¹ã‚³ã‚¢: {manifold_score:.4f}")
            print(f"  å¤šæ§˜ä½“åˆ¤å®š: {manifold_status}")

            metrics.update({
                'concentration_ratio': concentration_ratio,
                'distance_ratio': distance_ratio,
                'manifold_score': manifold_score,
                'manifold_status': manifold_status,
                'skilled_variance': skilled_var,
                'unskilled_variance': unskilled_var,
                'avg_skilled_distance': avg_skilled_distance,
                'avg_unskilled_distance': avg_unskilled_distance,
                'success': True
            })

            return metrics

        except Exception as e:
            print(f"  âŒ å¤šæ§˜ä½“ç‰¹æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _perform_clustering_analysis(self, z_skilled: np.ndarray, z_unskilled: np.ndarray) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã«ã‚ˆã‚‹å¤šæ§˜ä½“æ§‹é€ ã®è©•ä¾¡"""
        from sklearn.cluster import DBSCAN
        from sklearn.metrics import silhouette_score

        print(f"\nğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ...")

        try:
            results = {}

            # ç†Ÿé”è€…ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            if len(z_skilled) >= 4:  # DBSCANã«æœ€ä½é™å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«æ•°
                dbscan_skilled = DBSCAN(eps=0.5, min_samples=3)
                skilled_labels = dbscan_skilled.fit_predict(z_skilled)

                n_clusters_skilled = len(set(skilled_labels)) - (1 if -1 in skilled_labels else 0)
                n_noise_skilled = list(skilled_labels).count(-1)

                if n_clusters_skilled > 0:
                    try:
                        silhouette_skilled = silhouette_score(z_skilled, skilled_labels)
                    except:
                        silhouette_skilled = 0.0
                else:
                    silhouette_skilled = 0.0

                print(f"  ç†Ÿé”è€…ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_skilled}")
                print(f"  ç†Ÿé”è€…ãƒã‚¤ã‚ºç‚¹: {n_noise_skilled}/{len(z_skilled)}")
                print(f"  ç†Ÿé”è€…ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°: {silhouette_skilled:.4f}")

                results.update({
                    'skilled_clusters': n_clusters_skilled,
                    'skilled_noise_ratio': n_noise_skilled / len(z_skilled),
                    'skilled_silhouette': silhouette_skilled
                })

            # éç†Ÿé”è€…ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            if len(z_unskilled) >= 4:
                dbscan_unskilled = DBSCAN(eps=0.5, min_samples=3)
                unskilled_labels = dbscan_unskilled.fit_predict(z_unskilled)

                n_clusters_unskilled = len(set(unskilled_labels)) - (1 if -1 in unskilled_labels else 0)
                n_noise_unskilled = list(unskilled_labels).count(-1)

                if n_clusters_unskilled > 0:
                    try:
                        silhouette_unskilled = silhouette_score(z_unskilled, unskilled_labels)
                    except:
                        silhouette_unskilled = 0.0
                else:
                    silhouette_unskilled = 0.0

                print(f"  éç†Ÿé”è€…ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_unskilled}")
                print(f"  éç†Ÿé”è€…ãƒã‚¤ã‚ºç‚¹: {n_noise_unskilled}/{len(z_unskilled)}")
                print(f"  éç†Ÿé”è€…ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°: {silhouette_unskilled:.4f}")

                results.update({
                    'unskilled_clusters': n_clusters_unskilled,
                    'unskilled_noise_ratio': n_noise_unskilled / len(z_unskilled),
                    'unskilled_silhouette': silhouette_unskilled
                })

            results['success'] = True
            return results

        except Exception as e:
            print(f"  âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _create_manifold_visualization(self, z_skill: np.ndarray, skill_scores: np.ndarray,
                                     z_skilled: np.ndarray, z_unskilled: np.ndarray,
                                     density_analysis: Dict[str, Any], manifold_metrics: Dict[str, Any],
                                     clustering_analysis: Dict[str, Any]) -> plt.Figure:
        """ç†Ÿé”å¤šæ§˜ä½“ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        if not density_analysis.get('success', False):
            axes[0, 0].text(0.5, 0.5, 'å¯†åº¦æ¨å®šå¤±æ•—', ha='center', va='center', transform=axes[0, 0].transAxes)
            return fig

        # 1. ç†Ÿé”è€…å¯†åº¦ãƒãƒƒãƒ—
        im1 = axes[0, 0].contourf(density_analysis['xx'], density_analysis['yy'],
                                 density_analysis['density_skilled'], levels=20, cmap='Reds', alpha=0.8)
        axes[0, 0].scatter(density_analysis['z_skilled_2d'][:, 0], density_analysis['z_skilled_2d'][:, 1],
                          c='red', s=30, alpha=0.7, label='Skilled')
        axes[0, 0].scatter(density_analysis['z_unskilled_2d'][:, 0], density_analysis['z_unskilled_2d'][:, 1],
                          c='blue', s=20, alpha=0.5, label='Unskilled')
        axes[0, 0].set_title('Skilled Density Map')
        axes[0, 0].legend()
        plt.colorbar(im1, ax=axes[0, 0])

        # 2. å¯†åº¦æ¯”è¼ƒ
        im2 = axes[0, 1].contourf(density_analysis['xx'], density_analysis['yy'],
                                 density_analysis['skilled_density_ratio'], levels=20, cmap='RdYlBu_r', alpha=0.8)
        axes[0, 1].scatter(density_analysis['z_skilled_2d'][:, 0], density_analysis['z_skilled_2d'][:, 1],
                          c='red', s=30, alpha=0.7)
        axes[0, 1].set_title('Skilled/Total Density Ratio')
        plt.colorbar(im2, ax=axes[0, 1])

        # 3. ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        scatter = axes[0, 2].scatter(density_analysis['z_skill_2d'][:, 0], density_analysis['z_skill_2d'][:, 1],
                                   c=skill_scores, cmap='viridis', s=40, alpha=0.7)
        axes[0, 2].set_title('Skill Score Distribution')
        plt.colorbar(scatter, ax=axes[0, 2])

        # 4. é«˜å¯†åº¦é ˜åŸŸ
        axes[1, 0].contour(density_analysis['xx'], density_analysis['yy'],
                          density_analysis['density_skilled'], levels=[np.percentile(density_analysis['density_skilled'].ravel(), 90)],
                          colors='red', linewidths=2)
        axes[1, 0].scatter(density_analysis['z_skilled_2d'][:, 0], density_analysis['z_skilled_2d'][:, 1],
                          c='red', s=30, alpha=0.7, label='Skilled')
        axes[1, 0].scatter(density_analysis['z_unskilled_2d'][:, 0], density_analysis['z_unskilled_2d'][:, 1],
                          c='blue', s=20, alpha=0.5, label='Unskilled')
        axes[1, 0].set_title('High Density Regions (90th percentile)')
        axes[1, 0].legend()

        # 5. è·é›¢åˆ†å¸ƒæ¯”è¼ƒ
        if manifold_metrics.get('success', False):
            from scipy.spatial.distance import pdist

            if len(z_skilled) > 1:
                skilled_distances = pdist(z_skilled)
                axes[1, 1].hist(skilled_distances, bins=20, alpha=0.7, label='Skilled', color='red', density=True)

            if len(z_unskilled) > 1:
                unskilled_distances = pdist(z_unskilled)
                axes[1, 1].hist(unskilled_distances, bins=20, alpha=0.7, label='Unskilled', color='blue', density=True)

            axes[1, 1].set_xlabel('Pairwise Distance')
            axes[1, 1].set_ylabel('Density')
            axes[1, 1].set_title('Distance Distribution Comparison')
            axes[1, 1].legend()

        # 6. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒãƒªãƒ¼
        axes[1, 2].axis('off')
        summary_text = "Manifold Analysis Summary\n" + "="*25 + "\n"

        if manifold_metrics.get('success', False):
            summary_text += f"Manifold Score: {manifold_metrics.get('manifold_score', 0):.3f}\n"
            summary_text += f"Status: {manifold_metrics.get('manifold_status', 'N/A')}\n\n"
            summary_text += f"Concentration Ratio: {manifold_metrics.get('concentration_ratio', 0):.3f}\n"
            summary_text += f"Distance Ratio: {manifold_metrics.get('distance_ratio', 0):.3f}\n"

            if 'density_concentration' in manifold_metrics:
                summary_text += f"Density Concentration: {manifold_metrics.get('density_concentration', 0):.3f}\n"

        if clustering_analysis.get('success', False):
            summary_text += f"\nClustering:\n"
            if 'skilled_clusters' in clustering_analysis:
                summary_text += f"Skilled Clusters: {clustering_analysis.get('skilled_clusters', 0)}\n"
            if 'unskilled_clusters' in clustering_analysis:
                summary_text += f"Unskilled Clusters: {clustering_analysis.get('unskilled_clusters', 0)}\n"

        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,
                       verticalalignment='top', fontsize=9,
                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

        plt.tight_layout()
        return fig

    def _add_manifold_metrics(self, result: EnhancedEvaluationResult, manifold_metrics: Dict[str, Any],
                            clustering_analysis: Dict[str, Any], density_analysis: Dict[str, Any]) -> None:
        """å¤šæ§˜ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµæœã«è¿½åŠ """
        if manifold_metrics.get('success', False):
            result.add_metric('skill_manifold_score', manifold_metrics.get('manifold_score', 0),
                            'ç†Ÿé”å¤šæ§˜ä½“å½¢æˆã‚¹ã‚³ã‚¢', 'skill_manifold')
            result.add_metric('skill_concentration_ratio', manifold_metrics.get('concentration_ratio', 0),
                            'ç†Ÿé”è€…é›†ä¸­åº¦æ¯”', 'skill_manifold')
            result.add_metric('skill_distance_ratio', manifold_metrics.get('distance_ratio', 0),
                            'è·é›¢æ¯”ï¼ˆéç†Ÿé”è€…/ç†Ÿé”è€…ï¼‰', 'skill_manifold')

            if 'density_concentration' in manifold_metrics:
                result.add_metric('skill_density_concentration', manifold_metrics.get('density_concentration', 0),
                                'å¯†åº¦é›†ä¸­åº¦', 'skill_manifold')

        if clustering_analysis.get('success', False):
            if 'skilled_clusters' in clustering_analysis:
                result.add_metric('skill_skilled_clusters', clustering_analysis.get('skilled_clusters', 0),
                                'ç†Ÿé”è€…ã‚¯ãƒ©ã‚¹ã‚¿æ•°', 'skill_manifold')
            if 'skilled_silhouette' in clustering_analysis:
                result.add_metric('skill_skilled_silhouette', clustering_analysis.get('skilled_silhouette', 0),
                                'ç†Ÿé”è€…ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°', 'skill_manifold')

        if density_analysis.get('success', False):
            result.add_metric('skill_high_density_area_ratio', density_analysis.get('high_density_area_ratio', 0),
                            'é«˜å¯†åº¦é ˜åŸŸã®å‰²åˆ', 'skill_manifold')

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'subject_ids', 'experiment_id']