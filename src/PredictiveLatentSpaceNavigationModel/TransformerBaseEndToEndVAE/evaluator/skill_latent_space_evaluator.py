# ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®è©•ä¾¡å™¨
from typing import List, Dict, Any, Union, Tuple
import numpy as np
import matplotlib.pyplot as plt
import plotly
import plotly.express as px
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from base_evaluator import BaseEvaluator
from src.PredictiveLatentSpaceNavigationModel.TransformerBaseEndToEndVAE.evaluator import EnhancedEvaluationResult


class VisualizeSkillSpaceEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples_for_tsne = 30
        skill_components = config.get('evaluation', {}).get('skill_component', 2)
        self.n_components = skill_components if skill_components in [2, 3] else 2

    def evaluate(self, model, test_data, result: EnhancedEvaluationResult) -> None:
        """ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        pca_fig, tsne_fig = self._create_skill_latent_space_visualizations(
            z_skill=z_skill, skill_scores=skill_scores, subject_ids=subject_ids, n_components=self.n_components
        )

        result.add_visualization("skill_pca", pca_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®PCAå¯è¦–åŒ–ï¼ˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼‰",
                                category="skill_analysis")
        result.add_visualization("skill_tsne", tsne_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®t-SNEå¯è¦–åŒ–ï¼ˆã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼‰",
                                category="skill_analysis")

        print("âœ… ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“å¯è¦–åŒ–è©•ä¾¡å®Œäº†")

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'subject_ids', 'experiment_id']

    def _create_skill_latent_space_visualizations(self, z_skill, skill_scores, subject_ids, n_components=2) -> Union[Tuple[plt.Figure, plt.Figure], Tuple[plotly.graph_objs.Figure, plotly.graph_objs.Figure]]:
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ç”Ÿæˆ - ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘ã€‚2Dã®å ´åˆã¯Matplotlibã€3Dã®å ´åˆã¯Plotly Figureã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        print(f"\nğŸ¯ ã‚¹ã‚­ãƒ«ç©ºé–“å¯è¦–åŒ–ç”Ÿæˆä¸­...")

        # ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢æ­£è¦åŒ–ï¼ˆã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ç”¨ï¼‰
        skill_scores_normalized = (skill_scores - np.min(skill_scores)) / (np.max(skill_scores) - np.min(skill_scores) + 1e-8)

        # 2æ¬¡å…ƒã®å ´åˆï¼ˆmatplotlibï¼‰
        if n_components == 2:
            # 1. ã‚¹ã‚­ãƒ«ç©ºé–“PCA
            fig_pca, ax_pca = plt.subplots(figsize=(12, 10))
            if z_skill.shape[1] >= 2:
                pca_skill = PCA(n_components=2)
                z_skill_pca = pca_skill.fit_transform(z_skill)

                scatter = ax_pca.scatter(z_skill_pca[:, 0], z_skill_pca[:, 1],
                                        c=skill_scores, cmap='RdYlBu_r', alpha=0.7, s=50,
                                        edgecolors='black', linewidth=0.5)
                ax_pca.set_title(f'Skill Space PCA\n(Color: Skill Score, Explained: {pca_skill.explained_variance_ratio_.sum():.3f})')
                ax_pca.set_xlabel(f'PC1 ({pca_skill.explained_variance_ratio_[0]:.3f})')
                ax_pca.set_ylabel(f'PC2 ({pca_skill.explained_variance_ratio_[1]:.3f})')

                # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ 
                cbar = plt.colorbar(scatter, ax=ax_pca)
                cbar.set_label('Skill Score', rotation=270, labelpad=20)

                # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
                ax_pca.text(0.02, 0.98, f'Samples: {len(z_skill)}\nSubjects: {len(set(subject_ids))}\nSkill Range: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]', 
                           transform=ax_pca.transAxes, verticalalignment='top', 
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            plt.tight_layout()

            # 2. ã‚¹ã‚­ãƒ«ç©ºé–“t-SNE
            fig_tsne, ax_tsne = plt.subplots(figsize=(12, 10))
            if len(z_skill) >= self.min_samples_for_tsne:
                try:
                    tsne = TSNE(n_components=2, perplexity=min(30, len(z_skill) // 4), random_state=42)
                    z_skill_tsne = tsne.fit_transform(z_skill)

                    scatter = ax_tsne.scatter(z_skill_tsne[:, 0], z_skill_tsne[:, 1],
                                            c=skill_scores, cmap='RdYlBu_r', alpha=0.7, s=50,
                                            edgecolors='black', linewidth=0.5)
                    ax_tsne.set_title('Skill Space t-SNE\n(Color: Skill Score)')
                    ax_tsne.set_xlabel('t-SNE 1')
                    ax_tsne.set_ylabel('t-SNE 2')

                    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ 
                    cbar = plt.colorbar(scatter, ax=ax_tsne)
                    cbar.set_label('Skill Score', rotation=270, labelpad=20)

                    # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
                    ax_tsne.text(0.02, 0.98, f'Samples: {len(z_skill)}\nSkill Range: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]', 
                               transform=ax_tsne.transAxes, verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

                except Exception as e:
                    ax_tsne.text(0.5, 0.5, f't-SNE Failed: {str(e)}', transform=ax_tsne.transAxes, ha='center')
            else:
                ax_tsne.text(0.5, 0.5, 'Insufficient samples for t-SNE', transform=ax_tsne.transAxes, ha='center')

            plt.tight_layout()
            return fig_pca, fig_tsne

        # 3æ¬¡å…ƒã®å ´åˆï¼ˆplotlyï¼‰
        elif n_components == 3:
            fig_pca = None
            # 1. ã‚¹ã‚­ãƒ«ç©ºé–“PCA
            if z_skill.shape[1] >= 3:
                pca_skill = PCA(n_components=3)
                z_skill_pca = pca_skill.fit_transform(z_skill)

                df_pca = pd.DataFrame({
                    "PC1": z_skill_pca[:, 0],
                    "PC2": z_skill_pca[:, 1],
                    "PC3": z_skill_pca[:, 2],
                    "skill_scores": skill_scores,
                    "subject_ids": subject_ids
                })

                fig_pca = px.scatter_3d(
                    df_pca,
                    x="PC1",
                    y="PC2",
                    z="PC3",
                    color='skill_scores',
                    color_continuous_scale='RdYlBu_r',
                    title=f'Skill Space PCA (3D)\n(Color: Skill Score, Explained: {pca_skill.explained_variance_ratio_.sum():.3f})',
                    hover_data={
                        'subject_ids': True,
                        'skill_scores': ':.3f',
                        'PC1': ':.3f',
                        'PC2': ':.3f', 
                        'PC3': ':.3f'
                    },
                    labels={
                        'PC1': f'PC1 ({pca_skill.explained_variance_ratio_[0]:.3f})',
                        'PC2': f'PC2 ({pca_skill.explained_variance_ratio_[1]:.3f})',
                        'PC3': f'PC3 ({pca_skill.explained_variance_ratio_[2]:.3f})',
                        'skill_scores': 'Skill Score'
                    }
                )
                fig_pca.update_layout(title_x=0.5)
                fig_pca.update_traces(marker=dict(size=4, opacity=0.8, line=dict(width=0.5, color='black')))

            # 2. ã‚¹ã‚­ãƒ«ç©ºé–“t-SNE
            fig_tsne = None
            if len(z_skill) >= self.min_samples_for_tsne:
                try:
                    tsne = TSNE(n_components=3, perplexity=min(30, len(z_skill) // 4), random_state=42)
                    z_skill_tsne = tsne.fit_transform(z_skill)

                    df_tsne = pd.DataFrame({
                        "t-SNE1": z_skill_tsne[:, 0],
                        "t-SNE2": z_skill_tsne[:, 1],
                        "t-SNE3": z_skill_tsne[:, 2],
                        "skill_scores": skill_scores,
                        "subject_ids": subject_ids
                    })

                    fig_tsne = px.scatter_3d(
                        df_tsne,
                        x="t-SNE1",
                        y="t-SNE2",
                        z="t-SNE3",
                        color='skill_scores',
                        color_continuous_scale='RdYlBu_r',
                        title='Skill Space t-SNE (3D)\n(Color: Skill Score)',
                        hover_data={
                            'subject_ids': True,
                            'skill_scores': ':.3f',
                            't-SNE1': ':.3f',
                            't-SNE2': ':.3f',
                            't-SNE3': ':.3f'
                        },
                        labels={'skill_scores': 'Skill Score'}
                    )
                    fig_tsne.update_layout(title_x=0.5)
                    fig_tsne.update_traces(marker=dict(size=4, opacity=0.8, line=dict(width=0.5, color='black')))

                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ç©ºã®Figureã‚’ä½œæˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã§é€šçŸ¥
                    fig_tsne = plotly.graph_objs.Figure()
                    fig_tsne.add_annotation(text=f"t-SNE Failed: {e}", showarrow=False, font=dict(size=16))
                    fig_tsne.update_layout(title_text="Skill Space t-SNE (3D) - Failed")
            else:
                # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã‚‚åŒæ§˜
                fig_tsne = plotly.graph_objs.Figure()
                fig_tsne.add_annotation(text="Insufficient samples for t-SNE", showarrow=False, font=dict(size=16))
                fig_tsne.update_layout(title_text="Skill Space t-SNE (3D) - Skipped")

            return fig_pca, fig_tsne
        else:
            raise ValueError(f"ä¸»æˆåˆ†åˆ†æã®ä¸»æˆåˆ†æ¬¡å…ƒæ•°ãŒä¸é©åˆ‡ã§ã™ï¼2ã‹3ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼")


class SkillScoreRegressionEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‹ã‚‰ç°¡å˜ãªSVM,MLPã§ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã‚’å›å¸°å¯èƒ½ã‹ã‚’è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.test_size = 0.2
        self.cv_folds = 5
        self.random_state = 42
        self.min_samples = 20

    def evaluate(self, model, test_data, result: EnhancedEvaluationResult) -> None:
        """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã‹ã‚‰ã®ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡å®Ÿè¡Œ (MLP & SVM)")
        print("=" * 60)

        if len(skill_scores) < self.min_samples:
            print(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {len(skill_scores)} < {self.min_samples}")
            result.add_metric("regression_status", 0, "ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³", "skill_regression")
            return

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X_processed, y_processed = self._preprocess_data(z_skill, skill_scores)
        
        # 1. MLPå›å¸°è©•ä¾¡
        mlp_results = self._evaluate_mlp_regression(X_processed, y_processed)
        
        # 2. SVMå›å¸°è©•ä¾¡
        svm_results = self._evaluate_svm_regression(X_processed, y_processed)
        
        # 3. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆç·šå½¢å›å¸°ï¼‰
        baseline_results = self._evaluate_baseline_regression(X_processed, y_processed)
        
        # 4. äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹é ‘å¥æ€§è©•ä¾¡
        cv_results = self._perform_cross_validation(X_processed, y_processed)
        
        # 5. å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_regression_visualization(
            X_processed, y_processed, mlp_results, svm_results, baseline_results
        )
        
        # çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
        self._add_regression_metrics(result, mlp_results, svm_results, baseline_results, cv_results)
        
        # å¯è¦–åŒ–ã‚’è¿½åŠ 
        result.add_visualization("skill_regression_analysis", visualization_fig,
                                description="MLPã¨SVMã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°æ€§èƒ½åˆ†æ",
                                category="skill_analysis")
        
        print("âœ… ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢å›å¸°è©•ä¾¡å®Œäº†")

    def _preprocess_data(self, z_skill, skill_scores):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        from sklearn.preprocessing import StandardScaler
        
        print(f"\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†...")
        print(f"  å…¥åŠ›æ¬¡å…ƒ: {z_skill.shape[1]}")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(skill_scores)}")
        print(f"  ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç¯„å›²: [{np.min(skill_scores):.3f}, {np.max(skill_scores):.3f}]")
        
        # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(z_skill)
        
        # NaNé™¤å»
        valid_indices = ~(np.isnan(X_scaled).any(axis=1) | np.isnan(skill_scores))
        X_clean = X_scaled[valid_indices]
        y_clean = np.array(skill_scores)[valid_indices]
        
        print(f"  å‰å‡¦ç†å¾Œã‚µãƒ³ãƒ—ãƒ«æ•°: {len(y_clean)}")
        
        return X_clean, y_clean

    def _evaluate_mlp_regression(self, X, y):
        """MLPå›å¸°è©•ä¾¡"""
        from sklearn.neural_network import MLPRegressor
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ§  MLPå›å¸°è©•ä¾¡...")
        
        try:
            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            # è¤‡æ•°ã®MLPã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è©¦ã™
            mlp_configs = [
                {'hidden_layer_sizes': (50,), 'name': 'MLP-50'},
                {'hidden_layer_sizes': (100,), 'name': 'MLP-100'},
                {'hidden_layer_sizes': (50, 25), 'name': 'MLP-50-25'},
                {'hidden_layer_sizes': (100, 50), 'name': 'MLP-100-50'},
            ]
            
            best_mlp_result = None
            best_r2 = -np.inf
            
            for config in mlp_configs:
                try:
                    mlp = MLPRegressor(
                        hidden_layer_sizes=config['hidden_layer_sizes'],
                        activation='relu',
                        solver='adam',
                        alpha=0.001,
                        max_iter=1000,
                        random_state=self.random_state,
                        early_stopping=True,
                        validation_fraction=0.1
                    )
                    
                    mlp.fit(X_train, y_train)
                    y_pred = mlp.predict(X_test)
                    
                    # è©•ä¾¡æŒ‡æ¨™
                    mse = mean_squared_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    
                    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½
                    y_train_pred = mlp.predict(X_train)
                    train_r2 = r2_score(y_train, y_train_pred)
                    
                    result_config = {
                        'name': config['name'],
                        'architecture': config['hidden_layer_sizes'],
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'train_r2': train_r2,
                        'overfitting': train_r2 - r2,
                        'y_pred': y_pred,
                        'y_test': y_test,
                        'model': mlp,
                        'success': True
                    }
                    
                    print(f"  {config['name']}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, éå­¦ç¿’={train_r2-r2:.4f}")
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_mlp_result = result_config
                        
                except Exception as e:
                    print(f"  {config['name']}: ã‚¨ãƒ©ãƒ¼ {e}")
                    continue
            
            if best_mlp_result:
                print(f"  æœ€å„ªç§€MLP: {best_mlp_result['name']} (RÂ²={best_mlp_result['r2']:.4f})")
                return best_mlp_result
            else:
                return {'success': False, 'error': 'ã™ã¹ã¦ã®MLPæ§‹æˆãŒå¤±æ•—'}
                
        except Exception as e:
            print(f"  âŒ MLPè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _evaluate_svm_regression(self, X, y):
        """SVMå›å¸°è©•ä¾¡"""
        from sklearn.svm import SVR
        from sklearn.model_selection import train_test_split, GridSearchCV
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ“ SVMå›å¸°è©•ä¾¡...")
        
        try:
            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            # SVM ã‚«ãƒ¼ãƒãƒ«ã¨ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            svm_configs = [
                {'kernel': 'linear', 'name': 'SVM-Linear'},
                {'kernel': 'rbf', 'name': 'SVM-RBF'},
                {'kernel': 'poly', 'degree': 2, 'name': 'SVM-Poly2'},
            ]
            
            best_svm_result = None
            best_r2 = -np.inf
            
            for config in svm_configs:
                try:
                    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
                    if config['kernel'] == 'linear':
                        param_grid = {'C': [0.1, 1, 10]}
                    else:
                        param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1]}
                    
                    base_svm = SVR(kernel=config['kernel'], **{k: v for k, v in config.items() if k not in ['kernel', 'name']})
                    
                    # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã¯ç°¡å˜ãªè©•ä¾¡
                    if len(X_train) < 50:
                        svm = base_svm
                        if config['kernel'] != 'linear':
                            svm.set_params(C=1, gamma='scale')
                        else:
                            svm.set_params(C=1)
                    else:
                        grid_search = GridSearchCV(base_svm, param_grid, cv=min(3, self.cv_folds), 
                                                 scoring='r2', n_jobs=-1)
                        grid_search.fit(X_train, y_train)
                        svm = grid_search.best_estimator_
                    
                    svm.fit(X_train, y_train)
                    y_pred = svm.predict(X_test)
                    
                    # è©•ä¾¡æŒ‡æ¨™
                    mse = mean_squared_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    
                    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½
                    y_train_pred = svm.predict(X_train)
                    train_r2 = r2_score(y_train, y_train_pred)
                    
                    result_config = {
                        'name': config['name'],
                        'kernel': config['kernel'],
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'train_r2': train_r2,
                        'overfitting': train_r2 - r2,
                        'y_pred': y_pred,
                        'y_test': y_test,
                        'model': svm,
                        'success': True
                    }
                    
                    print(f"  {config['name']}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, éå­¦ç¿’={train_r2-r2:.4f}")
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_svm_result = result_config
                        
                except Exception as e:
                    print(f"  {config['name']}: ã‚¨ãƒ©ãƒ¼ {e}")
                    continue
            
            if best_svm_result:
                print(f"  æœ€å„ªç§€SVM: {best_svm_result['name']} (RÂ²={best_svm_result['r2']:.4f})")
                return best_svm_result
            else:
                return {'success': False, 'error': 'ã™ã¹ã¦ã®SVMæ§‹æˆãŒå¤±æ•—'}
                
        except Exception as e:
            print(f"  âŒ SVMè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _evaluate_baseline_regression(self, X, y):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å›å¸°è©•ä¾¡ï¼ˆç·šå½¢å›å¸°ï¼‰"""
        from sklearn.linear_model import LinearRegression
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        
        print(f"\nğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å›å¸°è©•ä¾¡...")
        
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state
            )
            
            lr = LinearRegression()
            lr.fit(X_train, y_train)
            y_pred = lr.predict(X_test)
            
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            print(f"  ç·šå½¢å›å¸°: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
            
            return {
                'name': 'LinearRegression',
                'mse': mse,
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'y_pred': y_pred,
                'y_test': y_test,
                'model': lr,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'error': str(e)}

    def _perform_cross_validation(self, X, y):
        """äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹é ‘å¥æ€§è©•ä¾¡"""
        from sklearn.model_selection import cross_val_score
        from sklearn.neural_network import MLPRegressor
        from sklearn.svm import SVR
        from sklearn.linear_model import LinearRegression
        
        print(f"\nğŸ”„ äº¤å·®æ¤œè¨¼è©•ä¾¡...")
        
        cv_results = {}
        models = {
            'MLP': MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, random_state=self.random_state),
            'SVM': SVR(kernel='rbf', C=1, gamma='scale'),
            'Linear': LinearRegression()
        }
        
        for name, model in models.items():
            try:
                scores = cross_val_score(model, X, y, cv=min(self.cv_folds, len(y)//4), 
                                       scoring='r2', n_jobs=-1)
                cv_results[name] = {
                    'mean_r2': np.mean(scores),
                    'std_r2': np.std(scores),
                    'scores': scores,
                    'success': True
                }
                print(f"  {name}: RÂ² = {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
                
            except Exception as e:
                print(f"  {name}: CV ã‚¨ãƒ©ãƒ¼ {e}")
                cv_results[name] = {'success': False, 'error': str(e)}
        
        return cv_results

    def _create_regression_visualization(self, X, y, mlp_results, svm_results, baseline_results):
        """å›å¸°æ€§èƒ½ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # çµæœãƒªã‚¹ãƒˆ
        results = [
            (mlp_results, 'MLP'),
            (svm_results, 'SVM'), 
            (baseline_results, 'Linear')
        ]
        
        # æˆåŠŸã—ãŸçµæœã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        successful_results = [(r, name) for r, name in results if r.get('success', False)]
        
        if not successful_results:
            axes[0, 0].text(0.5, 0.5, 'ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—', ha='center', va='center', transform=axes[0, 0].transAxes)
            return fig
        
        # 1. äºˆæ¸¬ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
        for i, (result, name) in enumerate(successful_results[:3]):
            if 'y_test' in result and 'y_pred' in result:
                y_test = result['y_test']
                y_pred = result['y_pred']
                
                axes[0, i].scatter(y_test, y_pred, alpha=0.6, s=30)
                
                # ç†æƒ³ç·š
                min_val, max_val = min(np.min(y_test), np.min(y_pred)), max(np.max(y_test), np.max(y_pred))
                axes[0, i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
                
                axes[0, i].set_xlabel('True Skill Score')
                axes[0, i].set_ylabel('Predicted Skill Score')
                axes[0, i].set_title(f'{name}\n(RÂ² = {result.get("r2", 0):.3f})')
                
                # RÂ²ã¨RMSEã‚’è¡¨ç¤º
                axes[0, i].text(0.05, 0.95, f'RÂ² = {result.get("r2", 0):.3f}\nRMSE = {result.get("rmse", 0):.3f}',
                               transform=axes[0, i].transAxes, verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 4. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼‰
        best_result = max(successful_results, key=lambda x: x[0].get('r2', -np.inf))
        if best_result and 'y_test' in best_result[0] and 'y_pred' in best_result[0]:
            result, name = best_result
            residuals = result['y_test'] - result['y_pred']
            axes[1, 0].scatter(result['y_pred'], residuals, alpha=0.6, s=30)
            axes[1, 0].axhline(y=0, color='r', linestyle='--')
            axes[1, 0].set_xlabel('Predicted Skill Score')
            axes[1, 0].set_ylabel('Residuals')
            axes[1, 0].set_title(f'Residual Plot - {name}')
        
        # 5. æ€§èƒ½æ¯”è¼ƒãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
        if len(successful_results) > 1:
            names = [name for _, name in successful_results]
            r2_scores = [result.get('r2', 0) for result, _ in successful_results]
            rmse_scores = [result.get('rmse', 0) for result, _ in successful_results]
            
            x_pos = np.arange(len(names))
            
            axes[1, 1].bar(x_pos - 0.2, r2_scores, 0.4, label='RÂ²', alpha=0.7)
            axes[1, 1].set_xlabel('Models')
            axes[1, 1].set_ylabel('RÂ² Score')
            axes[1, 1].set_title('Model Comparison (RÂ²)')
            axes[1, 1].set_xticks(x_pos)
            axes[1, 1].set_xticklabels(names)
            
            # æ•°å€¤è¡¨ç¤º
            for i, score in enumerate(r2_scores):
                axes[1, 1].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')
        
        # 6. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        axes[1, 2].axis('off')
        summary_text = "å›å¸°æ€§èƒ½ã‚µãƒãƒªãƒ¼\n" + "="*20 + "\n"
        
        for result, name in successful_results:
            summary_text += f"{name}:\n"
            summary_text += f"  RÂ² = {result.get('r2', 0):.3f}\n"
            summary_text += f"  RMSE = {result.get('rmse', 0):.3f}\n"
            summary_text += f"  MAE = {result.get('mae', 0):.3f}\n"
            if 'overfitting' in result:
                summary_text += f"  éå­¦ç¿’ = {result.get('overfitting', 0):.3f}\n"
            summary_text += "\n"
        
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,
                       verticalalignment='top', fontsize=9,
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        return fig

    def _add_regression_metrics(self, result, mlp_results, svm_results, baseline_results, cv_results):
        """å›å¸°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµæœã«è¿½åŠ """
        # MLPçµæœ
        if mlp_results.get('success', False):
            result.add_metric('skill_mlp_r2', mlp_results.get('r2', 0),
                            'MLPå›å¸°ã®RÂ²å€¤', 'skill_regression')
            result.add_metric('skill_mlp_rmse', mlp_results.get('rmse', 0),
                            'MLPå›å¸°ã®RMSE', 'skill_regression')
            result.add_metric('skill_mlp_overfitting', mlp_results.get('overfitting', 0),
                            'MLPã®éå­¦ç¿’åº¦', 'skill_regression')
        
        # SVMçµæœ  
        if svm_results.get('success', False):
            result.add_metric('skill_svm_r2', svm_results.get('r2', 0),
                            'SVMå›å¸°ã®RÂ²å€¤', 'skill_regression')
            result.add_metric('skill_svm_rmse', svm_results.get('rmse', 0),
                            'SVMå›å¸°ã®RMSE', 'skill_regression')
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœ
        if baseline_results.get('success', False):
            result.add_metric('skill_linear_r2', baseline_results.get('r2', 0),
                            'ç·šå½¢å›å¸°ã®RÂ²å€¤', 'skill_regression')
        
        # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«
        all_results = [mlp_results, svm_results, baseline_results]
        successful = [r for r in all_results if r.get('success', False)]
        if successful:
            best_model = max(successful, key=lambda x: x.get('r2', -np.inf))
            result.add_metric('skill_best_regression_r2', best_model.get('r2', 0),
                            'æœ€å„ªç§€å›å¸°ãƒ¢ãƒ‡ãƒ«ã®RÂ²å€¤', 'skill_regression')
        
        # äº¤å·®æ¤œè¨¼çµæœ
        for name, cv_result in cv_results.items():
            if cv_result.get('success', False):
                result.add_metric(f'skill_{name.lower()}_cv_r2_mean', cv_result.get('mean_r2', 0),
                                f'{name}ã®CVå¹³å‡RÂ²å€¤', 'skill_regression')
                result.add_metric(f'skill_{name.lower()}_cv_r2_std', cv_result.get('std_r2', 0),
                                f'{name}ã®CV RÂ²æ¨™æº–åå·®', 'skill_regression')

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'experiment_id']


class SkillLatentDimensionVSScoreEvaluator(BaseEvaluator):
    """ã‚¹ã‚­ãƒ«æ½œåœ¨å¤‰æ•°ã®ä¸»æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ãŒç·šå½¢ãªé–¢ä¿‚ã«ã‚ã‚‹ã‹ã‚’è©•ä¾¡"""
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples = 10
        self.significance_level = 0.05

    def evaluate(self, model, test_data, result: EnhancedEvaluationResult) -> None:
        """ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç·šå½¢é–¢ä¿‚è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        z_skill = test_data.get('z_skill')
        skill_scores = test_data.get('skill_scores')
        subject_ids = test_data.get('subject_ids')

        print("=" * 60)
        print("ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒ vs ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç·šå½¢é–¢ä¿‚è©•ä¾¡å®Ÿè¡Œ")
        print("=" * 60)

        if len(skill_scores) < self.min_samples:
            print(f"âš ï¸ ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³: {len(skill_scores)} < {self.min_samples}")
            result.add_metric("linear_relationship_status", 0, "ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³", "skill_linearity")
            return

        # 1. PCAåˆ†æã«ã‚ˆã‚‹ä¸»æ¬¡å…ƒç‰¹å®š
        pca_results = self._perform_pca_analysis(z_skill, skill_scores)
        
        # 2. å„æ¬¡å…ƒã¨ã®ç›¸é–¢åˆ†æ
        correlation_results = self._analyze_dimension_correlations(z_skill, skill_scores)
        
        # 3. ç·šå½¢å›å¸°åˆ†æ
        regression_results = self._perform_regression_analysis(z_skill, skill_scores)
        
        # 4. å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_linearity_visualization(
            z_skill, skill_scores, pca_results, correlation_results, regression_results
        )
        
        # çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
        self._add_linearity_metrics(result, pca_results, correlation_results, regression_results)
        
        # å¯è¦–åŒ–ã‚’è¿½åŠ 
        result.add_visualization("skill_linearity_analysis", visualization_fig,
                                description="ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç·šå½¢é–¢ä¿‚åˆ†æ",
                                category="skill_analysis")
        
        print("âœ… ã‚¹ã‚­ãƒ«æ½œåœ¨æ¬¡å…ƒ vs ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ç·šå½¢é–¢ä¿‚è©•ä¾¡å®Œäº†")

    def _perform_pca_analysis(self, z_skill, skill_scores):
        """PCAåˆ†æã§ã‚¹ã‚­ãƒ«æ½œåœ¨ç©ºé–“ã®ä¸»æ¬¡å…ƒã‚’ç‰¹å®š"""
        from sklearn.decomposition import PCA
        from scipy.stats import pearsonr
        
        print(f"\nğŸ” PCAåˆ†æå®Ÿè¡Œ...")
        
        # PCAå®Ÿè¡Œ
        pca = PCA()
        z_skill_pca = pca.fit_transform(z_skill)
        
        # å„ä¸»æˆåˆ†ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç›¸é–¢
        pc_correlations = []
        pc_p_values = []
        
        n_components = min(z_skill.shape[1], len(skill_scores))
        
        for i in range(n_components):
            try:
                corr, p_value = pearsonr(z_skill_pca[:, i], skill_scores)
                pc_correlations.append(corr)
                pc_p_values.append(p_value)
                
                significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
                print(f"  PC{i+1}: r={corr:.4f}, p={p_value:.4f} {significance}")
                
            except Exception as e:
                print(f"  PC{i+1}: è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {e}")
                pc_correlations.append(0.0)
                pc_p_values.append(1.0)
        
        # æœ€ã‚‚ç›¸é–¢ã®é«˜ã„ä¸»æˆåˆ†
        abs_correlations = [abs(c) for c in pc_correlations]
        best_pc_idx = np.argmax(abs_correlations) if abs_correlations else 0
        best_correlation = pc_correlations[best_pc_idx] if pc_correlations else 0.0
        
        print(f"  æœ€é«˜ç›¸é–¢: PC{best_pc_idx+1} (r={best_correlation:.4f})")
        
        return {
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'pc_correlations': pc_correlations,
            'pc_p_values': pc_p_values,
            'best_pc_idx': best_pc_idx,
            'best_correlation': best_correlation,
            'pca_components': pca.components_,
            'z_skill_pca': z_skill_pca
        }

    def _analyze_dimension_correlations(self, z_skill, skill_scores):
        """å„æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®ç›¸é–¢åˆ†æ"""
        from scipy.stats import pearsonr, spearmanr
        
        print(f"\nğŸ“Š æ¬¡å…ƒåˆ¥ç›¸é–¢åˆ†æ...")
        
        correlations = []
        p_values = []
        spearman_correlations = []
        spearman_p_values = []
        
        for dim in range(z_skill.shape[1]):
            try:
                # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ï¼ˆç·šå½¢é–¢ä¿‚ï¼‰
                pearson_corr, pearson_p = pearsonr(z_skill[:, dim], skill_scores)
                correlations.append(pearson_corr)
                p_values.append(pearson_p)
                
                # ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ï¼ˆå˜èª¿é–¢ä¿‚ï¼‰
                spearman_corr, spearman_p = spearmanr(z_skill[:, dim], skill_scores)
                spearman_correlations.append(spearman_corr)
                spearman_p_values.append(spearman_p)
                
                significance = "***" if pearson_p < 0.001 else "**" if pearson_p < 0.01 else "*" if pearson_p < 0.05 else ""
                print(f"  Dim{dim+1}: Pearson r={pearson_corr:.4f}(p={pearson_p:.4f}){significance}, Spearman Ï={spearman_corr:.4f}")
                
            except Exception as e:
                print(f"  Dim{dim+1}: è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {e}")
                correlations.append(0.0)
                p_values.append(1.0)
                spearman_correlations.append(0.0)
                spearman_p_values.append(1.0)
        
        # æœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ¬¡å…ƒ
        abs_correlations = [abs(c) for c in correlations]
        best_dim_idx = np.argmax(abs_correlations) if abs_correlations else 0
        best_dim_correlation = correlations[best_dim_idx] if correlations else 0.0
        
        print(f"  æœ€é«˜ç·šå½¢ç›¸é–¢: Dim{best_dim_idx+1} (r={best_dim_correlation:.4f})")
        
        return {
            'pearson_correlations': correlations,
            'pearson_p_values': p_values,
            'spearman_correlations': spearman_correlations,
            'spearman_p_values': spearman_p_values,
            'best_dim_idx': best_dim_idx,
            'best_dim_correlation': best_dim_correlation
        }

    def _perform_regression_analysis(self, z_skill, skill_scores):
        """ç·šå½¢å›å¸°åˆ†æ"""
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score, mean_squared_error
        from scipy import stats
        import warnings
        
        print(f"\nğŸ“ˆ ç·šå½¢å›å¸°åˆ†æ...")
        
        results = {}
        
        try:
            # å˜å¤‰é‡å›å¸°ï¼ˆæœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ¬¡å…ƒï¼‰
            correlations = []
            for dim in range(z_skill.shape[1]):
                try:
                    corr, _ = stats.pearsonr(z_skill[:, dim], skill_scores)
                    correlations.append(abs(corr))
                except:
                    correlations.append(0.0)
            
            best_dim = np.argmax(correlations)
            X_univariate = z_skill[:, best_dim].reshape(-1, 1)
            
            # å˜å¤‰é‡ç·šå½¢å›å¸°
            lr_uni = LinearRegression()
            lr_uni.fit(X_univariate, skill_scores)
            y_pred_uni = lr_uni.predict(X_univariate)
            
            r2_uni = r2_score(skill_scores, y_pred_uni)
            mse_uni = mean_squared_error(skill_scores, y_pred_uni)
            
            # å¤šå¤‰é‡å›å¸°ï¼ˆå…¨æ¬¡å…ƒï¼‰
            lr_multi = LinearRegression()
            lr_multi.fit(z_skill, skill_scores)
            y_pred_multi = lr_multi.predict(z_skill)
            
            r2_multi = r2_score(skill_scores, y_pred_multi)
            mse_multi = mean_squared_error(skill_scores, y_pred_multi)
            
            # çµ±è¨ˆçš„æ¤œå®š
            n = len(skill_scores)
            p_uni = z_skill.shape[1]  # å˜å¤‰é‡ãªã®ã§å®Ÿè³ª1
            p_multi = z_skill.shape[1]
            
            # Fçµ±è¨ˆé‡ï¼ˆå¤šå¤‰é‡å›å¸°ã®æœ‰æ„æ€§ï¼‰
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                f_stat = (r2_multi / p_multi) / ((1 - r2_multi) / max(1, n - p_multi - 1))
                f_p_value = 1 - stats.f.cdf(f_stat, p_multi, max(1, n - p_multi - 1))
            
            print(f"  å˜å¤‰é‡å›å¸° (Dim{best_dim+1}): RÂ²={r2_uni:.4f}, MSE={mse_uni:.4f}")
            print(f"  å¤šå¤‰é‡å›å¸° (å…¨æ¬¡å…ƒ): RÂ²={r2_multi:.4f}, MSE={mse_multi:.4f}")
            print(f"  Fæ¤œå®š: F={f_stat:.4f}, p={f_p_value:.4f}")
            
            # ç·šå½¢æ€§åˆ¤å®š
            if r2_uni > 0.7:
                linearity_status = "å¼·ã„ç·šå½¢é–¢ä¿‚"
            elif r2_uni > 0.4:
                linearity_status = "ä¸­ç¨‹åº¦ã®ç·šå½¢é–¢ä¿‚"
            elif r2_uni > 0.1:
                linearity_status = "å¼±ã„ç·šå½¢é–¢ä¿‚"
            else:
                linearity_status = "ç·šå½¢é–¢ä¿‚ãªã—"
                
            print(f"  ç·šå½¢æ€§åˆ¤å®š: {linearity_status}")
            
            results = {
                'best_dim': best_dim,
                'univariate_r2': r2_uni,
                'univariate_mse': mse_uni,
                'multivariate_r2': r2_multi,
                'multivariate_mse': mse_multi,
                'f_statistic': f_stat,
                'f_p_value': f_p_value,
                'linearity_status': linearity_status,
                'univariate_coef': lr_uni.coef_[0],
                'univariate_intercept': lr_uni.intercept_,
                'y_pred_uni': y_pred_uni,
                'y_pred_multi': y_pred_multi,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ å›å¸°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            results = {'success': False, 'error': str(e)}
        
        return results

    def _create_linearity_visualization(self, z_skill, skill_scores, pca_results, correlation_results, regression_results):
        """ç·šå½¢é–¢ä¿‚ã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. ä¸»æˆåˆ†ã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ•£å¸ƒå›³
        if pca_results and 'z_skill_pca' in pca_results:
            best_pc = pca_results['best_pc_idx']
            pc_data = pca_results['z_skill_pca'][:, best_pc]
            
            axes[0, 0].scatter(pc_data, skill_scores, alpha=0.6, s=30)
            axes[0, 0].set_xlabel(f'PC{best_pc+1}')
            axes[0, 0].set_ylabel('Skill Score')
            axes[0, 0].set_title(f'PC{best_pc+1} vs Skill Score\n(r={pca_results["best_correlation"]:.3f})')
            
            # å›å¸°ç›´ç·š
            z = np.polyfit(pc_data, skill_scores, 1)
            p = np.poly1d(z)
            axes[0, 0].plot(sorted(pc_data), p(sorted(pc_data)), "r--", alpha=0.8)
        
        # 2. æœ€ã‚‚ç›¸é–¢ã®é«˜ã„æ½œåœ¨æ¬¡å…ƒã¨ã‚¹ã‚­ãƒ«ã‚¹ã‚³ã‚¢ã®æ•£å¸ƒå›³
        if correlation_results and regression_results.get('success', False):
            best_dim = correlation_results['best_dim_idx']
            dim_data = z_skill[:, best_dim]
            
            axes[0, 1].scatter(dim_data, skill_scores, alpha=0.6, s=30)
            axes[0, 1].set_xlabel(f'Latent Dim {best_dim+1}')
            axes[0, 1].set_ylabel('Skill Score')
            axes[0, 1].set_title(f'Best Latent Dim vs Skill Score\n(r={correlation_results["best_dim_correlation"]:.3f})')
            
            # å›å¸°ç›´ç·š
            if 'y_pred_uni' in regression_results:
                sorted_indices = np.argsort(dim_data)
                axes[0, 1].plot(dim_data[sorted_indices], regression_results['y_pred_uni'][sorted_indices], "r--", alpha=0.8)
        
        # 3. ç›¸é–¢ä¿‚æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if correlation_results:
            correlations = correlation_results['pearson_correlations']
            n_dims = len(correlations)
            corr_matrix = np.array(correlations).reshape(1, -1)
            
            im = axes[0, 2].imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
            axes[0, 2].set_xticks(range(n_dims))
            axes[0, 2].set_xticklabels([f'D{i+1}' for i in range(n_dims)])
            axes[0, 2].set_yticks([0])
            axes[0, 2].set_yticklabels(['Skill Score'])
            axes[0, 2].set_title('Dimension-Skill Correlations')
            plt.colorbar(im, ax=axes[0, 2])
        
        # 4. å›å¸°è¨ºæ–­: æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        if regression_results.get('success', False) and 'y_pred_uni' in regression_results:
            residuals = skill_scores - regression_results['y_pred_uni']
            axes[1, 0].scatter(regression_results['y_pred_uni'], residuals, alpha=0.6, s=30)
            axes[1, 0].axhline(y=0, color='r', linestyle='--')
            axes[1, 0].set_xlabel('Predicted Skill Score')
            axes[1, 0].set_ylabel('Residuals')
            axes[1, 0].set_title('Residual Plot (Univariate)')
        
        # 5. äºˆæ¸¬ vs å®Ÿæ¸¬å€¤
        if regression_results.get('success', False):
            axes[1, 1].scatter(skill_scores, regression_results['y_pred_uni'], alpha=0.6, s=30, label='Univariate')
            if 'y_pred_multi' in regression_results:
                axes[1, 1].scatter(skill_scores, regression_results['y_pred_multi'], alpha=0.6, s=30, label='Multivariate')
            
            # ç†æƒ³ç·š
            min_score, max_score = np.min(skill_scores), np.max(skill_scores)
            axes[1, 1].plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.8)
            axes[1, 1].set_xlabel('True Skill Score')
            axes[1, 1].set_ylabel('Predicted Skill Score')
            axes[1, 1].set_title('Prediction vs True Values')
            axes[1, 1].legend()
        
        # 6. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        axes[1, 2].axis('off')
        summary_text = "ç·šå½¢é–¢ä¿‚åˆ†æçµæœ\n" + "="*20 + "\n"
        
        if regression_results.get('success', False):
            summary_text += f"å˜å¤‰é‡RÂ²: {regression_results.get('univariate_r2', 0):.3f}\n"
            summary_text += f"å¤šå¤‰é‡RÂ²: {regression_results.get('multivariate_r2', 0):.3f}\n"
            summary_text += f"åˆ¤å®š: {regression_results.get('linearity_status', 'N/A')}\n\n"
        
        if pca_results:
            summary_text += f"æœ€é«˜PCç›¸é–¢: {pca_results.get('best_correlation', 0):.3f}\n"
        
        if correlation_results:
            summary_text += f"æœ€é«˜æ¬¡å…ƒç›¸é–¢: {correlation_results.get('best_dim_correlation', 0):.3f}\n"
            
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes, 
                       verticalalignment='top', fontsize=10,
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        return fig

    def _add_linearity_metrics(self, result, pca_results, correlation_results, regression_results):
        """ç·šå½¢é–¢ä¿‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµæœã«è¿½åŠ """
        # PCAé–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if pca_results:
            result.add_metric('skill_pc_best_correlation', 
                            abs(pca_results.get('best_correlation', 0)),
                            'ä¸»æˆåˆ†ã¨ã®æœ€é«˜ç›¸é–¢ä¿‚æ•°', 'skill_linearity')
        
        # æ¬¡å…ƒåˆ¥ç›¸é–¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if correlation_results:
            result.add_metric('skill_dim_best_correlation',
                            abs(correlation_results.get('best_dim_correlation', 0)),
                            'æ½œåœ¨æ¬¡å…ƒã¨ã®æœ€é«˜ç›¸é–¢ä¿‚æ•°', 'skill_linearity')
            
            # æœ‰æ„ãªç›¸é–¢ã®æ•°
            significant_correlations = sum(1 for p in correlation_results.get('pearson_p_values', []) 
                                        if p < self.significance_level)
            result.add_metric('skill_significant_correlations',
                            significant_correlations,
                            'æœ‰æ„ãªç›¸é–¢ã‚’æŒã¤æ¬¡å…ƒæ•°', 'skill_linearity')
        
        # å›å¸°åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if regression_results.get('success', False):
            result.add_metric('skill_univariate_r2',
                            regression_results.get('univariate_r2', 0),
                            'å˜å¤‰é‡å›å¸°ã®RÂ²å€¤', 'skill_linearity')
            result.add_metric('skill_multivariate_r2',
                            regression_results.get('multivariate_r2', 0),
                            'å¤šå¤‰é‡å›å¸°ã®RÂ²å€¤', 'skill_linearity')
            result.add_metric('skill_regression_f_statistic',
                            regression_results.get('f_statistic', 0),
                            'å›å¸°ã®æœ‰æ„æ€§Fçµ±è¨ˆé‡', 'skill_linearity')

    def get_required_data(self) -> List[str]:
        return ['z_skill', 'skill_scores', 'experiment_id']