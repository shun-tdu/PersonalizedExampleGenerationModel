import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from typing import Dict, Any, List, Optional
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, silhouette_score
from scipy.spatial.distance import pdist, squareform
from torch.utils.data import ConcatDataset, DataLoader

from .base_evaluator import BaseEvaluator
from .result_manager import EnhancedEvaluationResult


class ComprehensiveLatentSpaceEvaluator(BaseEvaluator):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.min_samples_for_tsne = 30
        self.min_subjects_for_clustering = 2

    def get_required_data(self) -> List[str]:
        return ['test_loader', 'train_dataset', 'val_dataset', 'test_dataset', 'output_dir', 'experiment_id']

    def evaluate(self, model, test_data, device) -> EnhancedEvaluationResult:
        """åŒ…æ‹¬çš„æ½œåœ¨ç©ºé–“è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        experiment_id = test_data.get('experiment_id')
        output_dir = test_data['output_dir']
        train_dataset = test_data['train_dataset']
        val_dataset = test_data['val_dataset']
        test_dataset = test_data['test_dataset']

        train_config = self.config.get('training', {})

        result = EnhancedEvaluationResult(experiment_id, output_dir)

        print("=" * 60)
        print("åŒ…æ‹¬çš„æ½œåœ¨ç©ºé–“åˆ†æé–‹å§‹")
        print("=" * 60)

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµåˆ
        all_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset])
        all_dataloader = DataLoader(all_dataset, train_config.get('batch_size', 16))

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º
        extracted_latent_variables = self._extract_latent_variables(model, all_dataloader, device)

        if not extracted_latent_variables:
            result.add_metric('analysis_status', 0, 'ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå¤±æ•—', 'error')
            return result

        # è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
        analysis_results = self._perform_detailed_analysis(extracted_latent_variables, output_dir, experiment_id)

        #çµæœã‚’EnhancedEvaluationResultã«æ ¼ç´
        self._populate_evaluation_result(result, analysis_results)

        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = result.create_comprehensive_report()

        return result


    def _extract_latent_variables(self, model, all_dataloader, device)->Dict[str, Dict]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æ½œåœ¨å¤‰æ•°ã‚’æŠ½å‡º"""
        model.eval()

        z_style_list = []
        z_skill_list = []
        subject_ids = []
        is_expert_list = []

        result = {}

        with torch.no_grad():
            for batch_data in all_dataloader:
                trajectories = batch_data[0].to(device)
                subj_ids = batch_data[1] if len(batch_data) > 1 else [f"subj_{i}" for i in range(len(trajectories))]
                is_expert = batch_data[2] if len(batch_data) > 2 else torch.zeros(len(trajectories))

                encoded = model.encode(trajectories)
                z_style_list.append(encoded['z_style'].cpu().numpy())
                z_skill_list.append(encoded['z_skill'].cpu().numpy())
                subject_ids.extend(subj_ids)
                is_expert_list.extend(is_expert.cpu().numpy())

        if z_style_list:
            result = {
                'z_style': np.vstack(z_style_list),
                'z_skill': np.vstack(z_skill_list),
                'subject_ids': subject_ids,
                'is_expert': np.array(is_expert_list)
            }

            print(f"  âœ… {len(subject_ids)}ã‚µãƒ³ãƒ—ãƒ«, {len(set(subject_ids))}è¢«é¨“è€…")

        return result

    def _perform_detailed_analysis(self, data, output_dir, experiment_id) -> Dict[str, Any]:
        """è©³ç´°ãªæ½œåœ¨ç©ºé–“åˆ†æã‚’å®Ÿè¡Œ"""

        z_style = data['z_style']
        z_skill = data['z_skill']
        subject_ids = data['subject_ids']
        is_expert = data['is_expert']

        # è¢«é¨“è€…æƒ…å ±
        unique_subjects = list(set(subject_ids))
        n_subjects = len(unique_subjects)
        subject_to_idx = {subj: i for i, subj in enumerate(unique_subjects)}
        subject_labels = [subject_to_idx[subj] for subj in subject_ids]

        print(f"\nğŸ“ˆ æ½œåœ¨ç©ºé–“çµ±è¨ˆ:")
        print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(z_style)}")
        print(f"  è¢«é¨“è€…æ•°: {n_subjects}")
        print(f"  ã‚¹ã‚¿ã‚¤ãƒ«æ¬¡å…ƒ: {z_style.shape[1]}")
        print(f"  ã‚¹ã‚­ãƒ«æ¬¡å…ƒ: {z_skill.shape[1]}")

        # åˆ†æå®Ÿè¡Œ
        style_analysis = self._analyze_style_separation(z_style, subject_ids, unique_subjects)
        skill_analysis = self._analyze_skill_distribution(z_skill, is_expert)
        clustering_analysis = self._evaluate_clustering_performance(z_style, subject_labels, n_subjects)

        # å¯è¦–åŒ–ç”Ÿæˆ
        visualization_fig = self._create_comprehensive_visualizations(
            z_style, z_skill, subject_ids, is_expert, unique_subjects,
            output_dir, experiment_id
        )

        # çµæœçµ±åˆ
        results = {
            'total_samples': len(z_style),
            'n_subjects': n_subjects,
            'unique_subjects': unique_subjects,
            'style_analysis': style_analysis,
            'skill_analysis': skill_analysis,
            'clustering_analysis': clustering_analysis,
            'visualization_fig': visualization_fig
        }

        self._print_comprehensive_summary(results)

        return results

    def _analyze_style_separation(self, z_style, subject_ids, unique_subjects) -> Dict[str, Any]:
        """ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢ã®è©³ç´°åˆ†æ"""
        print(f"\nğŸ¨ ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢åˆ†æ:")

        # è¢«é¨“è€…ã”ã¨ã®çµ±è¨ˆ
        subject_stats = {}
        for subject in unique_subjects:
            mask = np.array(subject_ids) == subject
            subject_data = z_style[mask]

            if len(subject_data) > 0:
                stats = {
                    'mean': np.mean(subject_data, axis=0),
                    'std': np.std(subject_data, axis=0),
                    'n_samples': len(subject_data),
                    'centroid': np.mean(subject_data, axis=0)
                }
                subject_stats[subject] = stats
                print(f"  {subject}: {len(subject_data)}ã‚µãƒ³ãƒ—ãƒ«")

        # è¢«é¨“è€…é–“è·é›¢è¨ˆç®—
        if len(unique_subjects) > 1:
            centroids = np.array([stats['centroid'] for stats in subject_stats.values()])
            inter_distances = pdist(centroids, metric='euclidean')
            distance_matrix = squareform(inter_distances)

            # è¢«é¨“è€…å†…åˆ†æ•£ã®å¹³å‡
            within_variances = []
            for stats in subject_stats.values():
                if stats['n_samples'] > 1:
                    within_variances.append(np.mean(stats['std'] ** 2))

            avg_within_var = np.mean(within_variances) if within_variances else 0.001
            avg_between_dist = np.mean(inter_distances)

            # åˆ†é›¢æŒ‡æ¨™
            separation_ratio = avg_between_dist / (np.sqrt(avg_within_var) + 1e-8)

            print(f"  å¹³å‡è¢«é¨“è€…é–“è·é›¢: {avg_between_dist:.4f}")
            print(f"  å¹³å‡è¢«é¨“è€…å†…åˆ†æ•£: {avg_within_var:.4f}")
            print(f"  åˆ†é›¢æŒ‡æ¨™: {separation_ratio:.4f}")

            # åˆ¤å®š
            if separation_ratio > 3.0:
                status = "å„ªç§€ãªåˆ†é›¢"
            elif separation_ratio > 1.5:
                status = "ä¸­ç¨‹åº¦ã®åˆ†é›¢"
            elif separation_ratio > 0.5:
                status = "å¼±ã„åˆ†é›¢"
            else:
                status = "åˆ†é›¢ä¸ååˆ†"

            print(f"  åˆ¤å®š: {status}")

            return {
                'separation_ratio': separation_ratio,
                'avg_between_distance': avg_between_dist,
                'avg_within_variance': avg_within_var,
                'distance_matrix': distance_matrix.tolist(),
                'subject_stats': {k: {sk: sv.tolist() if isinstance(sv, np.ndarray) else sv
                                      for sk, sv in v.items()} for k, v in subject_stats.items()},
                'status': status
            }

        return {'status': 'åˆ†æä¸å¯ï¼ˆè¢«é¨“è€…æ•°ä¸è¶³ï¼‰'}

    def _analyze_skill_distribution(self, z_skill, is_expert) -> Dict[str, Any]:
        """ã‚¹ã‚­ãƒ«åˆ†å¸ƒã®åˆ†æ"""
        print(f"\nğŸ† ã‚¹ã‚­ãƒ«åˆ†å¸ƒåˆ†æ:")

        if len(set(is_expert)) < 2:
            print("  âš ï¸ ç†Ÿç·´åº¦ãƒ©ãƒ™ãƒ«ã®å¤šæ§˜æ€§ä¸è¶³")
            return {'status': 'insufficient_labels'}

        expert_mask = is_expert == 1
        novice_mask = is_expert == 0

        expert_data = z_skill[expert_mask]
        novice_data = z_skill[novice_mask]

        print(f"  ç†Ÿç·´è€…: {len(expert_data)}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"  åˆå¿ƒè€…: {len(novice_data)}ã‚µãƒ³ãƒ—ãƒ«")

        if len(expert_data) > 0 and len(novice_data) > 0:
            expert_centroid = np.mean(expert_data, axis=0)
            novice_centroid = np.mean(novice_data, axis=0)

            skill_separation = np.linalg.norm(expert_centroid - novice_centroid)

            expert_variance = np.mean(np.var(expert_data, axis=0))
            novice_variance = np.mean(np.var(novice_data, axis=0))
            avg_variance = (expert_variance + novice_variance) / 2

            skill_ratio = skill_separation / (np.sqrt(avg_variance) + 1e-8)

            print(f"  ç†Ÿç·´åº¦åˆ†é›¢è·é›¢: {skill_separation:.4f}")
            print(f"  ã‚¹ã‚­ãƒ«åˆ†é›¢æ¯”: {skill_ratio:.4f}")

            return {
                'skill_separation': skill_separation,
                'skill_ratio': skill_ratio,
                'expert_centroid': expert_centroid.tolist(),
                'novice_centroid': novice_centroid.tolist()
            }

        return {'status': 'insufficient_data'}

    def _evaluate_clustering_performance(self, z_style, subject_labels, n_subjects) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡"""
        print(f"\nğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡:")

        if n_subjects < self.min_subjects_for_clustering:
            print("  âš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡ä¸å¯ï¼ˆè¢«é¨“è€…æ•°ä¸è¶³ï¼‰")
            return {'status': 'insufficient_subjects'}

        try:
            # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            kmeans = KMeans(n_clusters=n_subjects, random_state=42, n_init=10)
            predicted_labels = kmeans.fit_predict(z_style)

            # è©•ä¾¡æŒ‡æ¨™
            ari = adjusted_rand_score(subject_labels, predicted_labels)
            silhouette = silhouette_score(z_style, predicted_labels)

            print(f"  Adjusted Rand Index: {ari:.4f}")
            print(f"  Silhouette Score: {silhouette:.4f}")

            # åˆ¤å®š
            if ari > 0.7:
                ari_status = "å„ªç§€"
            elif ari > 0.3:
                ari_status = "è‰¯å¥½"
            elif ari > 0.1:
                ari_status = "æ™®é€š"
            else:
                ari_status = "ä¸è‰¯"

            print(f"  ARIåˆ¤å®š: {ari_status}")

            return {
                'ari': ari,
                'silhouette': silhouette,
                'predicted_labels': predicted_labels.tolist(),
                'ari_status': ari_status
            }

        except Exception as e:
            print(f"  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {'status': f'clustering_error: {str(e)}'}

    def _create_comprehensive_visualizations(self, z_style, z_skill, subject_ids, is_expert,
                                             unique_subjects, output_dir, experiment_id):
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ç”Ÿæˆ - Figureã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        print(f"\nğŸ¨ å¯è¦–åŒ–ç”Ÿæˆä¸­...")


        fig, axes = plt.subplots(2, 3, figsize=(20, 14))

        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—æº–å‚™
        subject_to_idx = {subj: i for i, subj in enumerate(unique_subjects)}
        subject_colors = [subject_to_idx[subj] for subj in subject_ids]

        # 1. ã‚¹ã‚¿ã‚¤ãƒ«ç©ºé–“PCA
        if z_style.shape[1] >= 2:
            pca_style = PCA(n_components=2)
            z_style_pca = pca_style.fit_transform(z_style)

            scatter = axes[0, 0].scatter(z_style_pca[:, 0], z_style_pca[:, 1],
                                         c=subject_colors, cmap='tab10', alpha=0.7, s=30)
            axes[0, 0].set_title(f'Style Space PCA\n(Explained: {pca_style.explained_variance_ratio_.sum():.3f})')
            axes[0, 0].set_xlabel('PC1')
            axes[0, 0].set_ylabel('PC2')

            # é‡å¿ƒãƒ—ãƒ­ãƒƒãƒˆ
            for i, subject in enumerate(unique_subjects):
                mask = np.array(subject_ids) == subject
                if np.any(mask):
                    center = np.mean(z_style_pca[mask], axis=0)
                    axes[0, 0].scatter(center[0], center[1], c='red', s=200, marker='x', linewidth=3)
                    axes[0, 0].annotate(subject, center, xytext=(5, 5), textcoords='offset points')

        # 2. ã‚¹ã‚¿ã‚¤ãƒ«ç©ºé–“t-SNE
        if len(z_style) >= self.min_samples_for_tsne:
            try:
                tsne = TSNE(n_components=2, perplexity=min(30, len(z_style) // 4), random_state=42)
                z_style_tsne = tsne.fit_transform(z_style)

                axes[0, 1].scatter(z_style_tsne[:, 0], z_style_tsne[:, 1],
                                   c=subject_colors, cmap='tab10', alpha=0.7, s=30)
                axes[0, 1].set_title('Style Space t-SNE')
            except Exception as e:
                axes[0, 1].text(0.5, 0.5, f't-SNE Failed: {str(e)}', transform=axes[0, 1].transAxes, ha='center')
        else:
            axes[0, 1].text(0.5, 0.5, 'Insufficient samples for t-SNE', transform=axes[0, 1].transAxes, ha='center')

        # 3. ã‚¹ã‚­ãƒ«ç©ºé–“PCA
        if z_skill.shape[1] >= 2:
            pca_skill = PCA(n_components=2)
            z_skill_pca = pca_skill.fit_transform(z_skill)

            axes[0, 2].scatter(z_skill_pca[:, 0], z_skill_pca[:, 1],
                               c=is_expert, cmap='RdYlBu', alpha=0.7, s=30)
            axes[0, 2].set_title(f'Skill Space PCA\n(Expert=Blue, Novice=Red)')

        # 4. è¢«é¨“è€…é–“è·é›¢è¡Œåˆ—
        n_subjects = len(unique_subjects)
        distance_matrix = np.zeros((n_subjects, n_subjects))

        for i, subj_i in enumerate(unique_subjects):
            for j, subj_j in enumerate(unique_subjects):
                mask_i = np.array(subject_ids) == subj_i
                mask_j = np.array(subject_ids) == subj_j

                if np.any(mask_i) and np.any(mask_j):
                    center_i = np.mean(z_style[mask_i], axis=0)
                    center_j = np.mean(z_style[mask_j], axis=0)
                    distance_matrix[i, j] = np.linalg.norm(center_i - center_j)

        im = axes[1, 0].imshow(distance_matrix, cmap='viridis')
        axes[1, 0].set_title('Inter-Subject Distance Matrix')
        axes[1, 0].set_xticks(range(n_subjects))
        axes[1, 0].set_yticks(range(n_subjects))
        axes[1, 0].set_xticklabels(unique_subjects, rotation=45)
        axes[1, 0].set_yticklabels(unique_subjects)
        plt.colorbar(im, ax=axes[1, 0])

        # 5. è¢«é¨“è€…åˆ¥åˆ†å¸ƒ
        for i, subject in enumerate(unique_subjects):
            mask = np.array(subject_ids) == subject
            if np.any(mask) and z_style.shape[1] >= 2:
                subject_data = z_style[mask]
                axes[1, 1].scatter(subject_data[:, 0], subject_data[:, 1],
                                   label=subject, alpha=0.6, s=20)

        axes[1, 1].set_title('Style Space Distribution by Subject')
        axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # 6. æ½œåœ¨æ¬¡å…ƒã®åˆ†æ•£
        style_vars = np.var(z_style, axis=0)
        axes[1, 2].bar(range(len(style_vars)), style_vars)
        axes[1, 2].set_title('Style Dimension Variances')
        axes[1, 2].set_xlabel('Dimension')
        axes[1, 2].set_ylabel('Variance')

        plt.tight_layout()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        plots_dir = os.path.join(output_dir, 'plots')
        os.makedirs(plots_dir, exist_ok=True)
        
        save_path = os.path.join(plots_dir, f'comprehensive_latent_analysis_exp{experiment_id}.png')
        fig.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        
        print(f"  âœ… å¯è¦–åŒ–ä¿å­˜: {save_path}")
        print(f"  âœ… å¯è¦–åŒ–Figureç”Ÿæˆå®Œäº†")

        return fig

    def _print_comprehensive_summary(self, results):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ åŒ…æ‹¬çš„æ½œåœ¨ç©ºé–“åˆ†æã‚µãƒãƒªãƒ¼")
        print("=" * 60)

        print(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {results['total_samples']}")
        print(f"è¢«é¨“è€…æ•°: {results['n_subjects']}")

        # ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢çµæœ
        style_analysis = results.get('style_analysis', {})
        if 'separation_ratio' in style_analysis:
            print(f"ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æŒ‡æ¨™: {style_analysis['separation_ratio']:.4f}")
            print(f"ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢åˆ¤å®š: {style_analysis['status']}")

        # ã‚¹ã‚­ãƒ«åˆ†æçµæœ
        skill_analysis = results.get('skill_analysis', {})
        if 'skill_ratio' in skill_analysis:
            print(f"ã‚¹ã‚­ãƒ«åˆ†é›¢æ¯”: {skill_analysis['skill_ratio']:.4f}")

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
        clustering_analysis = results.get('clustering_analysis', {})
        if 'ari' in clustering_analysis:
            print(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ (ARI): {clustering_analysis['ari']:.4f}")
            print(f"ARIåˆ¤å®š: {clustering_analysis['ari_status']}")

        print("=" * 60)

    def _populate_evaluation_result(self, result: EnhancedEvaluationResult, analysis_results: Dict[str, Any]):
        """åˆ†æçµæœã‚’EnhancedEvaluationResultã«æ ¼ç´"""
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        result.add_metric('total_samples', analysis_results['total_samples'],
                          'ç·ã‚µãƒ³ãƒ—ãƒ«æ•°', 'basic')
        result.add_metric('n_subjects', analysis_results['n_subjects'],
                          'è¢«é¨“è€…æ•°', 'basic')

        # ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        style_analysis = analysis_results.get('style_analysis', {})
        if 'separation_ratio' in style_analysis:
            result.add_metric('style_separation_ratio', style_analysis['separation_ratio'],
                              'ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é›¢æŒ‡æ¨™', 'style_separation')
            result.add_metric('style_avg_between_distance', style_analysis['avg_between_distance'],
                              'è¢«é¨“è€…é–“å¹³å‡è·é›¢', 'style_separation')
            result.add_metric('style_avg_within_variance', style_analysis['avg_within_variance'],
                              'è¢«é¨“è€…å†…å¹³å‡åˆ†æ•£', 'style_separation')

        # ã‚¹ã‚­ãƒ«åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹
        skill_analysis = analysis_results.get('skill_analysis', {})
        if 'skill_ratio' in skill_analysis:
            result.add_metric('skill_separation_ratio', skill_analysis['skill_ratio'],
                              'ã‚¹ã‚­ãƒ«åˆ†é›¢æ¯”', 'skill_analysis')
            result.add_metric('skill_separation_distance', skill_analysis['skill_separation'],
                              'ç†Ÿç·´åº¦åˆ†é›¢è·é›¢', 'skill_analysis')

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        clustering_analysis = analysis_results.get('clustering_analysis', {})
        if 'ari' in clustering_analysis:
            result.add_metric('clustering_ari', clustering_analysis['ari'],
                              'Adjusted Rand Index', 'clustering')
            result.add_metric('clustering_silhouette', clustering_analysis['silhouette'],
                              'Silhouette Score', 'clustering')

        # å¯è¦–åŒ–(Figureã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’EnhancedEvaluationResultã«æ¸¡ã™)
        if 'visualization_fig' in analysis_results:
            result.add_visualization('comprehensive_latent_analysis',
                                     analysis_results['visualization_fig'],
                                     'åŒ…æ‹¬çš„æ½œåœ¨ç©ºé–“åˆ†æï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ã‚¹ã‚­ãƒ«åˆ†é›¢ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ã‚’å«ã‚€6ã¤ã®ãƒ—ãƒ­ãƒƒãƒˆï¼‰', 
                                     'comprehensive')






