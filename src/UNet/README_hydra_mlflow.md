# CLAUDE_ADDED
# Hydra + MLFlowçµ±åˆ UNetæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Hydraã¨MLFlowã‚’çµ±åˆã—ãŸUNetæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## æ¦‚è¦

- **Hydra**: è¨­å®šç®¡ç†ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
- **MLFlow**: å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ç®¡ç†
- **UNetæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«**: å€‹äººç‰¹æ€§ã«åŸºã¥ãè»Œé“ç”Ÿæˆ

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# Dockerç’°å¢ƒã®å ´åˆ
docker-compose exec app pip install -r requirements.txt

# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ
pip install -r requirements.txt
```

### 2. MLFlow UIã®èµ·å‹•

**é‡è¦**: MLFlow UIã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã¯ã€docker-compose.ymlã§ä»¥ä¸‹ã®ãƒãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

```yaml
services:
  app:
    ports:
      - "8888:8888"  # Jupyter
      - "5000:5000"  # MLFlow UI
```

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œ
cd /app/src/UNet
mlflow ui --host 0.0.0.0 --port 5000
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:5000` ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦MLFlow UIã‚’ç¢ºèªã§ãã¾ã™ã€‚

## ä½¿ç”¨æ–¹æ³•

### 1. è¨“ç·´å®Ÿè¡Œ

#### Hydraè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸè¨“ç·´
```bash
cd /app/src/UNet
python train.py
```

#### è¨­å®šã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¦è¨“ç·´
```bash
# ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¤‰æ›´
python train.py training.epochs=50

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å¤‰æ›´
python train.py training.batch_size=16 training.learning_rate=5e-5

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
python train.py training.use_dummy=true

# å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
python train.py data.train_data=data/Datasets/overfitting_dataset_cond_3.npz
```

#### è¤‡æ•°å®Ÿé¨“ã®ä¸¦åˆ—å®Ÿè¡Œ
```bash
# ç•°ãªã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¤‡æ•°å®Ÿé¨“ã‚’å®Ÿè¡Œ
python train.py -m training.learning_rate=1e-4,5e-5,1e-5 training.batch_size=16,32,64
```

### 2. è»Œé“ç”Ÿæˆ

#### Hydraè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ
```bash
python -c "from generate import hydra_generate; hydra_generate()"
```

#### å¾“æ¥ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ
```bash
python generate.py --checkpoint checkpoints/checkpoint_epoch_100.pth --data_path data/Datasets/overfitting_dataset_cond_3.npz --batch_size 8 --method ddim --steps 50
```

### 3. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡

```bash
python evaluate.py
```

è©•ä¾¡çµæœã¯ `evaluation_results/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã€MLFlowã«ã‚‚è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (config.yaml)

### ä¸»è¦ãªè¨­å®šé …ç›®

```yaml
# ãƒ¢ãƒ‡ãƒ«è¨­å®š
model:
  input_dim: 2
  condition_dim: 5  # è‡ªå‹•æ¤œå‡ºã•ã‚Œã¾ã™
  time_embed_dim: 128
  base_channels: 64

# è¨“ç·´è¨­å®š
training:
  batch_size: 32
  epochs: 100
  learning_rate: 1e-4
  save_interval: 10
  use_dummy: false

# ãƒ‡ãƒ¼ã‚¿è¨­å®š
data:
  train_data: "data/Datasets/overfitting_dataset_cond_3.npz"
  val_data: null
  val_split_ratio: 0.2

# ç”Ÿæˆè¨­å®š
generation:
  method: "ddpm"  # ddpm or ddim
  num_inference_steps: null
  seq_len: 101
  num_samples: 8

# MLFlowè¨­å®š
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "unet_diffusion_experiments"
  run_name: null
  log_model: true
  log_artifacts: true
```

### è¨­å®šã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ä¾‹

```bash
# ç•°ãªã‚‹ç”Ÿæˆæ‰‹æ³•ã§å®Ÿé¨“
python train.py generation.method=ddim generation.num_inference_steps=50

# ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã§å®Ÿé¨“
python train.py model.base_channels=128 model.time_embed_dim=256

# ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“
python train.py data.train_data=path/to/your/data.npz
```

## MLFlowã§ã®å®Ÿé¨“ç®¡ç†

### å®Ÿé¨“ã®ç¢ºèª
1. MLFlow UIã§å®Ÿé¨“å±¥æ­´ã‚’ç¢ºèª
2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’æ¯”è¼ƒ
3. æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š

### ãƒ­ã‚°ã•ã‚Œã‚‹æƒ…å ±
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: Hydraè¨­å®šã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: è¨“ç·´æå¤±ã€æ¤œè¨¼æå¤±ã€è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ**: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€è¨“ç·´æ›²ç·šã€ç”Ÿæˆè»Œé“

### ãƒ¢ãƒ‡ãƒ«ã®å–å¾—
```python
import mlflow.pytorch

# MLFlowã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model_uri = "runs:/<RUN_ID>/model"
model = mlflow.pytorch.load_model(model_uri)
```

## è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™ï¼š

### 1. è»Œé“å¤šæ§˜æ€§
- å¹³å‡ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢
- é–‹å§‹ç‚¹ãƒ»çµ‚ç‚¹ã®å¤šæ§˜æ€§

### 2. è»Œé“æ»‘ã‚‰ã‹ã•
- é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€ã‚¸ãƒ£ãƒ¼ã‚¯ã®çµ±è¨ˆ
- æ»‘ã‚‰ã‹ã•ã‚¹ã‚³ã‚¢

### 3. æ¡ä»¶ä¸€è²«æ€§
- æ¡ä»¶ã¨è»Œé“ç‰¹å¾´ã®ç›¸é–¢
- å¹³å‡çµ¶å¯¾ç›¸é–¢

### 4. çµ‚ç‚¹ç²¾åº¦
- å¹³å‡çµ‚ç‚¹èª¤å·®
- çµ‚ç‚¹èª¤å·®ã®åˆ†å¸ƒ

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **CUDA ãƒ¡ãƒ¢ãƒªä¸è¶³**
   ```bash
   python train.py training.batch_size=16  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
   ```

2. **æ¡ä»¶æ¬¡å…ƒã®ä¸ä¸€è‡´**
   - ãƒ¢ãƒ‡ãƒ«ãŒè‡ªå‹•çš„ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ¡ä»¶æ¬¡å…ƒã‚’æ¤œå‡ºã—ã¾ã™
   - å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã®æ¡ä»¶æ¬¡å…ƒãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„

3. **MLFlow UI ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„**
   - docker-compose.ymlã§ãƒãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚° `"5000:5000"` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
   - ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•ï¼š `docker-compose down && docker-compose up -d`
   ```bash
   # Dockerã®å ´åˆã€ãƒãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèª
   docker-compose exec app mlflow ui --host 0.0.0.0 --port 5000
   ```

### ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ç¢ºèª
```bash
# Hydraè¨­å®šã®ç¢ºèª
python train.py --cfg job

# è©³ç´°ãƒ­ã‚°ã®å‡ºåŠ›
python train.py hydra.verbose=true
```

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
src/UNet/
â”œâ”€â”€ config.yaml              # Hydraè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ train.py                 # Hydra+MLFlowçµ±åˆè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ generate.py              # Hydraçµ±åˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ evaluate.py              # è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ Model.py                 # UNetãƒ¢ãƒ‡ãƒ«å®šç¾©
â”œâ”€â”€ TrajectoryDataset.py     # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
â”œâ”€â”€ visualize.py             # å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ visualization.ipynb     # Jupyterå¯è¦–åŒ–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ mlruns/                  # MLFlowå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ checkpoints/             # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ evaluation_results/      # è©•ä¾¡çµæœ
â””â”€â”€ generated_trajectories/  # ç”Ÿæˆçµæœ
```

## Dockerç’°å¢ƒã§ã®ä½¿ç”¨

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker-compose exec app bash

# UNetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd /app/src/UNet

# è¨“ç·´å®Ÿè¡Œ
python train.py

# MLFlow UIèµ·å‹•
mlflow ui --host 0.0.0.0 --port 5000
```

## é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

### ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
```yaml
# custom_config.yaml
defaults:
  - config

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
training:
  epochs: 200
  batch_size: 64

model:
  base_channels: 128
```

```bash
python train.py --config-name=custom_config
```

### Hydra Sweeps (ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´)
```bash
# config.yamlã«ä»¥ä¸‹ã‚’è¿½åŠ 
# hydra:
#   sweep:
#     dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
#     subdir: ${hydra:job.override_dirname}

python train.py -m training.learning_rate=1e-4,5e-5,1e-5 training.batch_size=16,32
```

ã“ã‚Œã§ã€Hydraã¨MLFlowã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªå®Ÿé¨“ç®¡ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

â— Hydraã¨MLFlowã‚’çµ±åˆã—ãŸUNetæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºãƒ•ãƒ­ãƒ¼ã«ã¤ã„ã¦ã€æ®µéšçš„ã«è§£èª¬ã—ã¾ã™ã€‚

  é–‹ç™ºãƒ•ãƒ­ãƒ¼å…¨ä½“å›³

  1. ç’°å¢ƒæº–å‚™ãƒ»è¨­å®š
      â†“
  2. ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»ç¢ºèª
      â†“
  3. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»å®Ÿé¨“ç®¡ç†
      â†“
  4. çµæœåˆ†æãƒ»æ¯”è¼ƒ
      â†“
  5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
      â†“
  6. è»Œé“ç”Ÿæˆãƒ»å¯è¦–åŒ–
      â†“
  7. æœ€é©åŒ–ãƒ»æ”¹å–„

  1. ç’°å¢ƒæº–å‚™ãƒ»è¨­å®š ğŸ”§

  1.1 Dockerç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
  cd /path/to/PersonalizedExampleGeneration

  # Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•
  docker-compose up -d

  # ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
  docker-compose exec app bash

  1.2 ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

  # ã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œ
  pip install -r requirements.txt

  1.3 MLFlow UIèµ·å‹•

  cd /app/src/UNet
  mlflow ui --host 0.0.0.0 --port 5000 &
  â†’ ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5000 ã‚’é–‹ã„ã¦MLFlow UIã‚’ç¢ºèª

  2. ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ»ç¢ºèª ğŸ“Š

  2.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª

  # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
  ls /data/Datasets/

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ç¢ºèªï¼ˆJupyterä½¿ç”¨æ¨å¥¨ï¼‰
  python -c "
  import numpy as np
  data = np.load('/data/Datasets/overfitting_dataset_cond_3.npz')
  print('Available keys:', list(data.keys()))
  print('Trajectories shape:', data['trajectories'].shape)
  print('Conditions shape:', data['conditions'].shape)
  "

  2.2 è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª¿æ•´

  # config.yamlã‚’ç·¨é›†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’è¨­å®š
  vim config.yaml

  # ãƒ‡ãƒ¼ã‚¿è¨­å®šä¾‹
  data:
    train_data: "data/Datasets/overfitting_dataset_cond_3.npz"
    val_data: null
    val_split_ratio: 0.2

  3. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»å®Ÿé¨“ç®¡ç† ğŸš€

  3.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿé¨“

  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§åˆå›è¨“ç·´
  python train.py

  # å®Ÿè¡Œå¾Œã€MLFlow UIã§çµæœã‚’ç¢ºèª
  # - Experiment: unet_diffusion_experiments
  # - Runåã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèª

  3.2 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®Ÿé¨“

  # å­¦ç¿’ç‡ã®å®Ÿé¨“
  python train.py training.learning_rate=5e-5 mlflow.run_name="lr_5e-5"
  python train.py training.learning_rate=1e-5 mlflow.run_name="lr_1e-5"

  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å®Ÿé¨“
  python train.py training.batch_size=16 mlflow.run_name="batch_16"
  python train.py training.batch_size=64 mlflow.run_name="batch_64"

  # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å®Ÿé¨“
  python train.py model.base_channels=128 mlflow.run_name="channels_128"

  3.3 ä¸¦åˆ—å®Ÿé¨“ï¼ˆHydra Multirunï¼‰

  # è¤‡æ•°ã®å­¦ç¿’ç‡ã‚’åŒæ™‚å®Ÿé¨“
  python train.py -m training.learning_rate=1e-4,5e-5,1e-5 \
    mlflow.run_name="lr_sweep"

  # å­¦ç¿’ç‡Ã—ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
  python train.py -m \
    training.learning_rate=1e-4,5e-5 \
    training.batch_size=16,32 \
    mlflow.run_name="grid_search"

  4. çµæœåˆ†æãƒ»æ¯”è¼ƒ ğŸ“ˆ

  4.1 MLFlow UIã§ã®æ¯”è¼ƒ

  1. å®Ÿé¨“ä¸€è¦§ã®ç¢ºèª
    - Experiments â†’ unet_diffusion_experiments
    - å„Runã®è¨“ç·´æå¤±ã€æ¤œè¨¼æå¤±ã‚’æ¯”è¼ƒ
  2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ vs ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†æ
    - Compare runsæ©Ÿèƒ½ã‚’ä½¿ç”¨
    - æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã‚’ç‰¹å®š
  3. è¨“ç·´æ›²ç·šã®ç¢ºèª
    - Artifacts â†’ plots â†’ training_curves.png
    - éå­¦ç¿’ã®æœ‰ç„¡ã€åæŸçŠ¶æ³ã‚’ç¢ºèª

  4.2 ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯ãªåˆ†æ

  import mlflow
  import pandas as pd

  # å®Ÿé¨“çµæœã®å–å¾—
  client = mlflow.tracking.MlflowClient()
  experiment = client.get_experiment_by_name("unet_diffusion_experiments")
  runs = client.search_runs(experiment.experiment_id)

  # DataFrameåŒ–ã—ã¦åˆ†æ
  runs_df = pd.DataFrame([{
      'run_id': run.info.run_id,
      'learning_rate': run.data.params.get('training.learning_rate'),
      'batch_size': run.data.params.get('training.batch_size'),
      'final_train_loss': run.data.metrics.get('train_loss'),
      'final_val_loss': run.data.metrics.get('val_loss')
  } for run in runs])

  print(runs_df.sort_values('final_val_loss'))

  5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ğŸ¯

  5.1 æœ€é©ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š

  # MLFlow UIã§æœ€é©ãªRunã‚’ç‰¹å®šå¾Œã€ãã®Run IDã‚’ä½¿ç”¨
  export BEST_RUN_ID="your_best_run_id"

  # ã¾ãŸã¯æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
  ls checkpoints/ | tail -1

  5.2 åŒ…æ‹¬çš„è©•ä¾¡ã®å®Ÿè¡Œ

  # è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
  python evaluate.py

  # çµæœç¢ºèª
  ls evaluation_results/
  cat evaluation_results/evaluation_metrics.json

  5.3 è©•ä¾¡çµæœã®åˆ†æ

  # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª
  python -c "
  import json
  with open('evaluation_results/evaluation_metrics.json') as f:
      metrics = json.load(f)

  print('=== ä¸»è¦è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ===')
  print(f'è»Œé“å¤šæ§˜æ€§: {metrics.get(\"diversity_mean_pairwise_distance\", 0):.4f}')
  print(f'æ»‘ã‚‰ã‹ã•: {metrics.get(\"smoothness_smoothness_score\", 0):.4f}')
  print(f'çµ‚ç‚¹ç²¾åº¦: {metrics.get(\"endpoint_mean_endpoint_error\", 0):.4f}')
  print(f'æ¡ä»¶ä¸€è²«æ€§: {metrics.get(\"consistency_mean_abs_correlation\", 0):.4f}')
  "

  6. è»Œé“ç”Ÿæˆãƒ»å¯è¦–åŒ– ğŸ¨

  6.1 è»Œé“ç”Ÿæˆ

  # DDPMç”Ÿæˆ
  python generate.py \
    --checkpoint checkpoints/checkpoint_epoch_100.pth \
    --data_path data/Datasets/overfitting_dataset_cond_3.npz \
    --method ddpm \
    --batch_size 16

  # DDIMç”Ÿæˆï¼ˆé«˜é€Ÿï¼‰
  python generate.py \
    --checkpoint checkpoints/checkpoint_epoch_100.pth \
    --data_path data/Datasets/overfitting_dataset_cond_3.npz \
    --method ddim \
    --steps 50 \
    --batch_size 16

  6.2 Hydraçµ±åˆç”Ÿæˆ

  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ã§ã®ç”Ÿæˆ
  python -c "from generate import hydra_generate; hydra_generate()"

  # è¨­å®šã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
  python -c "from generate import hydra_generate; hydra_generate()" \
    generation.method=ddim \
    generation.num_samples=32

  6.3 å¯è¦–åŒ–

  # åŸºæœ¬å¯è¦–åŒ–
  python visualize.py --checkpoint checkpoints/checkpoint_epoch_100.pth

  # Jupyter Notebookã§ã®è©³ç´°åˆ†æ
  jupyter notebook visualization.ipynb

  7. æœ€é©åŒ–ãƒ»æ”¹å–„ ğŸ”„

  7.1 çµæœã«åŸºã¥ãæ”¹å–„ç­–ã®æ¤œè¨

  æ€§èƒ½ãŒä½ã„å ´åˆã®å¯¾å‡¦æ³•:

  # 1. ã‚ˆã‚Šé•·ã„è¨“ç·´
  python train.py training.epochs=200

  # 2. ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®æ‹¡å¼µ
  python train.py model.base_channels=128 model.time_embed_dim=256

  # 3. å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ãŒå¿…è¦ï¼‰
  python train.py training.learning_rate=1e-4

  # 4. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚„å‰å‡¦ç†ã®æ”¹å–„
  # â†’ TrajectoryDataset.pyã®ä¿®æ­£ãŒå¿…è¦

  7.2 ç¶™ç¶šçš„ãªå®Ÿé¨“ç®¡ç†

  # å®šæœŸçš„ãªå®Ÿé¨“ã®å®Ÿè¡Œ
  for lr in 1e-4 5e-5 1e-5; do
    for batch in 16 32 64; do
      python train.py \
        training.learning_rate=$lr \
        training.batch_size=$batch \
        mlflow.run_name="lr_${lr}_batch_${batch}"
    done
  done

  7.3 A/Bãƒ†ã‚¹ãƒˆçš„ãªæ¯”è¼ƒ

  # ãƒ¢ãƒ‡ãƒ«Aã¨ãƒ¢ãƒ‡ãƒ«Bã®æ¯”è¼ƒ
  python train.py model.base_channels=64 mlflow.run_name="model_A"
  python train.py model.base_channels=128 mlflow.run_name="model_B"

  # ä¸¡æ–¹ã‚’è©•ä¾¡ã—ã¦æ¯”è¼ƒ
  python evaluate.py  # model_A
  python evaluate.py  # model_B

  å®Ÿéš›ã®é–‹ç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ä¾‹ ğŸ’¼

  ã‚»ãƒƒã‚·ãƒ§ãƒ³1: åˆæœŸé–‹ç™º

  # 1. ç’°å¢ƒç¢ºèª
  docker-compose exec app bash
  cd /app/src/UNet

  # 2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
  python train.py training.epochs=50 mlflow.run_name="baseline"

  # 3. çµæœç¢ºèª
  mlflow ui --host 0.0.0.0 --port 5000 &
  # ãƒ–ãƒ©ã‚¦ã‚¶ã§MLFlow UIç¢ºèª

  # 4. åˆæœŸç”Ÿæˆãƒ†ã‚¹ãƒˆ
  python generate.py --checkpoint checkpoints/checkpoint_epoch_50.pth \
    --use_dummy --batch_size 8

  ã‚»ãƒƒã‚·ãƒ§ãƒ³2: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

  # 1. å­¦ç¿’ç‡å®Ÿé¨“
  python train.py -m training.learning_rate=1e-4,5e-5,1e-5 \
    training.epochs=100 \
    mlflow.run_name="lr_sweep"

  # 2. çµæœåˆ†æï¼ˆMLFlow UIï¼‰
  # 3. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ¬æ ¼è¨“ç·´
  python train.py training.learning_rate=5e-5 \
    training.epochs=200 \
    mlflow.run_name="final_model"

  ã‚»ãƒƒã‚·ãƒ§ãƒ³3: è©•ä¾¡ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤

  # 1. æœ€çµ‚è©•ä¾¡
  python evaluate.py

  # 2. è¤‡æ•°æ‰‹æ³•ã§ã®ç”Ÿæˆæ¯”è¼ƒ
  python generate.py --checkpoint checkpoints/checkpoint_epoch_200.pth \
    --method ddpm --batch_size 16 --output_dir results_ddpm

  python generate.py --checkpoint checkpoints/checkpoint_epoch_200.pth \
    --method ddim --steps 50 --batch_size 16 --output_dir results_ddim

  # 3. çµæœã®æ¯”è¼ƒåˆ†æ
  jupyter notebook visualization.ipynb

  é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ âš ï¸

  1. å®Ÿé¨“ã®è¨˜éŒ²: MLFlowã§å…¨å®Ÿé¨“ã‚’è¨˜éŒ²ã—ã€å†ç¾å¯èƒ½æ€§ã‚’ç¢ºä¿
  2. æ®µéšçš„æ”¹å–„: ä¸€åº¦ã«å¤šãã®å¤‰æ›´ã‚’ã›ãšã€ä¸€ã¤ãšã¤æ¤œè¨¼
  3. å¯è¦–åŒ–é‡è¦–: æ•°å€¤ã ã‘ã§ãªãã€ç”Ÿæˆçµæœã‚’è¦–è¦šçš„ã«ç¢ºèª
  4. è¨­å®šç®¡ç†: Hydraã§è¨­å®šã‚’ä½“ç³»çš„ã«ç®¡ç†
  5. ç¶™ç¶šçš„è©•ä¾¡: å®šæœŸçš„ã«åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ

  ã“ã®ãƒ•ãƒ­ãƒ¼ã«å¾“ã†ã“ã¨ã§ã€åŠ¹ç‡çš„ã‹ã¤ä½“ç³»çš„ãªãƒ¢ãƒ‡ãƒ«é–‹ç™ºãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
